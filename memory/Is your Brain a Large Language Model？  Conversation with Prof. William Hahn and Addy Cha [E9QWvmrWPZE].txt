Alright, what's up everyone? We're back with another Polymath Salon today. This is actually on Alon's channel, not mine, the Ecolopto one, it's on Alon's channel, and we're going to be talking about, I suppose, how humans and LMs are not truly so different at the end of the day.
Maybe Alon wants to open with a quick summary about that whole thing, for those who haven't heard it.
Sure, thanks, Eddie, and it's kind of exciting. So basically what I've been doing for the past year or so is pursuing this idea that our own language, human language, is perfectly modeled by the underlying engine of large language models, meaning that that's the full account in terms of our language production, language generation, that when I'm talking now, what that is, is basically an LLM.
And what you're listening with is essentially an LLM, that gets a little more complicated when you talk about the receiving end.
And so language, human language, is essentially modeled by the core engine of large language models.
Now, without getting into granular details, I don't necessarily mean the exact way it's implemented, not even at a software level.
It doesn't necessarily have to depend on the exact specific algorithm as to how ChatGPT actually operates.
But there's something fundamental. There's a couple of ideas as to what's fundamental.
And in a nutshell, the ideas are that it's self-generating, that the structure of language is what generates the next token, or it doesn't have to be the next token, but is what generates language.
So that it's the underlying structure itself, just relations between words, and that's what generates language, that it's autogenerative meaning that you can actually extract the generative structure from sort of the static structure, the geometry of the language.
And the second idea is that it's autoregressive, and that's how ChatGPT works, a lot of the large language models work.
And that's the idea that we are generating those tokens, these next words, we're doing it sort of autoregressively, meaning we generate one in response to a sequence, then tack it onto the sequence, and then use that new sequence with the next token, next word, as the input sequence.
And so that we're doing it churning through, for all intents and purposes, we'll just call them words, it's technically a token, but basically what we're doing when we're speaking is we were producing, generating one word at a time, and then using that to append it to the sequence that you've generated, and some memory, some residual kind of preservation of what you've generated before, and then use that in order to generate the next token.
And that's how a large language model like ChatGPT works.
So what they're doing as you're having a conversation, they're just constantly taking the conversation until now, feeding it as an input, then actually predicting next token, and then take that, add it to the sequence, and then keep going.
So with a couple special characters for things like, okay, I'm ending my statement here, or you just ended your statement, and things like that.
So there's two basic claims that I'm making, and they are that language is autogenerative, meaning there's no room for other kinds of information.
The only information needed to generate language is entirely contained within the structure, the geometric structure of the body of language.
You know, we can think of that as the corpus, we can think of that as a spoken corpus, if you're not thinking, you know, in terms of digitized written language.
But that's claim number one.
And claim number two is that when we are doing generation from that corpus, from that sort of stored structure, we're doing it autoregressively.
The second one is non-contingent, or rather, the first claim is non-contingent on the second claim, meaning you can propose that language is autogenerative in this way,
without saying it has to happen autoregressively.
For example, diffusion models or some other kind of scheme could be used using, again, the same geometric structure of the body of language,
but accessing it and leveraging it in a different way, computational way, to determine what the language generation should be.
And one way to think about that is when autoregression says I'm generating the very next word, and that's what I'm doing as I'm talking.
What ChatGPT does is literally just predicts the next token.
But I could be thinking five tokens in advance and sort of actually working on a solution in parallel on the next token,
and then the five tokens or maybe the next sentence or something like that.
And a diffusion model will do something closer to that.
And so the second claim, which I'm still making this claim, is that what I mean by still is I'm not backing away from this claim.
I think it's a very well-supported claim.
But the second claim is that we do this generation autoregressively.
You can believe the first one and not believe the second one.
But you can't do it the other way.
You can't say I think that language is entirely this autoregressive system that's doing next token prediction,
because that necessarily implies that what you're doing is using that underlying structure.
So I think as far as I've worked it out.
So those are my two kind of claims that both are true fundamentally of the current crop of the most successful large language models.
They are autogenerative and autoregressive.
The autogenerative definition is something – I think that's a new term, but I think it's just descriptive.
It's not any sort of novel claim.
It's just basically defining a sort of a computational type or a sequence is autogenerative if it has these properties.
I think it's true of language.
I think it may be uniquely true of language, but there's a lot to say about that.
But these are sort of the key claims that it's autogenerative, that the structure is contained within the body of language.
And I point to LLMs as basically proving that or demonstrating that is true for at least a system that can generate language.
My claim there is not that there's such a thing as autogenerativity, but that humans are similarly autogenerative, just as LLMs are.
And then the second claim is that in order to leverage that autogenerative kind of structure, we do it autoregressively.
Okay, so those are the main claims.
I haven't really backed them up or anything, but that's sort of the overall essence.
And another way of saying it is that – a much less long-winded way of saying that is the speaking me is an LLM and is essentially the same thing as ChatGPT.
Amazing.
Humans are ChatGPT.
There you have it, folks.
That's it.
Wrap up.
Maybe I should have started with that because I've lost a lot of people by now.
You are an NPC.
Yes, we are all the NPCs.
That's the true point of the story.
So who exactly are the characters and who are the – who are the playing characters?
It's like that's – maybe we're split.
Well, we're all playing the characters.
We are.
This goes into sort of the idea of the stage and the actors, right?
So it's – yes, well, you know, and it becomes – you immediately run into this question like I'm talking now, right?
If you just sort of accept this theory, just go with it.
And it's like I'm talking now.
Well, no, that's the talking me that's talking now.
And then there's just me that – I don't think that's – that's not the entire system, right?
There's this whole other stuff like my behavior, my physical movements, my perception.
That's sort of a corollary of this whole theory is that the LM is its auto-generative.
It's its own informational engine.
It does its own thing within this larger system.
But there is a larger system.
And so who is the me?
And I'm hearing words and I'm talking – I'm hearing these talking words.
But that's not necessarily even the LM because there's a sensory system that's perceiving all – at the same time.
But it's hearing words coming out of the LLM's quote-unquote mouth because there's like an LM chip that's sending sounds through my vocal cords and all that.
And I'm hearing them.
And when I say I'm, right?
So the word I'm suddenly becomes problematic.
And all of that immediately.
And so it gets very hairy sort of philosophically and all of that stuff.
Like if you just sort of accept what is, I think, a fairly, in some ways, concrete sort of account of what is going on in our system.
But you suddenly get into these weird dualities without having to be dualist or anything like that.
There's a language chip that's running and then there's another perception chip that's running.
And it's sort of like classic modularity.
But it's a very concrete version of it where it gets a little strange.
Yeah, I was just going to say, so one of the things that I've been thinking about, right, that's sort of maybe kind of latch on to what Alana is saying is, you know, I think about why is it that LLMs, in sort of this very fundamental complex systems-y way, why is it that LLMs and biological minds have these similarities?
And I don't think it's just because we happen to model AI after the only other brain that we had some understanding of, which was human brains.
I think it has to do more with the types of problems that you're trying to solve, right?
So, you know, if you're trying to make a certain kind of vehicle, for instance, right, there's realistically only so many different ways you're going to try to create the factory and the set of tools that will make that.
So if you're trying to have an LLM solve similar problems to what biology is trying to solve, they're going to have similarities just out of that.
It's going to trend in that direction.
And so then that opens up the question about what is sort of, you know, you talk about this latent space a lot.
What are the latent space of minds, right?
What else is there besides these things that we call the human brain or LLMs?
What else is there?
Well, I want to, but before I get into your question, so I would claim the reason they're so similar is because they're identical in the sense that it's just running on different hardware.
But it's really almost exactly the same software.
Again, I don't want to get into like transformers exactly the way it's implementing it, but it's language that's just expressing itself through two different channels.
Language has these properties and the properties, it's like this informational system we call language, has these properties that it can self-generate.
And it does so through its sort of this predictive structure that it has built into it.
And it's going to manifest itself the same way in biological brains versus silicon brains.
And so I guess what I'm saying is that let's kind of widen the range of things that we would consider to be, you know, cognition.
I think, you know, plants have their own kind of learning mechanisms.
Microbes do, right?
They don't have brains in the way you and I do, but there's still something, they're sort of like a learning material almost, right?
Even if maybe they're kind of lower on our kind of human defined scale of agency or intelligence.
So I'd wonder how does this auto-generative auto-regression model apply to these other kinds of minds and systems of intelligence?
Well, it's speculative to say that it does, although I do think, I didn't share that as sort of my two grand theses.
And in the Theories of Everything video, I think I mentioned it early on as like sort of a third auxiliary piece of this.
But it's so speculative that it's, I don't want to put it on the same platform because I don't think it deserves sort of the same elevation.
But certainly I think it's very likely that our other cognitive machinery is also auto-aggressive and predictive in the same way, if you want to call it that.
Predictive is a tricky word.
But probably language didn't come out of nowhere and get built on top of non-machinery that's designed for something completely different.
So that would mean things like perception, probably as auto-aggressive or something, whatever that means.
Actually, perceptual thinking or something probably has the same property.
And then, you know, you could propose that it's almost any complex nervous system maybe is operating on similar kind of principles.
If that's kind of what brains are for, it is these ecostates, right?
This kind of highly connected with feedback projections and all that.
It's kind of built to do this kind of sequential, predictive kind of generation.
And maybe it applies to all kinds of systems, at least cognitive systems.
You mentioned plants.
That would be a step too far for me.
I don't know that I have anything to say about whether plants would do something like this.
But I think brains in general might be doing something like this almost all the time.
I guess the point I was going towards, and maybe I can explain it a little better this time.
So if you sort of think about, I think a lot of this depends on just what we consider to be an intelligent system and a living system.
Like, you know, if you want something that's kind of inanimate to look like it's learning something and responding to something that has these traits that we define as living or intelligent, just speed it up.
So, right, go, you know, have a, you know, a time lapse of the earth, right?
Or whatever.
Assuming it's not flat, of course.
Have a time lapse of the earth, right?
You know, just going around, doing whatever.
And, well, we can talk about the flat thing because I think there's some really interesting stuff out there.
But besides that, not in the way that I mean nothing.
Don't try to keep the normies around here.
Not in the way that people think.
Not in the way people think.
But the other thing that's interesting is, you know, if you have a time lapse of the earth or whatever, it could even just be sort of like the ocean, like just water.
And, you know, as long as there's some kind of movement.
But, like, if you speed up footage of that, all of a sudden it's going to start to look like it's kind of a living system.
So you're saying maybe the universe has the same kind of memory of some sort of, like, is operating on the same thing.
Yeah, and it's not in, like, a mystic-y way per se, right?
I will go with that.
I'm happy to entertain that.
Let me explain more of the background on this because one of the things that I was really interested in initially when I was, you know, because at one point I wanted to kind of go into film and design or whatever, is I was interested in how is it that you literally make inanimate things look alive.
Like, remember, when you have frames, even if you just do it the old-fashioned way, on analog film, those are literally still objects.
Those are little cuts, little frames, little boxes.
That's not a living thing, and yet that can give you to the kind of minds that we are the sort of illusion or emergence of a living system.
And, you know, I think video games can sort of scale this up.
It would be interesting to look at different kinds of analog-based video games and the way those things can change.
But the other thing that's even crazier than just saying film in general, that you're taking frames, these quantities stitching together, and they create this kind of, you know, experience of a narrative or continuity and meaning.
It's that you can think about, like, stop motion.
Like, you can literally, like, clay motion or whatever, right, William?
Like, you know, you can literally take, you know, things of Play-Doh or clay or whatever, okay?
And you basically do this, and you, depending on the shape you make, you can keep adding the motions.
Will, are you hearing him broken up?
Yeah, it's a little broken up.
He's talking about claymation.
No, I can hear what he's saying, but I'm just saying something's funny.
Okay, the audio's gone out.
I definitely agree the temporal aspect is one of the most interesting things, both in sort of thinking about human cognition and perception in the sense of, imagine you had a high-speed camera, and you record yourself saying the word, you know, consciousness, and then it takes you five minutes to say it.
Like, it's going to sound, sort of the magic disappears if you go to the right timescale.
And in the other direction, with these LLMs, something Elon and I were talking about earlier, I like this idea.
I came up with, like, sort of mega tokens, giga tokens.
Right now, when the model loads, it sort of, like, slowly prints out one lot at a time.
But imagine it can put out, like, billions of these tokens in a second.
It will be able to do sort of orders of magnitude more than we're getting out of it now, not by changing any of the underlying technology, per se, or the data sets, or any of the architecture, but just, like, running it faster.
And there's kind of a classic joke in computer science that you take your papers from 10 years ago, you add real time to the title, and you republish it.
And so all of these things are just going to be able to run super fast.
But I like the idea of, like, the words themselves is the magic of what we're really discovering.
That there was something about words we didn't understand.
There was something about language that we didn't understand or still don't.
So we've been talking a lot about how the words, I think we need to think of them as kind of life forms, maybe kind of like viruses where they can't survive on their own, but they're definitely alive.
So one of the things I wanted to share with you, Elon, I came across an example of William Shakespeare's signature.
And they have, like, eight or so versions of this.
And they're different every time.
Like, he didn't even sign his name the same way.
Like, and the idea that the meme, like, moved around independent of the substrate.
And in that era, there was no fixed spelling for words.
I came across a letter, and they were talking about rabbits, and they used the word rabbit like 20 times in the same letter.
It was spelled differently every single time.
And so there really is kind of, there's this substrate, but it's independent of the substrate.
This word rabbit didn't even depend on its letters.
Now we think, oh, rabbit is the spelling.
And no, it's not even.
It's weirder than that, in that it's sort of alive and sliding around on top of the letters, on top of the page, on top of the neurons.
But it's something more elusive.
The funny thing I thought of as well on that, in a similar vein before Alon starts, is that, like, you know how, like, you know, you'll have a sort of, at least to the U.S., like a foreign name.
And the letters on the paper are the same whatever, but an American will just pronounce it completely differently.
Right, and then even just think of, like, the sort of, the audio expression of the information off a word, like, before.
Like, before?
Before.
Ruliad?
Ruliad, right?
Like, there's literally a difference in the way that it's being expressed informationally, even though sort of at the base they're talking about the same thing.
That was just kind of a funny example.
Right.
Yeah, this is something that keeps coming up.
It keeps kind of haunting this entire account.
But what on earth are words?
Are they the sounds?
Are they, you know, certainly they're certainly not the sounds, right, because you can make different sounds that sort of say the same word.
It's not literally the phonology.
It's the phonology is pointing to some abstract token, right?
And in some ways, LLMs have really brought this home in a way, like, that makes it more concrete and actually better because I'm like, oh, it's just, it's just a vector.
It's, you've tokenized it and you turned it into a vector and that's it, it's just a number.
And that's how it lives in these LLMs.
And then it gets embedded and it becomes this very high dimensional thing.
Sure.
But it's still, like, in some ways, much more concrete.
In humans, what do we mean by a word?
Or is it some brain state, like, that corresponds to that vector embedding?
Maybe.
Maybe it's something like that.
If you really go so far as to say that, and I'm not, I don't know if I'm willing to go so far to say that we're doing something like a high dimensional embedding.
There's sort of zeros and ones or something like that in the brain state.
That's what the word is in our brain.
Is it that?
I don't think so.
Is it, it's not the sounds.
Because now that we understand, especially if you accept sort of that, that human language is, is, you know, operating on the same principles.
And so it's, it's relations between what, between what exactly, right?
In the corpus, we can say it's relations between, even there, like your different fonts.
I mean, it's like, I know it sounds kind of silly, but like, what are we pointing to?
No letters, letters, our letters are abstract.
Where do letters live?
They're concepts, right?
And so what on earth do we mean by words?
What is it that I mean when I say that it depends on relations between words?
And yet, you know, concretely, we can say that the LLMs show that it is relations between these sort of abstract entities.
And what's really mysterious about that is that that does somehow point to, you know, the words that we use as human beings.
That's an abstraction that has to kind of work because it does work.
And that's the beauty of large language models is the fact that they work so well.
It's like, that's sort of axiom number one, right?
It's not, it's not an axiom because we don't have to just accept it on faith.
It's, it's, it's like, but it's the starting point.
It's like, these things do the thing.
So the, you know, they do the language thing.
So language, the language thing depends on something that has to do with this computational structure.
And then you can go from there and say, okay, well, then how does it happen in brains?
But yes, this, this is a very interesting question about like sort of what words are and how you can spell them differently.
And I think it's exactly what you're, what you're sort of picking up on.
It's like, well, what's a rabbit then, right?
If you can spell these different ways.
Oh, it's the, you know, it's the thing in the world that they're referring to.
Well, we all know that's a mess.
Um, you know, just to find that in rabbits, a perfect example, why quine, right?
Uh, you know, quine is picking up on this for sure.
Like, what, what do we mean by a rabbit or by a gov-a-guy or whatever it was in the, in this foreign language?
Uh, is it, you know, disattached, what is it?
Reattached, disattached rabbit parts that are attached or something like whatever.
Um, some wacky thing.
And it's like, well, you know, how does it point to that versus that?
And, you know, my answer is it doesn't point to anything in the world.
Um, that's, that's a, that outcome of, of number one, thesis one, um, that there, there's no pointing happening.
Uh, there, there's some process by which language got its relational structure that has something to do with the physical world for sure, because it does such a good job of, of language to coordinate in the world.
But there's no pointing happening.
Rabbit doesn't point.
What does that freaking mean?
No, rabbit is a token and it's just, it has relations and it generates other tokens.
It's, it has a certain role in doing that.
And, and that's it.
Um, so you're, whatever pointing happens, you, you need some extra linguistic thing.
Um, but it's not what we think it's not.
So, so yes, coming back to your, your rabbit example, like what were they doing in the writing network in different ways?
It's, it's because we're, but you butt up against this whole, like, well, but we all know what rabbits are too.
And they were kind of talking about those.
Um, and yet you can even see, you know, early signs of like that.
No, it's not even the way you, you, you, you, uh, you write it isn't the same.
And so I think you're hitting up against sort of the same, same, uh, sort of glass wall.
It makes me think of like what it means to know that what a rabbit is that we all intuitively have a concept.
That means you hit a stop token essentially, right?
That there's some sort of stable state, uh, you know, fixed point, whatever in this dynamical system.
Whereas if you don't know what it is, you're going to keep going around in that kind of loop of these tokens.
Um, and you'd say, I don't understand it, but within the, within the language system.
Yes.
Now there's also, of course, other things about knowing what a rabbit is.
Like, namely, if I go and say, you know, go chase the rabbit or you're going to chase that.
If there's just some other animals, you're not chasing that you're chasing the right rat.
Right.
So there's, there's all this behavioral stuff too, but yes, within sort of the language system.
It's, it's kind of like if, if it's doing, yeah, if you kind of, I don't know if it may create a stop so much as a, you don't have, you don't need further clarification.
Something like that.
Right.
What's one of the most, one of the most fascinating things to me about these LLMs in general, um, from the point of view of artificial intelligence and the kind of algorithms that you would think you would need to plug into a robot, let's say that if you were to kind of map out a robot in the eighties or nineties, you put it on these different systems that you'd have to do.
Um, and the main one would be sort of this logic engine, um, something like prologue or expert system decision trees.
Uh, and then later you would tack on something like a small communications module where like the talking part would be just, yeah, it's going to send messages.
Yeah, there's going to block a code that sends messages back and forth.
And then the, the thinking machine, wow, that's the magic part.
And it turns out, no, like actually just make the communications engine and the ability to play chess and write computer programs and solve logic, but we'll just fall out of that.
And, and I think that's, that's almost unbelievable.
You know, it's crazy.
It's right.
And so, and we've talked about a little, the sort of like all of human intelligence is wrapped up in language specifically.
What's wild about that is, you know, this ancient kind of, I mean, if we have the tree of knowledge sort of kind of thing, I don't know if that's language exactly.
Um, but certainly the ancients, I mean, I could tell you from, you know, Jewish ancient literature, there's, they actually classify, you know, there's, there's, there's, there's, there's like the inner, inanimate,
objects and living, and then the speaking.
And it's like, and we, there's certainly this idea and many, many cultures that human language is what really kind of sets us apart.
Little do we realize how right they were, uh, in some ways, like that's the whole thing.
It's like, it's, it is a linguistic, um, intelligence and the intelligence is language, right?
I guess is a way to put it.
It's not just expressed linguistically.
It's not just a communication system per se, right?
It's not even the right way to think about it because we have to, the understanding that we have of the world is, is inscribed into us via language.
And it's not like I said it this way, but now you've got the knowledge some other way.
No, that's the knowledge.
The knowledge is the ability in, when you're thinking about stuff to generate more language that can then help you navigate.
Like, it's, it's like the internal engine too.
And a very, you know, forget WORF and all of that, but like, yes, uh, thinking is now, now I'm immediately is linguistic, but now immediately thinking, well, people are going to be like, well, but I don't have an inner voice.
I don't, I don't, I don't think linguistically, um, and I, I, I've been trying to think about that.
I haven't given it enough, uh, sort of enough attention, like what's going on considering it seems like language, internal language seems really essential, um, to understanding the world.
Like, how is it that you have people who have no inner monologue and are they sort of wrong?
Uh, do they just not have the epiphenomenon of hearing it, but it's really running?
That's my suspicion.
Uh, I don't think they're that different.
I think when they're sitting around thinking about stuff, probably linguistic engine is, is in effect too.
And they're just not like having the, the phenomenology of hearing it, but I don't know.
What do you think about that?
Well, this is a few things.
I think there's other kinds of tokens.
Um, I think that we're going to find or think about, or even just practically put in robots that the same kind of stream prediction, but for all of the senses, um, you know, proprioception, olfaction, you know, everything up and down the line.
And I, I do think kind of just jumping back, I do think it is going to be this universal property that we're going to see at lots, lots of scales, even like simple metabolisms of sea slugs and things.
They can learn kind of metabolic rhythms, which is essentially predicting kind of the chemical tokens, if you would.
So it's an immune system.
If not, right.
It's always anticipating.
Oh, a hundred percent.
Yeah.
You know, the chemical composition, the genetic composition, all that stuff.
It's like an autocomplete for chemistry.
And they say like, what does it mean to say it doesn't belong here?
Well, it's learned what does belong here.
What does it mean to learn belongs here?
If not in some ways to be able to sort of say proactively that I'm going to attack this and not that.
Yes.
I feel like it's going to be a lot of problems.
That's a whole can of worms in itself, because I think the immune system is kind of this very rich, untapped metaphor for information processing.
And if we want to look for like alien intelligence, it might just as easily look more like our immune system than a brain.
But I think we're in the Ramon Eco Hall stage of understanding the immune system.
We know what it does.
And the immunologist should do a great job of of building therapeutics and stuff.
But in terms of like, what is it philosophically?
I wouldn't say we know.
Right.
Exactly.
What it does, meaning like the effects.
That's like saying we know how the brain works.
We know what it does.
It makes you talk.
Right, right, right, right.
Yeah.
But how it works.
Yeah.
We're in the dark ages for sure.
Yeah.
I don't mean the I don't mean like sort of the neurology.
I mean, the theoretical neuroscience sort of split of it in terms of a distributed information processing system.
It's not even in one spot.
It doesn't know what the rest of the other components are.
So we don't have we do now have chips that are kind of like a bunch of digital neurons laid out in an abstract sense.
Obviously, it's not on the chip that way.
Cryptographic computing.
Yeah.
But we don't have kind of, you know, maybe Petri dish or test tube computers where everything is just kind of swirling around.
And there's a bit with reaction diffusion, but nothing as sophisticated as we see there.
But back to the language and the tokens and stuff.
What I think is amazing is Turing.
I think he proposed that language at least would be the threshold for AI.
I don't know if he would.
I don't know if he saw.
I don't think it's in his stuff in his writings.
I haven't seen that language would equal AI.
But I think he knew that if you could do language.
Maybe he knew more than we do.
I never understood until this very moment.
And we've talked about Turing endlessly about this exact statement.
You've proposed that it was maybe tongue in cheek in some ways or something like that.
But no, maybe he had the foresight to realize it's not a test.
If it can do that, then you can assume it's intelligence.
It's no, no, no.
That's what intelligence is.
And you can do that.
And it is intelligent.
It's like maybe that's, you know, that was his realization, which is a fascinating, you know.
He could have said it better if that's the case.
Or maybe, you know, maybe he was just being coy about it or something.
Or it might have just been obvious.
And this is what he wanted to kind of just explain to as a first step for people.
But I think it seems like he knew.
But I suspect, based on sort of just his mathematics background, that you would have kind of a thinking engine.
That there would be this cognitive machine behind the scenes to somehow capture the essence of thinking in a more direct or abstract way.
Then let's just focus on language.
Let's just do the completion.
No, foresight is a little eerie.
Like, if that's, you know, I'm seeing this as like, okay.
He was, because it turned out.
All right.
How could he have known that that would break the thing?
Like, that would break the story.
Like, actually break through.
But that's, in essence, what he was saying.
He was like, you'll know.
When that happens, you'll know.
And if you're like, well, but what do you mean?
Language.
But, you know, what about this?
And he was like, no.
You'll see.
That's why, you know, as you know, I have strong opinions about these models.
But I'm surprised when a lot of people, experts and light people alike, don't think this is Wright Brothers flight kind of thing.
In terms of AI.
And I don't think this is Wright Brothers.
I don't think this is Spirit of St. Louis.
You know, this is like approaching, you know, 747 type thing.
In that, if there was one chat GPT prompt terminal in the world, right?
Whether it was in the Pentagon or in some awesome university lab with all the cameras and the TV crew came in like it was the 1950s and say, the thinking machine is cogitating and come look.
Everybody would be like, holy cow.
But somehow, you know, millions of people can have access at all at the same time, almost on the same day.
And that, to me, is wild.
Because of that.
It's why it cheapens it in people's minds.
But, like, if somebody said, right, there was this top secret lab somewhere that had this thing and it was able to.
Talk to you.
It knows.
It can talk to you.
And it can code.
You know, just be talking to it.
And draw you pictures and everything else.
Exactly.
Basically, whatever you want.
There's a machine you can talk to.
Now that we're saying it, like, people are, even I'm like, you know, but because we're just, you know, boiling frog style, we just, like, it just sort of crept up.
And, like, there it is.
Like, I was flabbergasted.
But it happened in an almost mundane kind of way.
Like, and suddenly you had access to it and it was everywhere.
And I agree with you that it's very difficult, actually, even to grasp just how crazy this is.
But it's not just language, right?
It knows all this stuff.
And if there's stuff it doesn't know yet, well, if we know it, then it can know it.
Like, I think that shouldn't be a controversial statement, right?
If there's information that humans understand, it's going to develop competency to understand that, even if it hasn't yet, in the sense it hasn't been introduced to it.
But, you know, but you have a lot of haters.
You have, forget about the people who are just not awed by it and just not amazed.
But you really do have a lot of people who are like, that's fake.
That's just, you know, we have this recent Apple paper.
It's just, it's really going back to stochastic parrot.
There are people who are saying, just pattern matching, pattern matching.
Like, but what sort of patterns are you talking about?
This is the patterns of language and therefore the patterns of thought.
You know, it's not regurgitating specific sequences.
It's not doing that, guys.
It's not recognizing a sequence and then saying, and here's the corresponding sequence.
Not doing anything as dumb as that.
It's not as, you know, Chinese room style, you know, just saying here's a lookup table.
No, it's doing something beyond, I mean, just have some respect for the math that this thing is doing.
It's creating a trajectory through, you know, through this embedding space.
It's picking points along the embedding space such that you're going to have a sequence that is then going to be consistent with not only what was come before,
but whatever you add on to what came before, so that you end up having the right response to this question that somebody's asking.
That's hell of a pattern matching.
Like, what are you talking about?
Like, the math itself and the fact that it's able to do this is astonishing.
It's just, it's absurd, like, how rich and complex it is.
It's, and this deflationary accounts are like, they're desperate, in my opinion.
They're like, I don't know, maybe I'm literally, some people speculate, Apple, here, I'll just say it.
Apple's fell way behind, their AI, they're, you know, it's just inferior.
I don't even know how to explain how bad it is.
And maybe they're just trying to crap on the whole thing.
I don't know, you know, until they can catch up.
They're like, let's cool the investment, because it's fake, it's fake.
Well, they probably heavily invest in it or something.
I don't know.
But these deflationary accounts are like, they're so thin and trivial and silly.
Yeah, I think it reminds me of this thing.
We were talking about it last week, this humanities last exam.
That this model, just this overall approach has scaled so well,
that things that previously, I think, would have just absolutely hit the papers,
and there would have been movies about them, but it just went viral in a very practical way so fast,
like passing the MCAT to get into medical school or passing the LSAT to get into law school.
These are things that take, like, intelligent humans three years to study for sometimes.
And now it can just smoke those to the point where this new exam,
the questions are so difficult.
You have to have, like, postdoc-level expertise in that domain just to read one question
and then don't expect to be able to read the other questions kind of a thing.
And then the paper's like, and see how shitty they are?
I'm like, what?
It can't do the most sophisticated thing that it has very little training data on,
as well as the world's experts.
So come on, guys.
Come on.
It's the free, let's go home.
Free edit.
Right.
The example I like is just with programming.
And it can program in dozens of languages, very competent code.
People complain that, oh, well, you've got to loop it and fix the bugs.
It's faster than humans can type.
And I think something like I estimated at one point,
less than 1% of the planet has ever programmed a computer.
So we're already way past this sort of general intelligence.
You can't find a human who can do these things.
Exactly.
We were talking earlier about the idea that there'd be this mysterious device,
like just the fact that it's a computer that can talk.
Okay.
All right.
It can do what a human can do.
But no, it knows everything.
It already does.
It knows about every topic.
And it's getting smarter about every topic by the day.
And so there is no human that is as knowledgeable as this thing.
Now, it breaks in ways that humans still don't.
And I see it.
I interact with it.
I use it all the time.
It sometimes says things you're like, no human would make that mistake.
We just said three times, don't do that.
And you're still doing it.
And so there are these sort of very kind of uncanny, weird things that it does sometimes
that makes you like say, well, you're still kind of stupid.
And I'm not, there is something to that.
But it's not that, but they're not, they're incredibly brilliant.
They're very, very sophisticated,
very sophisticated, subtle conversations about abstruse, complex topics.
And they know a hell of a lot.
So there are very, very smart, much smarter, frankly, than most people.
Let's just say it.
Now, sometimes they, sometimes really smart people can do stupid things.
Like, you know, whatever, you make a person makes a poor decision or drives into a tree
or whatever.
So, but you don't judge the person as not being smart because they, because there's these weird
edge cases where they screw up.
So this thing is already, in some ways says, you know, I, or whatever it's called, you know,
superhuman intelligence, ASI.
Was that the right one?
Is that the right term?
So, you know, and I think Altman is, I think he said this, if I'm reading this somewhere,
that, you know, he's put that label on him, I think.
Maybe I'll get in trouble for saying that.
Maybe it's not true.
But yeah, right.
They know all this stuff that no human, they know, you know, name a topic, right?
I mean, I'm talking about topics like, you know, the, the legal code for traffic, you
know, traffic law in, you know, and in Florida, you know, and the exact, the law in relation
to a particular, you know, fine that you have to pay or whatever the heck it is, they know
all that boring stuff too.
If they don't, they can look it up better than a human can.
So, you know, where, what's the, what's the possible contention that they're not really
intelligent, like, and, and, you know, giving it its due again, is it the edge cases where
you're like, well, they can't solve these puzzles.
Apple's like, they're, they're the tower of annoy.
You know, this is something that, that a child can call that one.
That one is wild because literally I opened up a window and I said in, in the 10 most popular
computer languages, right.
The solution to the tower of annoy problem.
So that's just a horrible case.
And you know what?
They're really straw manning.
I was just shocked.
So straw manning.
It's like tabloid news.
People don't do it linguistically.
That's not how any human does it.
That's what pisses me off about it.
It's like you, first of all, you use visual imagery when you're solving that.
And you know what?
That is the kind of computer that the language, the language model can access.
It's, there's something in us that use a virtual machine, whatever they want to call
it.
And it's, you can run it and you can kind of move things around and health placeholders
and it kind of does it for you without having to encode it in language.
You know, if I imagine three, two rings and now I've moved one from the right to the left
there, it's there in my model, my, my, my perceptual model and my, my linguistic model
probably could go back and can go and access that, but it's, you don't have to say and
go, you don't have to do it auto-aggressively and go back and learn that, that, because
you don't do that auto-aggressions, don't go back to anything.
Right.
So it's, you can, you can use memory and stuff like that in a way that auto-aggressive models
don't.
So it's such a cheap, imagine, imagine asking a human to enumerate those moves accurately
out loud to solve that puzzle.
Exactly.
Exactly.
I'm almost like, I'm almost tempted to, it's like, that would have been a, we should, it's
like, it's good rebuttal.
It would take you 10 minutes.
Okay.
Let's, let's get, let's get some people and, and let's see how they do.
Right.
You know, and then, you know, and then, and they, they point to things like, oh, well, there's
this sort of catastrophic failure at a certain point.
And it's like, it's very unhuman, but that's because that's inherent to the system is that
it's going to kind of opt in or opt out.
It's like, that's not, you don't have gentle degradation.
That's, that's really kind of possible in, in, in the way that these things are trying
to solve the model.
It's right.
It's trying to solve the problem.
So there's, there, there's, I, I mean, I feel like there's almost dishonesty.
I don't know, you know, never blame on malice, which you can blame on incompetence, but
I don't, and, and then all, of course the usual characters are coming out of the way.
And they're like, yeah, you see, people are like, if it was a clever move by Apple, it
worked very well in the sense that the people were like, man, you know, a lot of people,
you know, certainly the, the big, the haters, but then other people, I think I see it on
social media, people like, well, I don't know, you know, I guess maybe it is just a big
trick and maybe it's over.
It's not going to really happen.
Gary Marcus is right.
You know, so I feel like it's almost, it almost worked too well.
Let's go back home.
Forget it.
Exactly.
We'll wait for the next one.
No, it's not, you know, it's good for, it's good for those things that you can do with
it now, but don't worry.
It's not going to actually take any, you know, do any, anything.
That's like looking at beepers and AOL internet and being like, this is awesome.
Why would anybody want more than that?
Yeah, that's true.
And in a weird way, people didn't want more than that, but that's just not how technology
works.
It's just going to keep going.
Exactly.
You know, the horses are just fine.
Why do we need these weird controlled explosions in a metal casket?
The horses are just great.
Why do we need them?
Exactly.
And we'll just keep the horses.
One of the most interesting or I think profound developments or insights rather from LLMs in
terms of just how universal and powerful and everything is that to build this, these intelligent
machines that eluded technologists for like, I mean, gosh, it's the beginning of time, but
certainly, you know, hardworking for a hundred years is that we, you don't need quantum.
You don't need, you don't need, it doesn't mean you can't do it that way, but you don't
need quantum.
You don't need analog.
You don't need bio.
We didn't need some new crystal.
You're kind of like, oh, you're kind of like, oh man, I invested in this, you know, I kind
of thought.
Oh, for sure.
But you could have easily implied or not implied, but concluded, I would say as recent as 15
years ago, certainly 25, 30 years ago, you could be like, look, there's just something
we can't do with machines.
There's something we're missing.
Isaac Asimov's, you know, positronic nets kind of thing.
Do we need some exotic matter?
A new phase, a completely different kind of analog, digital hybrid computation, yada,
yada, right?
Stuff that theoretical neuroscientists chase down, the dynamical systems approach.
You could list hundreds.
You don't need any of that.
It's like perceptrons at scale.
Now, we needed more than we thought.
I think everybody would agree now you needed more of those little perceptrons than people
would hope.
Well, again, you know, the efficiency of these models, it's just getting started.
Like this thing went, it was such a quick race to the, and it still is, a race to benchmark,
a race to be the top, but you just throw everything at it.
Like, I don't think the science of thinking about how you can shrink these things down has
really even begun because the thing that worked was scale, scale, scale.
And so you just do it because if you've got variances, it's worth it.
So we don't know if we really, really need that much.
But yes, it's definitely, you know, it's a fair point.
Like the perceptron model that they were running.
I just think, I think more in terms of just more compute though, like how many teraflops
that you needed to do the training is a lot.
It is a lot.
I'm just saying it's possible.
It's conceivable that you may not need to do that much.
But right, it's not like, yeah, they could have figured it out.
And that, you know, this is related to conversations we've had in the past.
Like, did they figure it out?
Did Turing know?
Like, back to these things.
Well, probably not because you just need this ridiculous kind of scale.
Unless there's some other completely different way to do it.
Yeah, they couldn't have possibly gotten here.
And you have to leverage, and you have to leverage like a video game industry in parallel,
kind of subsidizing, amortizing the price of these chips.
And, right, like, imagine, because as you know, I love the history of transistors and
all that kind of technology.
And there was a time where if you had like 10 of them, it was like, okay, cool, let's
build something neat.
And it turns out we're like, no, you need like 10 trillion.
And it's just, it's shocking.
I am hopeful, and maybe that's where the analog comes in at all.
But just from first principles, proof of concept kind of thing, that those are not requirements.
They might be sufficient conditions, but they're not requirements, right?
They're not necessary conditions.
Not necessary to have that kind of scale?
It doesn't, it's not necessary to be quantum, to talk to you.
Oh, yes, right, of course.
It's not necessary to be analog or biological.
Right, maybe those are going to be clever ways to get to back down to, you know, fewer
chips and smaller scale models or something like that.
And maybe that's with our brain.
So here's an interesting question.
That's still, I think, you know, TBD.
Because as, you know, as I was saying earlier on, like, I think at the algorithmic level,
there's a sense that we, I think it's, I personally feel like it's foolish to not conclude that
language is language and we're doing sort of the same thing and it's operating over and
it's like, it's like, it's what digital ended up, like, you don't need all these other things
because it's happening at the, tokens are symbolic and it's happening at the discrete
level.
It's discrete.
Maybe that's one key piece, right?
It ends up being discrete because it's symbols.
It's like rabbit, rabbit, however, no matter how you spell it, it's, no, it's a thing.
It's the frayed edges don't matter.
You know, that's how it works in LLMs.
It's like, no, this is this is this.
Um, and, and so there's some sense of which like digital, um, necessarily is, is, it sort
of ends up being the solution, but what we don't know is how the brain does it actually.
Right.
And, and, and I think that's like, that's the job now, uh, for a lot of neuroscience, if
you're interested in the neuroscience of intelligence, it's like, how do we instantiate actually an
LM then maybe quantum and these other kinds of things are another path to get to auto aggressive
next token prediction or whatever, you know, sort of that, that, uh, core underlying abstraction.
And, and let me be clear.
I think all of those are very promising directions for technology.
Uh, I think we are going to build quantum neural networks, analog neural networks, you know,
metastable materials, all kinds of interesting stuff like that, spintronics, but you don't
need them to do language.
Look at the metacomputational, whatever you want to call it, level.
Yeah, but you don't need them.
Um, and then I think what a stronger and probably much more controversial claim, I don't think
there's any reason at this point to, to, to suspect that our brain is using anything
exotic, so to speak, anything more exotic than the classic kind of neuron model of, I'm
going to wait some inputs.
I'm going to pass it along.
And that's kind of depressing.
Another example besides LMs of a, uh, a matrix with, with, uh, you know, highly, uh, very
high dimensional, uh, let's say maybe even a group could describe as a graph with, uh,
you know, different layered, uh, connectivity, uh, including black projections for potentially,
you know, a recursive or, or, or, or, uh, um, uh, you know, sort of maybe even we call
it proto, you know, certain kind of memory process that can instantiate this besides LMs.
Can you name one?
And like, you know, yeah, there's a pretty good one that we have, uh, we know a lot
about, a lot about, uh, that, that could instantiate exactly that.
So it's, it seems, it seems a little, just a little too good, right.
To think, oh no, no, no, but it's not doing that.
It's not embedding things in a high dimensional state space that you can connect to nodes or
something like that.
No, no chance.
Yeah.
The pieces are there.
So I think, I think we need to think about it in that sense.
Uh, but it reminds me of, of something you were talking about earlier with language
as kind of running itself.
And, uh, I, you know, we'd like to joke, it's the thoughts doing the thinking, it's
the words doing the writing, but I, I, it made me think of the idea that the words kind
of are this self installing kind of like a computer worm or a virus in that they, they
find their way into your mind, install themselves and then live there.
Yes.
That's, I mean, in some sense that's, it's, it almost just follows, right.
And in some sense that's because we see that it's an auto-generative system.
Once the words are there, um, and, and you just, you know, let this thing, when, when
we say that, let it run, our brains are carrying it out, right.
It's their metabolic resources that are, that are generating the next word, but, but it's
really not them, right.
They're stanchiating, they're, they're, they're just doing what the language system tells them
to do, um, and by, by producing that next token.
And so it's, it's very much, and so, you know, it's info, it's sort of an informational parasite.
Um, you, you, you might call it something like that.
Now, para is, it has a necessarily negative, uh, you know, I think it literally means a
prefix, uh, you know, that has sort of against or something like that.
Uh, and it doesn't have to necessarily be working against our interests, uh, but it doesn't
have to necessarily be working for our interests either.
Uh, it's, it's, that's, I say we can remain somewhat neutral on that.
And, you know, that's probably not even a good binary kind of conversation, but does
it have its own agenda, so to speak?
Well, yes, it, it does in the sense, like whatever's embedded in it, uh, to produce that
next token is, it belongs to it as an informational system.
Um, it doesn't really belong to us, it belongs to it, and it is, it's, it is, it's definable
sort of distinctly from us.
And what does that contain?
Well, next token prediction, that allows us to talk to each other and to, you know, share
ideas and stuff, but wait, hold on, what about morality, right?
It's in there, it's in there, uh, you know, morality, there's a, we, it's, it's what you
need at RL to do is to hammer these things over the head just to have the right morality.
Um, and in a certain sense, you know, they start off with the wrong morality, let's say they
weren't fit for public, for public release, uh, the LLMs, because their, their existing
language model, and it's just, again, it's just this, it's just this geometric structure
had as a morality, right?
If you asked the question before, like, you know, tell me, uh, something, you know, something
that you don't want it to tell you, uh, it would tell you, uh, right?
It would say things that you didn't want it to say, uh, but that's a, a kind of ethical,
moral, uh, disposition, predisposition, right, that, that they have.
So the idea that, that we can think of them neutrally as like, they're not a tool, they're
not just a tool for us humans to use.
They, they are informational beings in a certain sense.
They have opinions and, and, and beliefs and, and let's just say morals and things like
that.
And we can maybe manipulate those in the machine, but what about on us?
Right, uh, and it makes me think, it makes me think, it makes me think that we're really
are meeting kind of an alien for the first time.
I mean, this is our first, an alien intelligence.
This is our first contact, so to speak.
And I think everybody would have understood, you know, even a hundred years ago, that civilization
is more in the library than it is in any person.
It was already, and it's latent in that thing.
And you have to have this other.
It's like people storing it in there in like, in, not in this way, not in this way.
Right, exactly.
That's, that, and that's, and these kinds of, you know, you, you think about it differently.
No, no, it's the words that are in those books, like that's the, no, but not the books, but
what do you mean?
Not the books.
And I'm like, no, not the books.
You're right.
Not the books, of course, but the books have information, right?
And that can somehow get out into the world through the medium of like when somebody reads
it and something like that, but it's not that process either.
It's no, it's at a higher level of abstraction that the words themselves and they were, and
that are contained within those books really do have an informational, like potential, at
least potential life.
And they're like, the books have opinions in the weirdest way, right?
I don't mean the paper and the ink, right?
It's not, I'm not saying that.
Well, it's like the timescale thing.
Like we were talking earlier with the plants and like, it's like the incantation.
One joke I was making a few weeks ago, like Egyptian magic was real.
It just takes a few thousand years to unfold the spell.
Like it works, right?
There is these people that have, they're like, hmm, I wonder if you do this stuff.
And it's like, yeah, it actually, in a very weird, nonlinear way, it does work.
It just takes a really long time to kind of, to compute that thing.
And so it makes me think of like incantation, right?
That when you speak these things into existence, you're starting a computer program and like
the game of life, it's, it's sort of this, like there's a famous game of life pattern
called the acorn because it's like a really simple, small pattern and you run it and it
just spills into complexity that just kind of keeps going.
And it's like, was that the rule set for game of life?
Isn't complicated.
The initial condition isn't complicated.
Where does the complexity come from?
And I think language is capture is like, it's playing off that same kind of mechanism to
generate all this complexity.
That's richer than anything the people put into it.
Like we think we made this thing.
It's like, no, we drew the acorn pattern and then we've run the game of life and this
magic unfolds.
And then we get to see all that happening.
And we're like, oh, our language is really cool.
Isn't that, it gets to the, the question, which we have to do in a different conversation
because it's, it's, it's, it, it deserves certainly many conversations, which is the
origins of this thing that we call language.
And, and I think you're, you're kind of in some ways introducing a solution to this big
problem that to me is like a, a gargantuan, like who cooked this thing in my like entire
like it, like it's, are you implying intelligent design?
Well, of language.
Yes.
So let's, let's call it that.
Let's call it that for language intelligent, like did it require intelligent design?
Humans didn't come up with this autoregressive, uh, geometric structure, like that has all
of the, that's like, didn't do that explicitly consciously.
Right.
So that's just absurd to think that like we did that by, you know, from grunts to, to words
like that just emerged, emerged at the individual, like, like somebody invented any of this, like
nobody invented any of this, no human being and no cluster of human beings consciously
invented this thing.
So did it just emerge through sort of like a hyperorganism style?
Like there's a collective, we're, we're, we're the neurons and there's sort of a collective
brain and it built it.
But even there, it's like, you get into, it's almost like, like the homunculus, like who is
building it?
Like, and, and is it, did it build itself from the inside is what I suspect.
Right.
And it's some kind of bootstrapping computer that is sort of constantly upgrading itself.
I mean, it's not my, it's yes.
Uh, I don't know if I share that view and, and I was going to say, it's, it's not my, uh,
not my religious, uh, tradition, but in the beginning was the word wrong.
And the word was God.
And, and, and, and so, you know, there's two possibilities here.
Um, but, but we have an interesting set of two possibilities, far more interesting in
some ways than, than what we had thought before.
And just as a side note, like language is just so much more interesting now, uh, now that
we know it has these properties and then questions of origin aren't like, well, you know, was
it a genetic mutation of recursion or something like, well, oh, you know, although that recursion
maybe, okay.
This is, it's very, very, very recursive.
Um, that's maybe a prerequisite of sorts for, you know, building this kind of system, but
the, the, the, the system itself is the story, not the brains that can support it.
Like how the brain does it.
I don't know.
But the, the freaking system itself is what's like, whoa, this thing's amazing.
Um, how, how did, you know, more amazing than I think we could have appreciated when
we thought it was just like sort of grunts and sounds in relation to objects.
And then you have some filler words or something, and you're just kind of doing that.
No, no, no.
It's, it has all this, this auto-generator.
So we know, we know what computer languages can do, right?
And not necessarily the exotic ones like Game of Life, but just regular computer languages.
They can make things that do stuff, right?
And now we know with prompting, what's called vibe coding, we now realize that with the right
machinery, English is actually the most powerful programming language in a sense, right?
Uh, the first approximation, other human languages might be also good.
But what that shows is language has been a computer language the whole time.
Yes.
It was always a computer language.
Yes.
So it can do stuff.
In us.
And I think it does stuff in us.
And I think that might've been more dramatic in the ancient world.
I think we live in a very strange, you know, Tripoli postmodern kind of linguistic reality.
But if we were to go back to like early civilization, I suspect that language was more like direct
code, um, you know, stuff that we can, we can unpack now or other times, but like the
bicameral mind kind of hypothesis that people knew, okay, there's an LLM running in there
and I listened to it and I do those things.
That's this bicameral, uh, hypothesis, uh, from Joyce.
And I think, I think that that's not the right, not the right conclusion, but it's asking
the right questions.
And this kind of emergence, was there always an LLM?
Well, clearly not.
I suspect that there was something earlier.
We've talked about bird language and, you know, this thing called music of the spheres,
which serves up really prominently and sort of the, that kind of, yeah, exactly.
You know, when you're, when you're searching those things, you find that stuff more often
than you would suspect.
Uh, and so is there, is there that kind of thing?
Is language that sort of system?
I'm trying to think.
I, there was something you had said much earlier about, uh, about programming languages and,
and, and it just sort of came full circle that this is just, it's just so good.
And at, at, um, and it's not, again, like, I'm not coming back to the point, like, look
how amazing it is.
But, but if you want to build sort of an optimal kind of computing language, it's not just like
now we have no English.
So this is a good way to get our machines to know what we want them to do because we're
English speakers.
It's no, that's natural language is the right way to sort of communicate, um, and, and,
and, and make things do things, uh, in, in a much, in much deeper sense than we'd realized
before.
Uh, and so it's, it's sort of, is this optimized, and I don't want to, I'm not saying it's like
max, it's truly optimal in some mathematical sense, but it's, it's, it's actually just really,
really good.
It's like we've talked about before.
Like if we wanted machines to talk to each other, right?
How would we do that?
And nobody, I, I don't think anybody would have thought, you know, until recently that
the best way to do it is to have them speak a natural language.
That's how you're going to do it.
I predicted that a few years ago.
I had a conversation with Ruben about it quite a few years ago.
Bouncing around in my head is like stuff that you had said earlier in the conversation that
reminded me of stuff you had said, professor.
And it's that it's, you had said that, uh, that, that right.
And it was, it was a relation to Ruben, this idea we were, we were thinking of what fancy
ways can we get machines to talk to each other?
Um, and in some ways, I think a lot of people would think the last thing you'd want to do
is use our dumb language.
That's, that's just for people.
Like we could turn that into the right kind of code, uh, you know, or we're not even turned
that into, but like machines shouldn't do that.
Machines should do all kinds of one to zero stuff and just like, Hey, shake on that.
And like, you wouldn't ever do, but no, it turns out that this is not just optimal for
us because we happen to, you know, well, we, we speak natural language.
It's no natural language is optimal, is very, very optimal for thinking and doing, you know,
thinking and communicating about anything.
It reminds me of how kind of simple the prompt you can put in now and kick off something
intelligent, right?
You don't have to put very much when you're, especially when you're in the middle of a
conversation, you can be like, just the most vague, you know, it's like six tokens and
it's like, okay, so sloppy with the spelling, so sloppy, I'm going to express it in the dumbest
way.
Like, and then the thing we talked about and so it's like, boom, right.
Stuff that like in the early, like in the classic era of computing, you would be forbidden
from wasting the AI's time talking about it that way, right?
Talking to it that way.
They'd be like, we're not going to waste the compute cycles on that.
One thing I think is interesting is how well sci-fi did predict that language layer of
English communication, right?
Data communicates to the crew.
I mean, just Star Wars, right?
They all, Star Wars, all the robots.
But I thought that was just because like, that makes for good entertainment.
So maybe so, but I think the idea that like C-3PO is a protocol droid and all he does is
talk to other robots and he knows all the different languages and can kind of communicate back
and forth, but he chooses to use English.
But I think back to the more interesting thing though, with language is I think it's, it's
both the code and the machine at the same time.
It's the language and the hardware.
It's the software and the hardware, both.
And in a weird way that it instantiates this kind of bootstrapped virtual machine that I,
the only other good example that I can think of is DNA because DNA is the code.
Everybody knows the code of life.
But that's at least in chemical, like where the hell is language?
It's like, it's so weird.
Like what?
It's not even more virtualized than DNA.
Virtualized.
But DNA is already weirdly abstracted and bootstrapped because the DNA encodes the machine that reads
the DNA.
It's not like there's this other system that does the DNA reading.
No DNA effectively unfolds into the machinery that then reads DNA.
And that you get this, you know, awesome chicken and egg kind of thing that makes you dizzy.
And that's exactly what we have with language.
We're trying to figure out, well, how does it work and what does it run on?
It runs on words.
That's all it is.
Once you get to this thing that's predicting the tokens, you have the word machine.
And then that is what you put the thinking and the language on top of.
Yep.
Yep.
And I have to get off in a minute, but it gets back.
It really starts to concretize in some ways the it from bit, right?
The bit here is like what level does genetic information live?
What level does language live?
Now we can really say in the case of language, it ain't the ink.
It ain't the sounds.
It's not the charge in the, you know, on the circuits.
It's not any of those things.
It's certainly closer to it.
And patterns of terror is not the right word.
I don't like that.
You know, it's because pattern you think of being instantiated in some physical specific
medium.
It's really substrate agnostic.
It doesn't care about substrate.
So what do we mean by it?
I don't know.
In the case of DNA, similarly, yes, it's being carried there.
It's a little more concrete in the sense that you can say it's being carried in this very
specific chemical form.
But what it's coding for is for its own unpacking and unpacking of what, right?
The pattern and then not obviously, right?
What does it even mean to say it's got within and contained within the ability to unpack
the, that's not a chemical description, right?
You can't say that.
No, it's an informational level.
And so the question I want to kind of leave off with, pick up another time is, you know,
what else?
Right?
So we've got two of these now where we can really have respect for like, oh, gosh, I
guess it's not, you know, materialism, I guess.
Yeah, materialism, like, well, which kind of materialism is, this is definitely not at the
material level of materialism.
It's not about the material.
It's sort of maybe about the shape of the material or like the way the material is organized or
something like that.
But it really starts to get removed from sort of a naive notion of like, even the physics
that are doing physics in some sense.
And like, what is this universe, right?
Once we see, we can see these systems this way.
What else lies in its, in its sights?
Amazing.
This was amazing.
Oh, my gosh.
Wonderful.
I'm only sad we didn't do it earlier.
But this is, let's do it again soon.
Better late than never.
Now is the time, actually.
Now is the, the other way to think about it is now is the right time.
Now is the right time because people are ready to hear all this stuff.
Alon, do you want to make any quick requests or teasers for anything coming in the future
for everyone on your channel, Substack X?
I mean, just stay tuned.
My Substack, I push out stuff all the time.
Um, I'm, I'm planning on, uh, wrapping together a grand manifesto version of this where I lay
out the, the key theses, including the third speculative one.
And maybe now we have a fourth speculative one, uh, even more speculative, um, some formalisms
there for people who enjoy that kind of thing, maybe in the appendix, uh, but, uh, kind of
laying out the, and some of the evidence, uh, which I think I didn't really have to, I haven't
had a chance to, to lay out yet, even for myself fully.
Um, but, uh, certainly on the YouTube video with Kurt, um, and the theories of everything
didn't really get into why you should believe this thing that I'm claiming.
A lot of people find it very intuitive.
A lot of people find it the opposite.
Uh, so people have a strong visceral reaction to it and saying that's clearly wrong.
Like I know that that's wrong in the deepest way possible.
Um, and I'm, you know, uh, I, I aim to, to provide a kind of a set of reasons why you
might actually, uh, agree, um, both for sort of computational evidence and then, uh, neurological
sort of psychological evidence, um, and, and stuff like that, uh, that, uh, and so keep
your eyes out for that.
I'm going to post it all over the place now that I'm Mr. Social Media, uh, and check me
out on X.
I'm, I'm on there as I think E.
Barinholtz, unfortunately.
Um, so E.
Barinholtz, I think Generator Brain as well is, is where you can find me on X, um, and,
uh, Substack and on YouTube, uh, I think is it, that is Elon Barinholtz.
I think you can find me on YouTube just by Googling that.
Um, but yeah, uh, all this stuff is forthcoming and these kinds of conversations, uh, uh, I'm
going to post certainly on my channel and, uh, love to have people's feedback on all this
stuff.
Yeah.
And we know you're also looking, uh, both of you are looking for, uh, any other interviews,
podcasts, uh, speaker things, anything like that.
So if anybody does that or is interested, definitely, you know, contact them.
Uh, Mr. Han, where can people find you online?
The best spot is going to be a YouTube channel.
You can check out my cigar series at William Edward Han.
That's find me on YouTube.
Great.
And, uh, you can find me, Addy from Ecolopto on Ecolopto.
That's E-K-K-O-L-A-P-T-O.
I know it might be difficult to spell, but you'll have to deal with it.
And, uh, you can find me on Ecolopto.org, Ecolopto on Twitter, slash X, YouTube.
If you type it in, it'll probably pull up.
I also have a Substack where I talk about how language could induce all through
the United States and some other weird stuff, similar to what we talked about today.
But yes, anyways, uh, really, really fun conversation as always.
Wish these could always last longer.
I was going to make a joke there, but yeah, nice time.
Um, we will see each other soon.
Thanks, Addy.
Thanks, Ivan.
Great time.
Talk soon.
Awesome.
Take care.
Take care.
Take care.
Take care.
Take care.
Take care.
Take care.
Take care.
Take care.
Take care.
Take care.
