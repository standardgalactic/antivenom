Whatever, you know, any reaction you have, stop me. I'm not like, I have to get through every slide kind of situation here.
So, in fact, I've been a professor for a whole bunch of years, and there's something I've noticed over many lectures that I've attended, many presentations, dissertations, is that you can say just about anything you want in the first slide, and no one will ask a question or object to it, because they're all getting settled in, and you can actually put a totally radical claim in there.
So, I decided to actually front load my entire talk into just the first slide. Oops. So, here it is, in a nutshell. So, we're all familiar with the mind-body problem, sometimes called the heart problem, phenomenal consciousness, subjectivity, where does it come from?
How do atoms and neurons and cells and electrical activity lead to the subjective feeling of rightness, the taste of chicken soup? How does the objective lead to the subjective?
And this is a hard nut to crack. It's a very difficult philosophical problem. The term heart problem is coined by David Chalmers, but he was coming on the heels of many other people who have articulated the same piece of phenomenal problem that is sometimes called the mind-body problem.
So, I wasn't actually expecting this in my lifetime, that there was going to be something that would actually help, I'd say, solve the problem.
I don't, I'm not claiming I've solved the mind-body problem, but what I am claiming is that something we've discovered by virtue of what's happened with large language models and AI actually has given us a core insight to the nature of the problem, where it comes from, and maybe perhaps a little bit of insight as to actually a different way to think about it.
So, here's my basic claim on a single slide, right, so you're not allowed to object.
So, LLMs, large language models, the way they work is they're trained on large purposes of text data, text data only.
What they learn is the basic, we could call statistical structure, topological structure, but it's basically how words are related to other words, just spatially.
Where do they show up in relative to one another? Based on learning those patterns, they're able to produce what looks like complete, total linguistic competence.
They can talk about whatever you want them to talk about, and they can do it in a way that is fundamentally indistinguishable from humans at this point.
So, LLMs show language operates without external reference, or a world model.
Now, what they don't get access to is pictures, they don't live in a body that moves around, they're not embodied, they don't experience the world through any sort of sensory processes.
All they get is actually digitized numerical versions of corpuses of text.
Okay, so that's a starter, they show the language can operate, I should say can operate, but language can operate without external reference to, or a world model, right?
They don't have a notion of objects, or redness, or any of those things that, the sensory conceptions that we have.
They don't interact with the world in the way that we do.
Now, language is, here's the second piece, language is an auto-generative behavior coordination tool, operating system.
Now, this is a big claim, but what I'm basically arguing here is that what we're doing when we talk, and when we tell people things,
is we're not actually relating anything about our inner world models or our experiences, but we're simply coordinating.
We're using this as a system to do things, like if I ask you, could you go grab me a cup of water, there's a, you know, there's a water dispenser over there.
That's gonna allow you to go and perform that behavior.
Um, and it's, this, in order for language to do this, we've come up, language has emerged as this, this objective, it uses objective terminology.
Like, there's a, there's a, a water, a water dispenser over there.
Um, when I said there's a water dispenser over there, there isn't actually any specific, I'm not describing any sensory experience to you.
I'm simply giving you information that you can then use, behaviorally, to go and, and perform some activity.
Now, phenomenal, the thing we call subjective experience, the thing we call qualia, consciousness, arises from a completely different process, and that's a sensory process.
The sensory process depends on, uh, embodied, and what I call pre-linguistic processing.
So it's, it's things like your eyes, the transducing light, uh, and then producing a neural signal that, uh, is, is transduced many times over, uh, converted into other, and, and converge with other, uh, kind of sensory processes.
And also coordinated with our body, as we move around, we see different things, there's a perception, an action loop.
All of this is pre-linguistic, meaning that there's processes that are happening that then tell us what to say, right?
I, I, I can see something and tell you, uh, based on my sensory process, what I'm seeing.
But the sensory process itself is completely and totally opaque and obscure, it is not available to language.
Language is this self-autogenerative system, it does its own thing, it has its own reasoning and own logic, it does its own thing, um, and it serves for the purpose of coordination,
but not in order to confer, convey information about my internal sensory process.
And that's basically in a nutshell. So language is not capable of conceiving of the subjective.
Subjective depends on embodied physiological processes to which language has absolutely no access,
because language is not meant to do that. Language is meant for humans to coordinate with each other in the objective, uh, framework.
Okay. So that's, that's the shell in, uh, that's, that's the, uh, the talk in a nutshell.
And so it's the answer to whose problem is, is the line, part of it is language's problem.
The language, language trades from things like atoms and physics and the material objects that are external and brains and all of that.
And these are linguistic constructs that we as humans have come up with.
Dogs don't know about atoms, they don't know about cells, they don't know about an external world.
They do know about seeing and hearing, they can experience it, but they don't have the problem.
Because they don't have conceptions that are purely linguistic conceptions. I saw a hand.
I just, real quick question about language and speech in your example.
Is them not hearing or seeing text a sensory observation?
Absolutely. So that's almost my last slide.
But what the heck.
Terrible, terrible idea to do this, but here again.
Do you want to take questions during the thing, or?
Yeah, yeah, I'm cool with that. I'd like people to, if they need clarification.
So yeah, I can, there's a lot of coordination between these two systems.
And if I say to you, imagine a snail on top of the upper stapling, you can create a mental image of that.
And language certainly interacts, and the other way as well.
You know, when I come into the room and I see everybody standing here, I can create a linguistic representation of that.
And so these things are certainly communicating.
There's certainly, I call it an end point.
So there's a sensory process. Lots and lots going on.
All this transduction, all of this physiology.
Then there's an end point where it's like, and now I'm going to let the linguistic system know what's going on so that you're going to be able to communicate.
But there's much, there's an iceberg right underneath that tip.
The tip of the iceberg is maybe the thing that language has access to.
The iceberg is all of the sensory processes that language has absolutely no access to.
And it's not that there's no access to it. It's of a fundamentally different point, different sort.
Like Eli was mentioning, right? It's embodied. It's a very, very important point.
Language is inherently not embodied. Language is meant to be objective.
It's meant so that we can all agree. There's a table over there.
That's not a statement about my sensory phenomenon that lead me to that conclusion about there's a table over there.
We all agree there's a table, but for you the table being over there is very different than for you the table being over there.
And language is meant to form this objective, this publicly observable kind of, you know, view from nowhere truth that is fundamentally quantitative,
that's computationally different than what it is that's sensory processing, which is telling me there's a table over there in this depth from me.
It has these sensory qualities so that I know it's a table. All of these things that are necessary to get to the point that I can communicate that.
Also, by the way, necessary to do lots of other stuff like walk over to the table to have an idea.
To have an idea of what the table would feel like if I touched it, all of that stuff.
But at the end of that process is also the ability to communicate.
Did I answer your question? Okay, awesome.
Yes, again, so feel free to jump in. Like I said, if I don't get through all my slides, no big deal.
So, yeah, in a nutshell, yeah, go ahead.
I was just going to say, you know, I couldn't help but think as you were talking about this about Richard Dawkins' memetics
and the idea that the human mind may be in a sense almost a petri dish for ideas that then use that to spread itself.
Yes, you're jumping right.
We talk about language in many ways as something contagious and the ideas that sort of talk about ideas going viral and things like that.
Yeah, that's a great point. And, yeah, the funny thing about language is it exists in societies and cultures.
It doesn't exist inside of us. And there's something, there's a very, that is almost definitely part of the fundamental divide.
We experience, right, it's like the old inverted qualia problem, you know, it's the married problem.
It's the same thing. You can't really explain to a blind person what it is to see.
I can't tell you what my red looks like. Maybe my red is your green.
But we can both agree that this is, you know, I don't know what color is that.
That's a blue tablecloth, right? But we don't know internally.
So it's a construct of communication and it's designed by societies.
And that means it's sort of running in us, but it isn't precisely us.
And I think the mean, you know, kind of, you know, ideas, they're, and let's just go back to the first line.
One of the crazy things about language is it's auto-generative. What do I mean by that?
I'm going to walk through auto-aggression in just a second.
But that means language generates language.
What neural networks, what the AI that you've heard about is doing, what it has learned is not how to generate language.
What it's learned is that they are contained within any given sequence of language, a meaningful one,
is a prediction about what the next token should be, about what you should say next.
And that's all that AI really is.
There's nothing really that clever in the neural networks, the fundamental tech that is driving these things.
What turns out to be really clever is that language contains within itself this ability to auto-generate.
And that means thoughts, is Will here?
I don't know, he's not here.
My colleague William Hahn coined the phrase that I often use, which is, it's the thoughts that do the thinking.
So language just generates itself, right?
All the AI is really doing is just making this prediction that the language already mates, that internally contains within itself.
And so that's where the mimetic ideas, I don't think Dawkins even quite meant it at this level, because he wasn't taught, he was before the age of LLMs.
But it's actually really makes the idea much more powerful, that ideas actually self-generate.
The ideas are what creates, create themselves in some sense, or at least elaborate on themselves.
So are you going to comment that meaning precedes language?
Meaning and language are separate.
So meaning is a loaded word.
Because a child can have a meaning with no language yet, right?
I have a six-month-old and a year-old grandson, and he knows meaning everywhere.
And he can express it, not verbally.
But language, something he adds on top of the meaning later.
Exactly.
And meaning is a slippery word in any, you know, there's the famous Chinese room, that kind of discussion, if something could just be purely symbolic, does it have meaning?
I think, you know, the concept of meaning, I think, is very tied to this conversation.
I think what meaning is to a lot of people, if I say, do you know what a share means?
What it means to say somebody's in a share, they're thinking in terms of sensory.
They form an image in their head, or they're able to name that.
And again, it's the same dichotomy.
I don't think language, per se, is referring to that sensory process.
I think it can generate that specific sensory process in this person, generate a very different one in somebody else, have a completely different...
So language and meaning, in a sense, are really a fundamental difference.
Yes, you're touching on a key idea.
AIs don't really have meaning, so, you know, it's like kind of separating the two concepts.
Well, so what do AIs have and what don't they have?
So let's talk about that real quick.
So yes, language is from Mars, sensation is from Venus, that's just a throw-in kind of...
But these two systems don't understand each other.
The sensation and sensory perceptual system is actually non-linguistic.
And I think we see that left brain, right brain, kind of you see in split brain, kind of experiments that there's a feeling thing, a feeling system.
It's very sophisticated.
It doesn't really mean that it's not pejorative, but it doesn't think in terms of...
In the same terms as language at all.
It doesn't think in objective terms at all.
It only knows sensation.
It only knows the personal and the subjective.
It doesn't think of, for example, the future, in a long-distance future.
It can't perceive that directly, so it doesn't have a notion of that.
That's a linguistic construct.
It doesn't think about the objective.
It doesn't think about atoms or even things like frames of reference, that you can see something.
I can't see none of that.
Okay, so these are two different systems.
They don't understand each other.
And my argument, in a nutshell, again, is that that is where the mind-body-mom sort of emerges from that.
That the reason we have a problem is because language and all of science, all of rationality, everything we think that humans think,
the knowledge base that humans have built over these millennia is constructed of language.
And the fundamental gap is that that entire knowledge system that we've constructed is fundamentally ill-purposed,
it's ill-fitted to thinking about the underlying subjective that it doesn't have access to.
So, where do I have?
Well, if you're going to do mixed Q&A and you have...
Okay, let's call this mixed Q&A.
You have like 12 minutes left.
Okay, that's great.
So, I just want to start.
Believe it, the original insight that led me down this path happened at last year's MindFest at a party, if I remember.
And there was no particular event that happened.
I can't explain why.
But that's just how it happened to be.
But it was this big aha moment that all that AI is doing is churning through patterns of what to us is meaningless, right?
There's no meaning in the text.
It's like, it's literally as if you don't speak English, but you're really, really smart.
You're savant.
And what you're doing is you're just studying these patterns, and you've noticed that the word the is in this relation to the word, you know, boy.
And the word boy is in this relation to the word girl.
Again and again, you've noticed this pattern.
It's kind of queers.
And when those are in relation to each other, they're often in that whole phrase in relation to some other words.
But these are just meaningless squiggles.
And we have to think of the LLM as just ultimately dealing with meaningless squiggles.
It doesn't have any notion of external reference, meaning that the word boy refers to some, you know, concept that we think that that word does.
Now we, of course, typically we assume that our linguistic system is different.
The LLM's doing that, but it's not really doing language.
And so that would sort of be the counterclaim.
It's like, well, yeah, but that's true of LLM's.
But maybe it's not true of us.
So the part you're going to have to, I'm going to have to go through quickly, is the argument that we are LLM.
Or not, and I don't mean we by, I mean not the holistic us, because I don't mean the embodied sensory system, because I'm claiming exactly not that.
But rather, the thing that's standing up here doing the talking right now, the thing that's generating language, is fundamentally doing the same thing.
So I just want to tell you real quick, and it's, it's, I don't want to get too much into the weeds here, but it's really, it's useful to understand in some ways how dumb these systems are, how simple they are.
What they're actually doing at the bottom level, and then you'll see that there's no ghost in the machine.
There is no there there.
There's no meaning in there.
So, so how does, how does, so there's so-called autoregressive next token generation.
I, I, I acronym that is aren't, which I think is kind of useful.
They aren't the things that they claim to be.
And with the next token generation, they aren't actually referring to anything.
And so here's how autoregression works.
Autoregression works like this.
You have an input sequence, and you have some function.
And you take an input, you put in the function, the function produces an output.
Then you take that output, and you put it together with that previous input sequence.
And so now we have next input sequence.
We stuck that output onto it, and then we feed it in again, to the exact same function.
And then we get a new output, because it's a different input.
And then we do it again, and again, and again, and again, right?
And that's, this is this next token generation.
That is what, when you watch your LLM generate, if you, you, you, and you see it, you're spitting out words.
It's doing that in real time.
It's spitting it out, and then it's taking that word, sticking it out to the previous sequence, the entire context, spitting out the next word.
Okay.
So that's, that's autoregression.
Now what, what can you do with autoregression?
Here's a famous case of autoregression.
Anybody familiar with this particular sequence?
Has a name, Misha?
Fibonacci.
Fibonacci, right?
So how does Fibonacci work?
So here, it's a very, very simple function.
So you take, you take a sequence that you have, you have a zero, one, and then you just add those together.
Uh, zero, one, you get one, and then you take that output and you stick it on.
Now you would, you leave this one behind, uh, and now you've got one and one, because this output was stuck onto that one.
And you've got, and then you add those together.
Simple, simple function, add.
And then you, and then this thing has really cool properties.
It's actually pretty complicated what you end up doing, what you end up with.
Um, but the, the sequence isn't in the system at all.
It's generated autoregressively through this process.
Now, the same thing is true in your large language models.
They're doing the same thing.
Um, so I, you know, here's my prompt.
A vivid, expressive, and sensory discernment, biting into a juicy orange.
Um, now did I actually walk through this one?
I don't even know.
This is my second slide.
I don't even talked about this.
Here it is.
Uh, yes, right.
So this is the quote I wanted to share.
Right, so I, I told, I told, uh, I think this is Claude.
Um, you know, give me a first-person experience of, uh, of biting into a juicy orange.
And here I think I still tasted the way the skin tore under my fingers, releasing the sharp,
citrusy mist that clung to my hands.
You know, Claude's a liar, right?
It doesn't know about any of this stuff.
It's never experienced this, but oh, oh my God, right?
This sure as heck sounds like something that I experienced it, but when you look under the hood,
it's just autoregression.
What you've got is a model that's been trained, if you've got this sequence, tell me what
the next output's gonna be.
So I said, here's a prompt.
A vivid, expressive, and sensory description of biting into a juicy orange.
And this output is, I can still taste it the way the skin tore under my fingers, right?
So, let's see it happen.
A vivid, expressive, and sensory description of biting into a juicy orange.
Now, in the real Claude model, there would be a little special token here that would say
that's the end-of-user input.
Okay, we'll leave that out, but then tell me, Claude, what's the next thing you should say?
And all it gets is, ah, uh, oops, it's off-screen.
Is it here?
I don't know.
That's it, right?
But it's gonna start with the word, the same word, ah.
And it's not thinking about the rest of it.
It doesn't have a mental concept of what biting into a juicy orange is like.
It's just doing this dumb squiggle-to-squiggle prediction.
That given these squiggles, this is the next most likely squiggle.
Uh, uh, or I'm sorry, I, um, and then stick that onto the end.
Again, now you have a good expression of the orange.
I take that entire sequence, put it back in.
I can, right?
And then I can still taste it, the way the skin tore under my feelings.
And lo and behold, you run this thing, and you end up with something that sounds like
it has a complete world model of what an orange is, and it's thinking, even as it's talking,
it's thinking about it, and it's gonna explain the whole thing to you.
But no, it didn't do that at all.
It's just a function, it takes in a sequence, and it produces one single output.
Then you take that output, you put it back, and does it sort of aggressively, and lo and
behold, you end up with language.
So, this was my reaction when I finally understood that this is what these things are doing.
How could that possibly be, right?
How could the, all the expressiveness, and all the conceptual, all of the meaning that
we consider to be fundamental to language be due to this next token generation?
But I've come to terms with it.
And the answer is that LMs aren't doing what we think the language did.
It's not what we thought language was, or at least what I thought language was.
My assumption about language was, we sort of have a conceptual framework of the world.
Maybe we've loaded it into our minds, into our heads, but we are referencing it when
we are talking.
But what we're actually doing, and again, my claim is that what we are doing is the
same thing as these language models, is really just next token generation.
It's predicated on sensory processes, granted, but in the end, what we're really doing is
just saying, here's a prompt.
I'm sitting in a room, a bunch of people are standing and looking at me, and I'm holding
a microphone in my hand.
I better say something, right?
And that's the prompt.
And that's the prompt.
The prompt is, and my autoregression is telling me to say all this stuff that I'm saying to
you.
Five minutes?
Okay.
So this leads us to question, how does this language work?
If it's just this autoregressive generative system, how does it actually work as a coordinating
system?
How am I able to say to you that there's a water pool or a water dispenser over there
on the right side of the table?
And that leads you to, you know, if I ask you to get me some water, I'm fine.
I need some water.
But if I were to ask you to do that, how does that actually lead you to be able to perform
the activity?
And the answer is that because it's generative in this structured way, that it actually can
do the things you want it to do.
It's all in there somehow, in the corpus, in the structure of language as it's been expressed
by humans over, you know, this millennia, is actually the sufficient information so that
if you run autoregeneration, if you run the predictive engine, it does the sensible thing.
So here's just an example.
Here I gave this a claw.
Imagine a rectangular room with a large window.
I'm not even going to read the whole thing.
There's a bookshelf in the corner, a green armchair.
How would you navigate from the armchair to the bookshelf?
And it gives you a reasonable and structured answer to this question.
Using just next token generation.
And so that means this language production thing that you're able to do just from prediction
is actually sufficient to come up with a plan.
That means if I conveyed to you that this kind of information, I want you to do something
for me, then you're going to be able to run that language in your brain to come up
with an actual plan and then behaviorally execute.
So language is able to both auto-generate and also operate.
I think of language now not even as a communication system.
That's what I think historically we think of it as.
I tell you that there's something over there and now you know it.
And you've incorporated that into your world model.
Another way, but a different way to think about it is it's an operating system.
By giving it this code, I've placed it into your mind, that is going to have impacts.
It does things.
It generates other language.
Oh, you said that there is something outside that I need to get.
Well, I can think about that.
I need to go through those doors and I need to retrieve it from outside.
The language I gave you generated the instruction set.
And so this is basically thinking about generation as being fundamentally productive
and ultimately not only productive in a linguistic sense,
but ultimately productive in a behavioral sense.
So language is a coordination system, not a representation system.
It operates using objective constructs.
And this is again, this is where my body problem really comes from.
It has to be, it's not my redness.
You don't care about my redness.
We just have to agree that thing is red so that we can both do the same thing
with that red object versus the blue object.
And so it created, you know, and of course as language emerged,
it emerged collectively such that we were talking about things we can agree upon,
no access to the underlying subjective processing.
Okay, yeah, and I think I've already mentioned this.
I'm not going to spend time on it.
But there's all this other stuff that happens in the body that guides our behaviors
and that's what we call sensory perceptual processing.
And those are, as I said, they're completely out of reach except at the end point
to the linguistic system.
And it's bi-directional.
And as I mentioned, perception certainly influences language.
Language can certainly influence perception.
But never the twain shall meet.
They are two submerged icebergs.
And they're calling out to each other from the tip.
But they do not have access to what's submerged.
Yeah.
So if you could make a large language model in a body that has sensory stuff,
would you call it the same as us?
So I'm not, I, you know, there was a conversation,
and there's many conversations about, you know, whether AI could ever have consciousness.
Here's my view on that.
If we create an AI that has sensors and it's embodied, you know, it's doing light
transduction and it's using that to make decisions, it could be that, that could be conscious.
My own view on this is it might not be because there's all these other sort of analog kind
of processes that, these physiological processes that may, they're still the primary.
If, oh, maybe if they're purely electrical.
If they're digital, I'm not so sure.
If, if they are sort of analog electrical, then maybe.
But what I'm, what I'm claiming strongly is that the language models themselves are not.
That they are talking about these things.
We can see the whole thing, right?
You just look around in there.
There's just no room in there for any of the other kinds of information.
And I don't just mean like maybe there's some emergent thing.
No, the information is just not in there.
It simply isn't in there.
There's no way, there's no, the generative process is the only thing there's room for.
There's no room for other kinds of information to be latent in there.
And the generative process is sufficient.
We see it sufficient.
So that means you can't hide other kinds of world information in there that would lead
to sort of quality or something like that.
So I would say clearly, I feel very strongly that language models, despite, you know, they can,
as somebody mentioned, like they may claim that they're feeling.
But the only kind of feeling that we have any awareness of, that we have any reason to believe in,
is the time that begins with sensory processes.
And so I think we should, and we know that they can lie in a sense.
We know that they can sound like they've experienced vision and an audition.
But clearly the language can carry that and support that without any of that other stuff in there.
So, actually on that note, we have to keep it tight.
We're time?
Yeah.
We're at 25.
Okay.
So if everyone could join me and round of applause.
Thanks for it.
Thank you.
