Hey everybody, it's Ilan Barinholtz. I'm a professor at Florida Atlantic University and I'm working on a generative theory of cognition in the brain.
And it's all based on large language models, the things that underlie, chat GPT.
And the core idea is that the basic math of how these things run isn't just a metaphor for thinking about the mind and the brain, but it is actually the fundamental core algorithm that is what our brain is running.
So it's not even about explaining the brain, it's about what is the hardware of the brain, what's the software, what's the algorithm that is substantiating.
And what I want to talk about today is this idea of complexity, which riddles the core enterprise of building machines that can do smart stuff.
And it's been, it's an intricate and complex mathematical field that tries to capture what computer systems are and aren't going to be able to do and in what time.
And the notion, the framework, the computational framework is based on something like a Turing machine.
And what I think we've learned in the last couple of years is that we can definitively say that that's not how the brain works.
And then the question then becomes, okay, well, what lessons from computationalism actually may have relevance to thinking about cognition?
So I'm obviously going out on a, in some ways, on a theoretical limb, but I don't think it's, I think it's a very firm limb.
I don't think it's going to snap by saying that human cognition, at least linguistically expressed cognition.
Like what I'm doing right now, all philosophy, science, ultimately math, math is going to be a tool used by this linguistic system.
But all of it is going to be folded into an autoregressive model similar to what large language models do.
And if that's the case, then the kinds of questions we were interested in asking in relation to computational systems are still going to be relevant, but only in regards to symbolic, sort of classical computational systems.
In other words, the computers that we have and the way they operate and they run is still going to be an area for complexity theory to operate.
The question in my mind is, okay, are there lessons learned there that could potentially fold over to the human mind slash brain?
And I think certainly at a high level, there are commonalities, but then there are very distinct differences.
And maybe those are what are most interesting in some ways in thinking about this contrast is that the brain doesn't break.
The brain is not a computational system in that what happens at time step, you know, T plus 10 isn't necessary for doing time step T plus 1.
And therefore, there isn't this contingency break in this brittleness that's inherent to sort of symbolic computing, where you have to get to the end of the argument to find out what the argument was, such that you can now say, what am I supposed to do with this input?
The auto-aggressive model says there's one rule, one update rule, and that is next token, and there's no contingencies built in.
And that means it's an extremely simple computational algorithm, first of all.
It's simple in the sense, first of all, you can write it down very simply.
But that, of course, you know, common goal of complexity or something, what you've got here is probably easy in some ways to instantiate in hardware because of its inherent simplicity.
But I think even more deep than the simplicity is this unbreakability.
So, you know, computer programs hang, and they loop, and that's because of this inherent brittleness of this contingency, these dependencies, that it goes beyond just next token.
But if you can solve things with next token, that means you can't F up the system too hard.
Because what can happen is, like a stream running down water, it's going to hit a rock, it's just going to go around it.
And then that's what the stream does, and it's computing that.
And so what you throw at it, and that's, you know, all of the context, whatever is going on in our sort of computational environment that goes into our input,
is what shape the stream is going to take, but that's called computing.
And there's no such thing as the stream saying, wait, I wasn't supposed to do that, go back up and start over.
No, the stream runs.
Now, in the case of cognition, in the case of human cognition, of course, you've got to think about functionality.
It's got to run and do something.
The stream is computing something, of course, and it's not just defined within the stream.
You can build an autoregressive system that spits out pure nonsense.
Well, I mean, of course, take any untrained transformer, and it will do something.
And it is autoregressive, and it's doing the computation, so to speak, it's supposed to do.
But that's like a stream that's, and maybe the metaphor is going to break down real fast, but, you know, a stream that's just going off in all different directions,
and isn't sort of actually computing a shape of a trajectory, which is, you know, we can think of as a stream as doing.
We want our autoregressive computation to do things, and it does do things.
And, of course, that's the essential question.
How do you build something that does stuff, and not just language, by the way, but maybe other stuff, too, in this fail-safe way?
I think that's the question, the new question, I think, of psychology, of neuroscience, of, I would say, even philosophy.
Whatever this amalgam is that's trying to figure out the nature of thought and computation,
is how does such a system that's just doing next token, mind you, you've got the residual, you've got the context,
and you can represent that in very rich ways, but you're still doing this kind of next token.
How do you get utility out of that, the kind of utility that we know we have?
This is what the new science, the new field has to do, has to figure out.
How do you build things out of that?
But I think it's a very key core insight that there's an anti-brittleness inherent in this kind of building system this way.
And so this is what evolution, nature, information itself, computation itself, sort of converged on as being the solution.
One of the deeply challenging outcomes of thinking of the mind auto-aggressively,
is that it really kind of destroys the common sense notion of a memory.
And it's so deeply entrenched in us when I think, you know, what does your mother look like?
What did you do last summer?
We qualitatively have this experience of, we're calling something and somebody brings it to us and it shows up in our mind.
But it's already there.
The image of your mother or some sort of video of splashing through the waves.
These are recalled and we sit and watch them, like, sort of passively.
But that's not what's actually happening.
What's happening is we have this, we have this capability, we have this engine that's able in the moment to generate that image of your mother on command with, you know, the appropriate input.
You say, hey, visual imagery system, give me an image of my mom.
And it generates it in that moment.
But that image doesn't exist anywhere in the system.
It's not there.
It's not in your brain.
Even if I could completely decode your brain, unless I was able to run it with that input, the image isn't really there.
And so memories in some ways aren't real.
They're not the things that we kind of intuitively feel that they are.
But at the same time, they're very real in the sense that we can generate them on demand.
Sometimes these are memories that we cherish.
These are things that we want to be able to pull out of our kind of mental time capsule and look at it again.
And the idea isn't that your mom's face is forever lost and it isn't really there.
It's an illusion.
It's that the ability to generate that is what it means to remember what your mom's face is.
Now you can picture your mother not just in a single kind of front-facing, but picture from the side, picture her making you breakfast as a little kid.
You can do whatever you want with this because it has this kind of endless potentiation.
But this kind of raises the question, okay, thinking about it this way, thinking about memory this way, what if there are memories that I want to be able to generate later?
Or a better way to put it, it's not a memory I want to be able to generate, but what if I want to have certain kind of generative abilities later on?
Instead of thinking of it, how do I store something now that I can later retrieve?
It's how can I ensure that I'm going to be able to pull it up on demand later on?
And so let's say there's something that's really important to you, really precious, really valuable to remember.
Is there a way to encode that so that you have a hard probability of being able to later generate it?
Now, of course, we know that there's a vast literature on this.
There is a substantive, empirical, scientifically established literature on how to encode information such that you can later remember.
It's basically the entire field of learning.
How do we learn information such that we're later able to retrieve it?
But the characterization, thinking about how information actually is encoded and then later used,
has almost certainly dramatically influenced the way that this field evolved.
So many of its findings, of course, are going to be valid.
If you learn better under one circumstance but the other, and we can measure that in later job performance, later academic performance, that's real.
I'm not suggesting that all of the tools that have been designed are no longer valid.
But what I am suggesting is that there may be other kinds of tools, other kinds of frameworks that are sitting right in front of us that may be obvious if we think about memory in this other way,
if we think it as a generative process rather than as a retrieval process.
So that's something I think I'd also like to explore going forward in a research framework.
And what does that look like?
What does it mean to think of memory not as a storage retrieval process but as a generative process?
How does that change how we educate?
How does it change how we ourselves come into contact with and digest and process information and feed it to ourselves?
And maybe also, what can we do in our minds?
What sort of cognitive kind of frameworks that we can actually induce autonomously, autonomically, internally might be available to us?
Thinking about processes this way.
Because it's a much more dynamic thing.
If you're thinking about how do we just store information?
Well, we can dress up the information in certain ways.
We can package it.
We can organize it.
That means once it gets in there, you're done.
And hopefully you'll remember it later on.
But if in fact what we're talking about is a much more continuous generative process,
then there may be far more actual tools and points of intervention at our disposal
than thinking about it simply in terms of cold storage and retrieval.
So there's this big question, why is math so effective in making predictions and physics and why in general?
Well, physics is really just a mathematization of physical observations, predictions.
And why is it so unreasonably good?
And I think the thing we're kind of missing is that math and all the derivative kind of subjects that extend from it
is a human artifact that is couched in based on sort of the same, what appear to be the same core principles as language.
Of course, it is itself math and language.
It's saying we're going to, what it's doing is self-consciously doing symbolic representation.
But where did we get the idea of symbolic representation?
Well, of course we got it from language.
That's a cat over there.
Oh, there's this thing.
We can write it down.
We can say it.
And it picks out an object there.
And then we can string these things together to have some representation of the world.
And, you know, math, of course, borrowed this idea from the observation from natural language
that you can represent things like, you know, numbers using certain kinds of symbols.
And then if you string them together a certain way and then run certain kind of operations on them,
the thing that speaks back out is going to be some fact about the world as well.
Well, abstract mathematics ends up not worrying about any sort of actual natural kind physical representation
that the numbers represent necessarily that the symbols can just be abstract entities in their own right.
But the whole idea of symbolization, playing around with symbols, of course, is borrowed from language.
But I think the unreasonable utility of it is because it's actually derivative of the much greater power of natural language.
Well, let's start, actually, let's try to dissolve this kind of divide between long-term and working memory
and thinking about these as distinct systems.
Because I don't think that's how it works.
I think we now have a very good model of how it works, and it doesn't actually subscribe to this poor notion
about what memory is and what it's doing, and therefore this divide.
And then let's think about what the implications might be in relation to what we're actually trying to do.
Because, parenthetically, I'll say that our notion of memory
is very deeply tied up in, believe it or not, even if you've never thought about it before,
in a sort of computational framework.
The idea that we're storing a bunch of stuff for later,
and then where we're going to go again, we're going to pull that stuff out and remember it.
And I think that could have very broad implications for thinking about
what it would mean to enhance, let's not even call it memory, enhance our performance.
What do we really want?
We really want, in the moment, to do the right thing, to do this sort of functional thing.
And if that requires retrieving information, as we would typically think of it,
we want that to work.
We don't really care if and when and how it's stored.
What we really care about is runtime.
We want to be able to do the thing that we tend to think memory supports.
We want to be able to generate meaningful responses to questions.
Some of those questions require certain past information making its way into the current moment.
And at that level of generality, everybody wants to enhance their capabilities.
But if you think about it from the wrong standpoint,
you might not be thinking about how to do that enhancement correctly either.
Because if what you're thinking of is, like, how do we appropriately store facts,
store information, and then later on have the right tools to get that information back,
well, that's going to come with certain kind of regimens,
certain ways of thinking about how to solve the problem of enhancing the later performance.
My argument is that that's fundamentally, that's just actually the wrong model.
It's the wrong model for thinking about what the actual process of generating the behaviors we want.
Like, when I'm talking to you now, and I'm talking about large language models,
and I'm talking about memory and enhancement and all of that,
in some ways I'm obviously using information from the past, right?
That information is, well, this is what we're talking about, is the conversation.
And I know what a large language model is because I read about it, and I programmed them.
And it's certainly true that that information is coming through in what I'm saying right now.
But that doesn't mean that we have to think about it in terms of storing information in this static form.
We want to produce the behavior.
However, we have a model now that produced the behavior that doesn't store information in this way.
And at runtime, it's not retrieving anything.
So what are the implications if instead of thinking about runtime performance as storage and retrieval,
but just thinking about the performance itself and thinking about that there's this stream of activity that's happening cognitively.
We can think of it as computational level.
We can think of it at the mental level.
It may have some overlap even with sort of conscious thought, but that's for a later conversation.
But the process is one that we can model in terms of influence rather than memory.
Past events, things that have happened in your long-distant past, things that have happened in your more recent past,
are influencing your current generation.
Can we call that memory?
Well, sure, in the sense that the system changes in the past and that affects its performance later on.
But is information stored in the traditional sense?
No, because in a large language model, in an artificial neural network,
what you've got is just changes in these parameters, changes in the structure,
if you want to think about it that way, of the network.
And those changes aren't actually representing some specific set of facts at all.
What they represent is, given a current input right now, what are you going to do about it?
And that input could be, what is the conversation we're having right now?
Well, it's a conversation maybe about large language models and autoregression as a model.
Okay.
What's the core thesis?
And I could answer that question.
What's the 10th word that I said, you know, when I started the conversation?
I can't answer that question, but that's yet another thing I can do with this information, right?
In other words, what's happened in my recent past has an infinitude of possible outcomes.
And so we can't think in terms of a single static fact that's going to somehow end up coming out of my mouth.
Instead, like an neural network, we're changing things such that we have now a completely different trajectory of how the system is going to go.
So from a metaphor standpoint, instead of thinking of sort of a computer program or a storage retrieval process,
the metaphor that keeps coming to my mind is sort of a stream that's running,
and it's doing stuff along the way, useful stuff.
But if something interacts with that stream, it's going to change the shape that it takes.
And it's going to do so with something like what we'd call memory.
But it's that the information sort of, it sticks around.
It sticks around in the system.
The previous words I said, for example, are influencing what I'm about to say.
And they're still doing it now, right?
The words I just said, you know, 10 words ago, now it's 15 words ago, now it's 20 words ago,
are influencing what I'm going to say.
That's what working memory is, in my view.
It's not that there's information that we're going and retrieving.
It's that the past history, and it's non-Marcovian,
the past history is continuing to have an influence on what I'm doing now.
And so from a standpoint of sort of enhancement,
and thinking about how we can improve our runtime performance,
what this means is we have to think in terms of how do we nudge the system?
How do we move the system in a direction that gets us where we want to go?
How do we set up sort of the preconditions?
Or we shouldn't even call them preconditions.
It happens in real time.
What can we do to the stream such that it takes the shape, a more optimal shape?
And this is a very different question than thinking about
how do we store certain kinds of information with fidelity
so that you can later retrieve them accurately.
It's how do we encourage the right kind of influence?
And so I don't have a clear idea as to how you actually do this kind of manipulation.
But I think a paradigm shift could lead to completely different ways
of thinking about these kinds of tools.
What thought tools might exist already?
Or what's at our disposal that can help us shape that kind of ongoing process?
How can we experiment with that?
How can we introduce information somewhere on the stream?
Where should it be?
You know, is it a minute in that we should introduce some concept
that would end up showing up in later ThoughtStream to,
and we don't want it to be one specific concept, of course,
but given a particular task ahead of, let's say,
you're trying to get somebody to, you're giving them some instructions,
and you later on want to produce and be able to carry out some task
or explain what it is you told them, right?
These are sort of very basic sort of memory-based tasks.
What can we do along the way to potentially influence the outcome
as being in the direction you want?
What would enhance, quote-unquote, understanding?
What would that mean?
Well, we could show that this means
that somebody's able to produce the task later on
more efficiently, maybe more dynamically,
maybe more flexibly,
so that if something comes along, it doesn't throw them off.
The stream is able to bypass it in a more effective way.
What do these tools look like?
What could they possibly be?
How could we experiment on them?
So I think these are new kinds of questions
that we can contemplate now,
hopefully use some of the existing tools we have
in, say, even in the meditation
or in just basic mind training,
whatever, even your traditional educational toolkit.
How can we take these tools
but now use them for actually a radically different task?
So that's, I think, something I'm very interested
in exploring and understanding and experimenting with.
And the cool thing is we can do these experiments cheap.
We can kind of run classic experiments.
They would not be, by the way,
short-term retrieval memory kind of experiments
because those are stupid
because what you ask people to do
is to explicitly retrieve what happened in the past.
And the whole point of my theory
is that that's not actually what we do.
We're influenced by the previous past.
We don't go and retrieve it.
The reason why people studied that
was because they were kind of looking where the light is
and it's a very, it's a simple thing you can measure.
But it's not a particularly useful thing to measure
and it's not a particularly useful thing for people to do.
What we really want from memory is to be functional.
What we want from memory is to do the right thing
in this circumstance.
In 99.999% of cases,
that's not to explicitly recall
and retrieve information from the recent past
in some form.
It's to be influenced by it.
The things I'm going to say now
in this part of the sentence
are meaningful and smart
because of the beginning of my sentence.
And so we need that influence to come through.
And so we shouldn't be thinking
in terms of measurement of explicit retrieval
because that was actually
a red herring in the first place.
What we need to be doing
is thinking in terms of the kinds of retrieval,
but what it really is,
is runtime, efficaciousness, efficacy.
In doing the kind of stuff
we want people to actually be able to do
with memory.
And then we measure that.
Like, okay, here's a set of instructions.
Now, carry out the task.
Go, right?
Quickly as you can.
Okay.
What if we repeat the instruction?
What if we inject something
during the instruction
at a certain point?
How does that either,
how does that influence
negatively or positively
the actual capability
to keep going
with the instruction set?
And we could call this understanding,
but we can now see
that that's not,
that's too vague
and also too specific a word, right?
There's variance.
There's degrees.
It's not like you understand
or you don't understand.
The extent to which
you're able to finish
a sentence meaningfully
is not binary.
Like, you either completely forgot
what you said in the beginning
or you've got it
in pristine form.
That is influencing
what you're doing.
And we have to,
so we have to have softer measures
that are able to measure
sort of efficacy in this way
that is not dependent
on sort of this binary,
okay, can you retrieve it or not?
That's far too crude
and frankly,
a misguided measure.
So that's what I think
is maybe a really interesting project here
and it's twofold.
One is to maybe rethink
a little bit
the kind of things
you want to measure
because that's not
what's historically,
certainly measured
in the so-called
short-term memory span window
of time.
But that's a really,
really important span.
It's like finishing your thoughts.
It's being able to compress
in real time
and these are all conjectures
so I don't even want
to commit to this
but something we're doing
right now
when you're listening to me
is you're compressing this stuff.
You're not representing it
as the explicit tokens
that I'm saying.
You're doing this thing
but you're doing it
in such a way
not that you can retrieve it,
right?
Because what the hell
did I say 30 seconds ago?
You have no idea.
You can't retrieve it.
But I promise you
it's influencing
your understanding
of what I'm saying right now
and your ability
to then respond to it
as long as you know
if you know
what I'm talking about.
If you've lost your weight
entirely I apologize
but if you're still
following me here
then the influence
is continuing
and so we have to measure
this in a new way.
Psychology needs to think
of this in a new way
in terms of measurement
but that's number one,
right?
Measurement,
capturing what this thing
really does
even specifically
let's just think of
what historically
it's not the short-term
memory window
but let's
we'll start with that
we can elaborate it further
but that's number one
how do we capture it?
How do we measure
the really
the interesting
the real thing
that we wanted to do?
Well first of all
that it's meaningfully doing
and then second of all
that we wanted to do
and then once we have
what we wanted to do
now we can get to
the hard work
and maybe
I think it's
in some ways
more difficult work
of okay
what can we do about it?
What do we
if we have the right measurements
what are the possible
interventions
that could be helpful
in doing it more effectively
in doing it
more efficiently
and
but you need to
you need to solve one
before you solve two
to some extent
but you can also
try to solve these
in concert
don't wait
just try to be
more effective
right away
we want
we want people
to be able to
understand information better
so they can perform
some tasks
or explain it back to you
that's not hard
to measure
we can start with that
start with pretty crude
measurements
did this person
was this person able
to perform this task
based on my
narrative
description of
what they're supposed to do
and then
mess around with that
and see if there's
a kind of
measurable
testable
quantifiable
and
and
settable
you know
parameterizable
factors
that you can
mess around with
and then start to
to see what the
impacts are
maybe that needs
a rethinking
I almost feel like
that's too didactic
and too
deconstructionist
and
and maybe that's not
the right approach
but in any case
kind of the work
before us
in some sense
is kind of clear
exactly how to go
about each step
obviously
will take a lot
of autoregressing
all right
so that's
that's my thoughts
for today on this
I do think
it's
it's time for
a kind of new
psychophysics
and that's
going to
require
a kind of
rethinking
of what
measurement is
in this space
but that's a good
thing
the way we've been
doing it
hasn't been
particularly effective
and so I think
we need to start
that today
thanks for listening
I'm on
Substack
Generative Brain
you can also
find me at
baronhals.ai
and I'm also
on x slash
twitter
