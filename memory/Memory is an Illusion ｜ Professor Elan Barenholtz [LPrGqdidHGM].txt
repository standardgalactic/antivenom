If you spend a lot of time with large language models like I do, you can
definitely be under the impression that they know a bunch of stuff, that they
have all kinds of information, they even remember previous conversations, that
they're kind of facts that live within them. But the truth is, if you think
about the architecture of these things, there's no sense in which you could say
there's knowledge in there, not in the conventional sense like in a computer
where there's a file that you access that contains the information. The way
large language models work is they are trained to just do something called next
token generation. If you give them a sequence, so the sequence could be, you
know, a question like, what is it like to be an LLM? And what they have learned to
do is to predict the very next token word. For all intents and purposes, we can just say
word. And so what is it like to be an LLM? It's going to try to predict the
next word, maybe it's as, and then you take that, stick it onto the sequence again, and
feed it again. What is it like to be an LLM? As, what would be the next token
prediction? And then it could be as N, or as N, and then you can get as N, LLM. You
daisy chain them together, and you get this thing, this auto-regressive next token
generation. You keep doing it, iterate, and then you can get very meaningful responses,
responses that are highly consistent with the kind of things we say that people
would say if they knew things, if they remembered things. And so the output is
consistent with the system knowing all kinds of information about just about
anything you want to talk about, whether it's, you know, a recipe for lentil soup or
string theory. These things can speak knowledgeably and pretty competently about
all of these things. And yet, if we think about what the weights contain, what the
actual model has within it, it's just next token prediction. There's no sense in
which it knows the long string of ideas that come spewing out of it when you run
it auto-regressively. And yet, it's able to produce these sequences. Now, the twist is,
of course, that I'm going to argue is that we are also just auto-regressive next token
generators. And that means we don't really have the knowledge we think we do. When we recall facts and
information, we're not really recalling them. They don't live in our mind waiting to be accessed
like a file on a computer. Instead, what we have is a predisposition to generate
certain kinds of sequences. If you ask me, you know, who's my favorite musician or
something? I don't know if I want to answer that right now. But let's just say I love
Sufjan Stevens. Okay, he's a big fan. Was that fact about me loving Sufjan Stevens, him being my
favorite musician, sort of already there in my brain? Well, yes, in the sense that if you ask me
the question, I'm going to give you that response, maybe, you know, 99 out of 100, maybe 100% of the
time. So I'm going to have that predisposition. But if we are LLMs, what I really have is just
that next token generative predisposition to say that. And so there's no sense in which I could say
that piece of information lives in my mind, independent of just the sequence that I generate
in response to that question. So I'm not exactly sure what that means in relation to is how illusory is
our general conception of memory. I think this applies to in some maybe more intuitive, or less
intuitive cases, like thinking about a memory of an event, or some fact, like, you know, where I leave my
keys. Oh, I remember, they're on the kitchen table. And it could feel like I'm just retrieving
that fact, like there's a stored kind of recording, whatever it is, of me leaving those keys on my
table. But what I'm proposing is really going on is when I think about the question, where did I leave
my keys, my brain queues up a visual memory, a sequence of me putting my keys on my table. And I see
that, and I observe that, that generative generated sequence, and lo and behold, I'm like, okay, boom,
that's where my keys are. And so again, it can seem like you're access, like I'm accessing some sort of
stored visual sequence, whereas what I'm really doing is just running the next token generation in
sequence, I'm just generating on demand in that moment. So to me, this really does kind of run against the
grain of how it feels intuitively to remember something, to have information, to have
knowledge, even beliefs, I would argue, this is something more like this predisposition rather
than articulated beliefs, they can't live in cold storage, if it's just this autoregressive kind of
process, then that's what it is, there's no sense in which there's an articulated belief, there's just a
predisposition to articulate a belief, or maybe a predisposition to exhibit a certain behavior.
So that's what I'm thinking about right now. I'm very curious to hear people's reaction to that.
And if people have a critique, an alternative perspective, reasons why they think this is
crazy, I'd love to hear them.
