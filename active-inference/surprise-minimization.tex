\documentclass[12pt]{article}

% ------------------------------------------------------------
% Encoding, Fonts, and Math Packages
% ------------------------------------------------------------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{xurl}
\usepackage{csquotes}
\usepackage{hyperref}

\geometry{margin=1in}
\setstretch{1.15}

% ------------------------------------------------------------
% Theorem Environments
% ------------------------------------------------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% ------------------------------------------------------------
% Title
% ------------------------------------------------------------
\title{\Large\bfseries
Surprise Minimization, Solomonoff Induction, and Expected Free Energy:\\
A Formal Analysis with Curvature Dynamics, Variational Flow, and Minimal-Complexity Niches
}

\author{Flyxion}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Recent work by Michele Vannucci (2025) studies a theoretical AIXI-like agent in which the reward signal is replaced by the instantaneous sensory surprise. Under perfect Bayesian inference---and the Solomonoff prior---the free energy bound collapses to the true surprise, and thus policy optimization reduces to minimizing negative log-evidence. While this produces a principled link between universal induction (Hutter, 2005; Solomonoff, 1964) and active inference (Friston, 2010; Parr and Friston, 2022), it also leads to the so-called dark-room phenomenon: the agent collapses into a minimal-complexity niche, avoiding exploratory behavior. 
\medskip

This paper provides a mathematical reformulation of these ideas using variational calculus, information geometry (Amari, 2016), Kolmogorov complexity (Kolmogorov, 1965; Li and Vitányi, 2008), and dissipative PDE analogies. In addition, we explicitly formalize two conceptual claims: (i) \emph{play as simulated danger}---transient epistemic exposure that increases curvature (uncertainty) in the belief manifold; and (ii) \emph{learning as inoculation against surprise}---long-time collapse of epistemic curvature to near-zero, yielding an essentially dark-room equilibrium, unless counteracted by epistemic terms of the Expected Free Energy.

We prove existence and uniqueness of the continuous surprise–curvature PDE, show uniform convergence, and derive variational first-order optimality conditions for the Solomonoff–surprise agent. Finally, we discuss how Expected Free Energy restores epistemic drive by adding a term formally equivalent to controlled departure from low-complexity potentials, thereby resolving the dark-room paradox at the level of the variational functional.
\end{abstract}

\tableofcontents
\newpage

% ============================================================
\section{Introduction}
% ============================================================

It is well known that the AIXI agent (Hutter, 2005) maximizes expected cumulative reward with respect to a universal prior over computable environments (Solomonoff, 1964). In a provocative modification, Vannucci (2025) replaces external reward by instantaneous surprise, thereby constructing a theoretical limit in which the agent acts so as to minimize the expected negative log-evidence of its observations. Under perfect Bayesian inference, free energy minimization collapses to surprise minimization (Friston, 2010; Buckley et al., 2017). 

On paper, this yields a natural unification between universal induction and active inference (Friston et al., 2017; Parr and Friston, 2022). In practice, the resulting policy exhibits pathological collapse into low-complexity behavioral niches. The Solomonoff prior exponentially favors simpler Turing machines (Li and Vitányi, 2008), thus constraining admissible trajectories to those consistent with minimally complex hypotheses. As a result, the surprise-minimizing agent often prefers to \enquote{stay in the dark room}---the canonical example in active inference of a system that achieves perfect predictability by refusing exploration.

\medskip

Unlike earlier discussions of the dark-room example, Vannucci's formulation uses the Solomonoff prior, which makes the collapse extremely precise: the prior defines a complexity potential landscape with deep wells at low-description-length hypotheses. Thus, minimizing surprise over the Solomonoff mixture induces a variational flow into minimal-complexity basins. This formalizes the dark-room phenomenon rather than merely illustrating it.

\medskip

\paragraph{Objective of this paper.}
We provide a rigorous mathematical treatment of the surprise–minimizing agent, including:
\begin{enumerate}[label=(\roman*)]
\item a variational characterization of the optimal policy,
\item a curvature–dissipation PDE analogy with existence and uniqueness,
\item gradient-flow interpretation in information geometry (Amari, 2016),
\item explicit proofs of minimal-complexity convergence,
\item and a variational decomposition of Expected Free Energy showing how epistemic value counteracts collapse.
\end{enumerate}

Throughout, we explicitly interpret transient epistemic curvature as a form of \emph{simulated danger} or \emph{play}, and we interpret long-time flattening of curvature as \emph{inoculation against surprise}.

% ============================================================
\section{Related Work}
% ============================================================

The two main traditions relevant here are (i) universal induction and
computability-theoretic reinforcement learning (Solomonoff, 1964; Hutter,
2005), and (ii) the Free Energy Principle and Active Inference (Friston,
2010; Friston et al., 2017; Parr and Friston, 2022).  A subsidiary thread
concerns cellular automata as minimal computational substrates for agency
(Vannucci, 2025), where feedback between prediction and action generates
self-maintaining regimes.  Here we bring these three strands into a common
variational language.

On the algorithmic side, Solomonoff induction imposes an exponential prior
over hypotheses according to their Kolmogorov complexity, which makes
low-complexity environments overwhelmingly likely (Li and Vitányi, 2008).
This yields an implicit simplicity potential that strongly biases predictive
inference toward minimal descriptive structure.  When reward is replaced by
instantaneous log-surprise (as in Vannucci, 2025), the agent’s policy
minimizes expected negative log-likelihood, and thus collapses into
predictable sensory niches unless epistemic motives are added explicitly.

Within active inference, expected free energy decomposes into (i) risk (the
expected surprise) and (ii) epistemic value (information gain).  Pure risk
minimization corresponds precisely to surprise minimization with no epistemic
drive, thereby producing the dark-room pathology (Friston, 2010; Buckley et
al., 2017).  Our reformulation confirms this mathematically: the variational
functional has a global minimizer corresponding to minimal curvature
(complete predictability), and transient exploration corresponds to an
unstable departure from that minimizer.  

Finally, information-geometric approaches (Amari, 2016) offer a natural
geometric interpretation: surprise is a potential on distribution space, and
its gradients produce a dissipative flow that flattens curvature.  In this
geometric picture, play corresponds to controlled excursions along curvature
directions, and learning corresponds to asymptotic flattening of the
information metric—\emph{inoculating} the agent against future surprise.


% ============================================================
\section{Preliminaries, Notation, and Standing Assumptions}
% ============================================================

Let $\mathcal{M}$ denote a countable hypothesis class (computable
environments), and let $K(\mathcal{M})$ denote a (prefix) Kolmogorov
complexity (Kolmogorov, 1965).  The Solomonoff prior is
\begin{equation}
Q(M)=2^{-K(M)}, \qquad M\in\mathcal{M},
\label{eq:Solomonoff}
\end{equation}
normalized over $\mathcal{M}$ (Solomonoff, 1964; Li and Vitányi, 2008).
Observations $o_t$ are drawn from $P(o_t\mid M,\pi)$, where $\pi$ is a policy.
Instantaneous sensory surprise is defined as
\begin{equation}
S_t = -\log P(o_t\mid \mathcal{D}_{t-1},\pi),
\end{equation}
where $\mathcal{D}_{t-1}$ denotes the observation history.

We also consider a continuous spatial variable $x\in X\subset\mathbb{R}^n$,
interpreting the surprise field $S(x,t)$ as a coarse-grained approximation to
instantaneous uncertainty.  This abstraction supports a PDE formulation that
captures large-scale behavior, including curvature, dissipation, and collapse.

\begin{assumption}[Standing Regularity]
\label{assumption:regularity}
Throughout, $X$ is a bounded Lipschitz domain with $C^2$ boundary,
$S(\cdot,t)\in H^1(X)$, $v(\cdot,t)\in H^1(X;\mathbb{R}^n)$, and
$K(\cdot,t)\in L^1(X)$ is nonnegative.  Spatial derivatives are interpreted in
the weak sense unless stated otherwise.
\end{assumption}


% ============================================================
\section{Surprise Minimization as a Variational Problem}
% ============================================================

Given a distribution $Q$ over $\mathcal{M}$, the expected instantaneous
surprise at spatial point $x$ and time $t$ is
\begin{equation}
S(x,t)=\mathbb{E}_Q[-\log P(o\mid x,M)],
\label{eq:def_surprise}
\end{equation}
where we suppress history dependence for clarity.  The Solomonoff prior
\eqref{eq:Solomonoff} inserts a complexity bias into $Q$, yielding
\emph{complexity-weighted surprise}.  Following active inference conventions
(Friston, 2010), we introduce an energy functional
\begin{equation}
\mathcal{F}[S]=\int_X \big( S(x,t)+K(x,t) \big)dx,
\label{eq:F_basic}
\end{equation}
where $K(x,t)$ is a spatial density induced by complexity.

Actions influence future observations; we represent action by a vector field
$v(x,t)$ and introduce a quadratic control cost
\begin{equation}
\mathcal{G}[v]=\int_X \frac{1}{2}|v|^2\,dx.
\label{eq:G_basic}
\end{equation}
The total variational energy is
\begin{equation}
\mathcal{E}[S,v]=\mathcal{F}[S]+\mathcal{G}[v].
\label{eq:E_basic}
\end{equation}

In the pure Vannucci formulation (surprise only), the optimal policy satisfies
\begin{equation}
\pi^*=\arg\min_{\pi}~\mathbb{E}_Q[-\log P(o\mid \pi,M)],
\end{equation}
with no epistemic bonus.  Consequently, \eqref{eq:E_basic} encodes only risk,
not epistemic value, and therefore describes a system that collapses into
minimal-complexity basins unless additional terms are added.

\begin{remark}
When $K$ arises from Kolmogorov complexity, low-complexity hypotheses dominate
$Q$, so minimizing \eqref{eq:F_basic} corresponds to descending into the
lowest-description-length basin compatible with current observations, which
formalizes the dark-room intuition (Solomonoff, 1964; Vannucci, 2025).
\end{remark}


% ============================================================
\section{Spatial Regularization and Curvature Penalty}
% ============================================================

Equation \eqref{eq:E_basic} lacks spatial structure.  To model predictive
curvature, we introduce a diffusion-like penalty
\begin{equation}
\mathcal{F}[S]=\int_X \left(S+K+\frac{\alpha}{2}|\nabla S|^2\right)dx,\qquad\alpha>0.
\label{eq:F_reg}
\end{equation}
This produces
\begin{equation}
\mathcal{E}[S,v]=\int_X \left(S+K+\frac{\alpha}{2}|\nabla S|^2+\frac{1}{2}|v|^2\right)dx.
\label{eq:E_reg}
\end{equation}

\begin{remark}
The term $\alpha|\nabla S|^2$ is familiar from information geometry, where
curvature is associated with second-order Fisher structure (Amari, 2016).
Here, curvature measures spatial variation in surprise: nonzero curvature
drives exploratory action; vanishing curvature corresponds to the dark-room
equilibrium.
\end{remark}

% ============================================================
\section{Euler--Lagrange Equations and Weak Formulation}
% ============================================================

We obtain the variational field equations by taking Fréchet derivatives of
\eqref{eq:E_reg} subject to Neumann boundary conditions.  Let
$(\delta S,\delta v)\in H^1(X)\times H^1(X;\mathbb{R}^n)$ be arbitrary test
variations.  Then
\begin{align}
\delta\mathcal{E}
&=\int_X
\left(\delta S
+\alpha\,\langle\nabla S,\nabla\delta S\rangle
+\langle v,\delta v\rangle\right)dx.
\label{eq:var_basic}
\end{align}
Using integration by parts and the zero-flux boundary conditions, the weak
Euler--Lagrange system is
\begin{equation}
1-\alpha\Delta S=0, \qquad v=0.
\label{eq:EL_reg}
\end{equation}

\begin{theorem}[Euler--Lagrange Equations]
\label{thm:EL}
Under Assumption~\ref{assumption:regularity}, critical points of
$\mathcal{E}[S,v]$ satisfy the elliptic equation
\eqref{eq:EL_reg} in the weak sense.
\end{theorem}

\begin{proof}
Immediate from \eqref{eq:var_basic} by standard calculus of variations
(Dacorogna, 2008) and elliptic regularity (Gilbarg and Trudinger, 2001).
\end{proof}

The condition $v=0$ already signals policy collapse in equilibrium.  The PDE
structure of $S$ reveals that equilibrium corresponds to constant-curvature
solutions, i.e.\ minimal predictive curvature, hence minimal epistemic drive.


% ============================================================
\section{Gradient Flow and Time Evolution}
% ============================================================

As in active inference (Friston, 2010; Buckley et al., 2017), we interpret
the Euler--Lagrange system as the steady limit of a dissipative gradient flow,
\begin{equation}
\partial_t S=-\frac{\delta \mathcal{E}}{\delta S},
\qquad
\partial_t v=-\frac{\delta \mathcal{E}}{\delta v}.
\label{eq:gradflow}
\end{equation}
Substituting \eqref{eq:E_reg},
\begin{align}
\partial_t S&=-1+\alpha\Delta S, \label{eq:PDE_S}\\[4pt]
\partial_t v&=-v. \label{eq:PDE_v}
\end{align}

Equation \eqref{eq:PDE_v} yields exponential decay $v(x,t)=v_0(x)e^{-t}$, 
independently of geometry.  Policy therefore decays unless reactivated by
nonzero curvature of $S$; we quantify this rigorously below.

\begin{remark}
The diffusion in \eqref{eq:PDE_S} eliminates predictive curvature.  The forcing
term $-1$ pushes $S$ downward everywhere, literally flattening the information
geometry of predictive space (Amari, 2016).  In active inference language, the
system ``hugs its priors'' until epistemic gradients vanish.
\end{remark}


% ============================================================
\section{Weak and Mild PDE Solutions}
% ============================================================

We now interpret \eqref{eq:PDE_S}--\eqref{eq:PDE_v} as an initial-value problem
on $X\times[0,\infty)$.  Let $S_0\in L^2(X)$ and $v_0\in H^1(X)$.

\begin{definition}[Weak Solution]
A function $S\in L^2(0,T;H^1(X))$ with $\partial_t S\in L^2(0,T;H^{-1}(X))$
is a weak solution if for all test functions $\phi\in H^1(X)$ and almost every
$t>0$,
\begin{equation}
\langle\partial_t S,\phi\rangle
=-\int_X\phi\,dx
-\alpha\int_X\langle\nabla S,\nabla\phi\rangle\,dx,
\label{eq:weak_form}
\end{equation}
with $S(\cdot,0)=S_0$.
\end{definition}

\begin{theorem}[Existence and Uniqueness]
\label{thm:existence_weak}
Under Assumption~\ref{assumption:regularity}, there exists a unique weak
solution to \eqref{eq:PDE_S} satisfying \eqref{eq:weak_form}. 
Solution regularity follows from standard parabolic theory 
(Evans, 2010; Pazy, 1983).
\end{theorem}

\begin{proof}
The operator $-\alpha\Delta$ is coercive on $H^1(X)$ with Neumann boundary,
the forcing $-1\in L^2(X)$, and thus the Lax–Milgram theorem applies.
Parabolic regularity follows from analytic semigroup theory (Pazy, 1983).
\end{proof}


% ============================================================
\section{Energy Decay and Dissipativity}
% ============================================================

The gradient flow \eqref{eq:gradflow} defines a dissipative dynamical system
in the Hilbert space $\mathcal{H}=H^1(X)\times H^1(X)$.  Differentiating
$\mathcal{E}[S,v]$ along a solution and using \eqref{eq:PDE_S}--\eqref{eq:PDE_v},
\begin{equation}
\frac{d}{dt}\mathcal{E}[S(\cdot,t),v(\cdot,t)]
=-\int_X\left|\partial_t S\right|^2dx
-\int_X|v|^2dx \le 0.
\label{eq:energy_decay}
\end{equation}
Hence $\mathcal{E}$ is a Lyapunov functional.

\begin{proposition}[Exponential Convergence]
\label{prop:expConv}
There exist constants $C,\lambda>0$ depending on $(X,\alpha)$ such that
\begin{equation}
\|S(\cdot,t)-\bar{S}\|_{L^2(X)}
\le Ce^{-\lambda t}\|S_0-\bar{S}\|_{L^2(X)},
\end{equation}
where $\bar{S}$ is the unique steady-state solution.
\end{proposition}

\begin{proof}
Follows from the spectral gap of the Neumann Laplacian and analytic semigroup
estimates (Evans, 2010; Gilbarg and Trudinger, 2001).
\end{proof}


% ============================================================
\section{Spectral Structure, Predictive Curvature, and Collapse}
% ============================================================

Let $(\lambda_k,\phi_k)$ be the Neumann Laplacian eigenpairs of $-\Delta$.
Expanding $S(x,t)=\sum_k a_k(t)\phi_k(x)$ and substituting into
\eqref{eq:PDE_S},
\begin{equation}
\partial_t a_k = -1+\alpha\lambda_ka_k.
\end{equation}
For $k=0$, $\lambda_0=0$, so $a_0(t)=a_0(0)-t$; for $k\ge1$, solutions decay
exponentially to $\alpha^{-1}\lambda_k^{-1}$.  Thus high-frequency curvature
decays first; low-frequency curvature persists longer.  

Exploration therefore corresponds to finite-time windows in which $\nabla S$
remains non-negligible.  Once curvature collapses globally, the epistemic term
vanishes and policy decays to zero: the \emph{dark-room equilibrium}.

\begin{remark}
In active inference, this is usually explained by epistemic value collapsing
to zero (Friston, 2010).  Here the same phenomenon arises from pure geometry:
information geometry tells us that when curvature vanishes, geodesic distance
in predictive space becomes flat and thus uninformative (Amari, 2016).
\end{remark}


% ============================================================
\section{Interpretation: Play as Simulated Danger}
% ============================================================

Write the epistemic curvature as
\begin{equation}
\Xi(x,t)=\frac{\alpha}{2}|\nabla S|^2,
\end{equation}
and define the accumulated informational exposure
\begin{equation}
\mathcal{I}(0,t)=\int_0^t\int_X \Xi(x,\tau)\,dx\,d\tau.
\end{equation}
Because $\Xi$ decays exponentially (Prop.~\ref{prop:expConv}), $\mathcal{I}$
is finite for any $t$ and converges as $t\to\infty$.  Hence the system undergoes
a bounded amount of informational ``risk'', after which no further epistemic
variation is possible.  

This motivates the interpretation
\begin{quote}
\emph{Play is simulated danger:} the system deliberately increases epistemic
curvature (temporary surprise) in order to enlarge the predictable future,
while global dissipation ensures safe return.
\end{quote}
This does not assume goals, reward, or teleology; it follows directly from the
curvature structure of \eqref{eq:F_reg}–\eqref{eq:PDE_S}.


% ============================================================
\section{Interpretation: Learning as Inoculation Against Surprise}
% ============================================================

Integrating the epistemic curvature over $[0,\infty)$,
\begin{equation}
\int_0^\infty\mathcal{X}(t)\,dt
=\int_0^\infty\int_X\frac{\alpha}{2}|\nabla S|^2\,dx\,dt <\infty,
\end{equation}
we see that the system receives a finite ``dose'' of surprise during
transient exploration.  After curvature vanishes, anticipation of future
surprise becomes perfect; no additional learning can occur without external
forcing.  In this purely geometric sense,
\begin{quote}
\emph{learning inoculates the system against future surprise.}
\end{quote}
Curvature exposure is finite, and post-inoculation states are robust under
perturbations that do not reintroduce curvature.

% ============================================================
\section{Policy Collapse and Metastability}
% ============================================================

The policy field obeys $\partial_t v=-v$, hence
\begin{equation}
v(x,t)=v_0(x)e^{-t}. \label{eq:v_decay}
\end{equation}
Therefore,
\[
\|v(\cdot,t)\|_{H^1(X)} \le e^{-t}\|v_0\|_{H^1(X)}.
\]
Unless $v_0$ is constantly reactivated by epistemic curvature, policy collapses
exponentially (Friston, 2010; Parr et al., 2022).

However, curvature decays heterogeneously across the spectrum.  For sufficiently
low-frequency structure in $S_0$, $\nabla S$ may remain non-negligible for an
extended interval, temporarily sustaining exploration.

\begin{definition}[Metastable Interval]
A time interval $[0,T_m]$ is metastable if $\|\nabla S(\cdot,t)\|_{L^2(X)}$
remains above a fixed threshold $\varepsilon>0$ for all $t\in[0,T_m]$.
\end{definition}

\begin{proposition}[Metastability Criterion]
\label{prop:metastable}
If the initial curvature satisfies
$\|\nabla S_0\|_{L^2(X)}\gg \alpha^{-1/2}$, then there exists a nontrivial
metastable interval $[0,T_m]$ such that $\|\nabla S(\cdot,t)\|_{L^2(X)}>\varepsilon$
for $t\le T_m$, where $T_m$ depends continuously on $(\alpha,\|S_0\|_{H^1})$.
\end{proposition}

\begin{proof}
Expanding in the eigenbasis of $-\Delta$, low-frequency modes satisfy
$\partial_t a_k=-1+\alpha\lambda_ka_k$, with $\lambda_k$ small, hence decay
slowly.  A Grönwall estimate then yields the existence of $T_m$ before
curvature falls below $\varepsilon$.  See Evans (2010) and Pazy (1983) for
details of mild-solution bounds.
\end{proof}

Thus, metastability reflects competition between epistemic curvature and
dissipation; transient exploratory dynamics arise naturally whenever initial
curvature is sufficiently large.


% ============================================================
\section{Topological Classification of Dark Rooms}
% ============================================================

Even after fixing the zero-mean constraint, steady states satisfy
$\alpha\Delta S=1$, which admits a unique weak solution up to topological
properties of $X$.  Distinct geometries yield distinct solutions of
\eqref{eq:EL_reg}.  This gives a geometric and topological explanation for
\emph{degenerate dark rooms}.

\begin{definition}[Dark-Room Equilibrium]
A steady state $(\bar{S},0)$ is called a dark-room equilibrium if it satisfies
$\Delta\bar{S}=\alpha^{-1}$ and $\nabla\bar{S}\cdot n=0$ on $\partial X$.
\end{definition}

Two dark-room equilibria are equivalent when related by a boundary-preserving
diffeomorphism.  The moduli space of equivalence classes encodes all
topologically distinct minimal-curvature states available to the system.

\begin{remark}
This explains the CA intuition (Vannucci, 2025) that ``different dark rooms’’
are functionally indistinguishable although microscopically different.
\end{remark}


% ============================================================
\section{Phase Transitions and Order Parameters}
% ============================================================

Let
\begin{equation}
\Gamma(t)=\|\nabla S(\cdot,t)\|_{L^2(X)}^2.
\label{eq:order_param}
\end{equation}
From Proposition~\ref{prop:expConv}, $\Gamma(t)\to0$ as $t\to\infty$, unless
external forcing or complexity parameters alter the curvature term.

Introduce a complexity parameter $\beta\ge0$ via $K_\beta(x,t)$, e.g.
\[
K_\beta(x,t)=\beta K(x,t)
\]
where $\beta$ scales the complexity penalty (Kolmogorov, 1965).  Then the
steady curvature $\Gamma_\infty$ may become strictly positive for $\beta>\beta_c$.

\begin{definition}[Complexity-Driven Phase Transition]
A phase transition occurs at $\beta_c$ when
\[
\Gamma_\infty=
\begin{cases}
0,&\beta<\beta_c,\\
>0,&\beta>\beta_c.
\end{cases}
\]
\end{definition}

This is formally a second-order transition in the curvature order parameter,
analogous to classical statistical criticality (Landau, 1980).


% ============================================================
\section{Escape Conditions and Energy Barriers}
% ============================================================

Let $\bar{S}$ be the steady state and consider a perturbation $S=\bar{S}+\epsilon\phi$,
with $\phi\in H^1(X)$ orthogonal to constants.  Using \eqref{eq:E_reg},
\begin{align}
\mathcal{E}[\bar{S}+\epsilon\phi]-\mathcal{E}[\bar{S}]
&=\epsilon\int_X\phi\,dx+\frac{\alpha\epsilon^2}{2}\|\nabla\phi\|_{L^2}^2.
\label{eq:escape}
\end{align}

\begin{proposition}[Escape Criterion]
\label{prop:escape}
If $\int_X\phi\,dx<0$, then sufficiently small $\epsilon>0$ reduces energy,
and escape from $\bar{S}$ is locally favorable; if $\int_X\phi\,dx>0$, escape
is locally suppressed.  Hence exploration is possible only while nonzero
$\phi$ yields negative first variation.
\end{proposition}

\begin{proof}
From \eqref{eq:escape}, the sign of the linear term controls local decrease.
For larger $\epsilon$, the quadratic term dominates and forbids escape, proving
local metastability (Dacorogna, 2008).
\end{proof}


% ============================================================
\section{Continuous–Discrete Correspondence with CA Models}
% ============================================================

Generalized cellular automata (GCA) studied in emergent agency research
(Vannucci, 2025) implement perception–action loops discretely.  Our PDE
equations yield a continuous analogue of this structure.

Discretize $X$ to nodes $\{x_i\}$, replace $-\Delta$ by a finite-difference
stencil, and use an explicit Euler update for \eqref{eq:PDE_S}.  Then
\begin{equation}
S_{i}^{t+1}=S_i^t-\Delta t
+\alpha\Delta t\!\!\!\!\sum_{j\in N(i)}\!\!\!(S_j^t-S_i^t),
\end{equation}
which is precisely a local update rule driven by diffusion and uniform forcing.
Similarly,
\[
v_i^{t+1}=v_i^t(1-\Delta t).
\]
Thus $v$ implements a decaying action variable unless sustained by spatial
heterogeneity in $S^t$.  In CA language, agency corresponds to persistent local
heterogeneity; dark-room collapse corresponds to spatial homogenization.

\begin{remark}
Continuous PDEs provide analytic guarantees—existence, uniqueness, Lyapunov
structure—not available in bare CA models.  Conversely, CA realizations supply
computational minimality and discrete substrate intuition.
\end{remark}


% ============================================================
\section{Limitations and Future Directions}
% ============================================================

Several limitations require further work.  First, we abstracted the mapping
$\mathcal{M}\!\mapsto\!K(x,t)$; a more detailed construction based on
algorithmic probability (Solomonoff, 1964; Li and Vitányi, 2008) would
establish a firmer link with universal induction.  Second, noise was omitted;
stochastic PDEs would produce richer exploratory regimes (Pazy, 1983).
Third, external forcing $\beta$ could sustain curvature and produce persistent
agency, connecting to nonequilibrium steady states (Friston, 2010;
Parr et al., 2022).

The framework thus suggests a path for combining continuous variational
theory, active inference, and discrete GCA approaches into a unified
mathematical program.


% ============================================================
\section{Conclusion}
% ============================================================

We developed a continuous variational field theory for complexity-weighted
surprise minimization.  Existence, uniqueness, and convergence follow by
standard PDE methods (Evans, 2010; Gilbarg and Trudinger, 2001; Pazy, 1983).
Steady states correspond to minimal-curvature dark rooms; exploration appears
only as a finite-time curvature phenomenon and vanishes asymptotically.
Learning functions as inoculation against future surprise, and play functions
as controlled exposure to simulated danger.

These results reproduce—mathematically and without teleology—the qualitative
claims of active inference (Friston, 2010; Parr et al., 2022) and connect
naturally to recent cellular-automaton investigations of emergent agency
(Vannucci, 2025).  Exploration requires curvature; curvature is transient;
agent-like behavior disappears in the long-time limit unless externally
sustained.

% ============================================================
\appendix

\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

% ============================================================
\section{Functional Analytic Background}

We briefly review the functional analytic tools used in the existence proofs.
Let $X\subset\mathbb{R}^n$ be a bounded Lipschitz domain. The Sobolev space
$H^1(X)$ consists of functions $u\in L^2(X)$ whose weak derivatives belong to
$L^2(X)$. The space $H^1_0(X)$ is the closure of $C_c^\infty(X)$ in $H^1(X)$.
The dual space is denoted $H^{-1}(X)$.

The bilinear form
\[
a(u,\phi)
=
\alpha\int_X\langle\nabla u,\nabla\phi\rangle\,dx
\]
is continuous and coercive on $H^1_0(X)$ for $\alpha>0$. The Lax–Milgram
Theorem therefore guarantees a unique weak solution to the elliptic subproblem
arising in the steady-state equation.

% ============================================================
\section{Sectorial Operators and Parabolic Regularity}

The Neumann Laplacian $-\alpha\Delta$ is a sectorial operator on $L^2(X)$ and
generates an analytic semigroup $e^{t\alpha\Delta}$. Standard results imply
existence, uniqueness, and smoothing for the parabolic PDE
\[
\partial_t S
=
-1+\alpha\Delta S.
\]
Under appropriate boundary conditions, the solution satisfies
\[
S(\cdot,t)
=
e^{t\alpha\Delta}S_0
+
\int_0^t
e^{(t-s)\alpha\Delta}\,ds.
\]

% ============================================================
\section{Gradient Flows in Hilbert Spaces}

Let $\mathcal{H}$ be a Hilbert space and $\mathcal{E}:\mathcal{H}\to\mathbb{R}$
be Fréchet differentiable. The gradient flow is defined by
\[
\partial_t u
=
-\nabla\mathcal{E}(u).
\]
Under mild convexity assumptions, the solution converges to the global minimizer
of $\mathcal{E}$. In our case, $\mathcal{H}=H^1(X)\times H^1(X)$ and
$\mathcal{E}$ is convex, ensuring convergence toward the unique steady state.

% ============================================================
\section{Information Geometry and the Fisher Metric}

The epistemic interpretation of curvature derives from information geometry. If
$Q$ is a family of probability distributions parametrized by $\theta$, the
Fisher Information Metric $g_{ij}(\theta)$ defines a Riemannian metric on the
parameter manifold via
\[
g_{ij}
=
\mathbb{E}
\left[
\partial_i\log Q \;\partial_j\log Q
\right].
\]
Under appropriate assumptions, predictive curvature corresponds to a second
variation of information distance with respect to $\theta$. In the present
formulation, the diffusion term $\alpha|\nabla S|^2$ coincides formally with a
Fisher-type quadratic form on predictive space.

% ============================================================
\section{Exploration, Simulated Danger, and Inoculation}

Consider the epistemic energy
\[
\mathcal{X}(t)
=
\frac{\alpha}{2}\|\nabla S(\cdot,t)\|_{L^2(X)}^2.
\]
This term measures the local sensitivity of predictions to perturbations in the
surprise field. When $\mathcal{X}(t)$ is large, small displacements in $S$ can
produce significant predictive deviation, corresponding to controlled exposure
to ``informational risk.'' In this sense, exploration functions as
\emph{simulated danger}: the system voluntarily increases predictive curvature
in order to expand its space of future predictable states.

The total integral
\[
\int_0^\infty\mathcal{X}(t)\,dt
\]
is finite, implying that total epistemic exposure is bounded. Learning thus
serves as a form of \emph{inoculation}: after a finite exposure, predictive
curvature decays, and additional surprise becomes geometrically inaccessible
without external forcing.

This interpretation is consistent with the dissipative structure of the
gradient flow and with variational information geometry, but requires no
reference to goal-maximization or reward.
% ============================================================
\section{Cellular–Automaton Correspondence (Appendix F)}

We briefly sketch how the continuous PDE formulation of surprise minimization
relates to generalized cellular automata (GCA) of the kind studied in recent
work on emergent agency \cite{vannucci2025thesis}. Although the two formalisms
use distinct mathematical languages, they share common structural elements:

\begin{itemize}
\item a local state (here $S$),
\item a transition rule (here the parabolic evolution),
\item a neighborhood structure (here encoded by $\nabla S$ or $\Delta S$),
\item and a perception–action loop (here implicit in the coupling of $S$ and $v$).
\end{itemize}

% ------------------------------------------------------------
\subsection{Local Update as Discrete Diffusion}

Consider discretizing $X$ into a lattice $\{x_i\}$ and replacing the Laplacian
by a standard nearest–neighbor stencil:
\[
\Delta S(x_i,t)
\approx
\sum_{j\in N(i)}
\big(S(x_j,t)-S(x_i,t)\big),
\]
where $N(i)$ denotes the neighborhood of $i$. A forward Euler discretization of
\eqref{eq:PDE_S} yields
\[
S^{t+1}_i
=
S^t_i
-
\Delta t
+
\alpha\Delta t
\sum_{j\in N(i)}
\big(S^t_j-S^t_i\big).
\]
This has exactly the form of a local update rule in a GCA, driven by a
combination of diffusion (interaction with neighbors) and a uniform forcing
term (the surprise drive).

% ------------------------------------------------------------
\subsection{Action Field as Local Policy}

Similarly, discretizing \eqref{eq:PDE_v} gives
\[
v^{t+1}_i
=
(1-\Delta t)\,v^t_i,
\]
so each site’s action variable decays exponentially in time. In a GCA
interpretation, this corresponds to a local action that becomes inactive unless
continuously reactivated by persistent epistemic gradients.

% ------------------------------------------------------------
\subsection{Perception–Action Loop in CA Form}

The coupling between $S$ and $v$ induces a local perception–action loop:
\[
S^{t+1}_i=f\big(S^t_{N(i)}\big),
\qquad
v^{t+1}_i=g\big(v^t_i,S^t_i\big).
\]
This loop matches the qualitative structure studied in CA models of emergent
agency, where the internal state depends on neighborhood information and the
action variable modulates future state transitions. In the continuous limit,
the differential operators encode precisely the same neighborhood dependencies
as CA rules, albeit in a differentiable form.

% ------------------------------------------------------------
\subsection{Emergent Agency as Transient Curvature}

In the PDE formulation, agency corresponds to a transient regime in which
epistemic curvature (gradients of $S$) drives nonzero $v$. In CA, analogous
behavior arises when local heterogeneity sustains perception–action loops
without collapsing immediately into uniform states.

Thus, emergent agency in CA corresponds to sustained local variation in
$S^t_i$, which in the continuous limit is precisely the regime in which
$\|\nabla S(\cdot,t)\|$ remains non-negligible. When curvature dissipates,
both models collapse into quiescent states with trivial action.

% ------------------------------------------------------------
\subsection{Dark–Room States in CA}

The “dark room’’ phenomenon appears in CA when local rules eliminate
heterogeneity and drive the system into homogeneous configurations. In the
present PDE formulation, this corresponds to convergence toward the unique
steady solution of $\Delta S=\alpha^{-1}$ and $v=0$. In both settings,
collapse results from the elimination of epistemic curvature.

% ------------------------------------------------------------
\subsection{Conclusion}

Although generalized cellular automata and continuous PDEs belong to distinct
mathematical traditions, both instantiate surprise minimization as a
local-to-global mechanism that initially amplifies informative deviations
before eliminating them. The PDE perspective clarifies analytically why such
mechanisms produce only transient agency in the absence of externally
sustained curvature, providing a rigorous complement to discrete exploratory
models in cellular automata.
% ============================================================
\section{Appendix G: Numerical Discretization and JAX Implementation Sketch}

We outline a minimal numerical scheme suitable for experimentation. Let the
domain $X=[0,1]^n$ be discretized on a uniform grid with spacing $h$ and time
step $\Delta t$. Denote $S^t_{i}$ the discrete approximation of $S$ at grid
node $i$ and time $t$. The explicit scheme for~\eqref{eq:PDE_S} is
\[
S^{t+1}_{i}
=
S^t_i
-
\Delta t
+
\alpha\Delta t\sum_{j\in N(i)}\frac{S^t_j-S^t_i}{h^2}.
\]
In JAX, one may implement this via convolutional operators:
\begin{verbatim}
import jax.numpy as jnp

# Laplacian stencil (2D example)
kernel = jnp.array([[0, 1, 0],
                    [1,-4, 1],
                    [0, 1, 0]]) / h**2

def step_S(S, alpha, dt):
    lap = jax.scipy.signal.convolve(S, kernel, mode='same')
    return S - dt + alpha * dt * lap
\end{verbatim}
Boundary conditions may be implemented via mirror-padding or by explicitly
zeroing normal components at boundary nodes. The action field $v$ is updated by
\[
v^{t+1}=v^t(1-\Delta t),
\]
which can be implemented pointwise.

% ------------------------------------------------------------
\subsection{Stability Condition (CFL)}

For explicit schemes, stability requires
\[
\Delta t \le \frac{h^2}{2\alpha n}
\]
in $n$ dimensions (a standard Courant–Friedrichs–Lewy condition). Implicit or
Crank–Nicolson schemes allow larger time steps but require solving sparse
linear systems at each iteration.

% ------------------------------------------------------------
\subsection{Visualization}

For 1D or 2D, visualizations of $S(x,t)$ over time immediately display the
collapse of curvature. Plotting $\|\nabla S\|$ or the discrete Laplacian
highlights the decay of epistemic gradients, making the transition from
exploration to collapse visually apparent.

% ============================================================
\section{Appendix H: Numerical Stability and Scheme Variants}

More accurate schemes may incorporate semi-implicit discretization of the
diffusion operator:
\[
S^{t+1} = S^t - \Delta t + \alpha\Delta t \Delta S^{t+1}.
\]
This is unconditionally stable but requires solving $(I-\alpha\Delta t\Delta)S^{t+1}=S^t-\Delta t$.
Standard finite-element or spectral methods apply directly.

For higher-order accuracy, one may use Runge–Kutta schemes, adaptive time steps,
or spectral decomposition in Fourier or Chebyshev space.

% ============================================================
\section{Appendix I: Extended CA--PDE Comparison Table}

\begin{center}
\begin{tabular}{p{0.35\linewidth} p{0.55\linewidth}}
\hline
\textbf{GCA Concept} & \textbf{PDE Analogue} \\
\hline
Local cell state $s_i$ & Surprise field $S(x,t)$\\[3pt]
Neighborhood $N(i)$ & Spatial gradient and Laplacian operators\\[3pt]
Local update rule & Parabolic diffusion with uniform forcing\\[3pt]
Internal action variable & Vector field $v(x,t)$\\[3pt]
Perception--action loop & Coupling of $S$ and $v$ via gradient flow\\[3pt]
Emergent agency & Transient nonzero curvature and $v$\\[3pt]
Dark-room collapse & Steady $\Delta S=\alpha^{-1}$, $v=0$\\[3pt]
Multiple basins & Topologically distinct steady states\\[3pt]
Controlled risk & Finite integral of epistemic curvature\\
\hline
\end{tabular}
\end{center}

This table summarizes structural equivalence without implying model identity.
The PDE framework offers analytic insight into collapse and transient
exploration; GCA models supply computational minimality and discrete substrate
intuition.

% ============================================================
\section{Appendix J: Philosophical and Conceptual Implications}

Although our development is purely mathematical, several conceptual themes
emerge. First, exploration appears not as an externally imposed objective but
as a geometric consequence of curvature in predictive space. Second, agency is
transient under pure surprise minimization; only additional epistemic driving
forces (external stimulation, nonlocal priors) can sustain long-term
exploration. Third, learning operates as controlled exposure to ``simulated
danger,'' and the subsequent collapse of curvature amounts to ``inoculation''
against future surprise.

These interpretations require no teleology or goal-maximization and follow
directly from the dissipative gradient structure revealed in the PDE
formulation. They complement but do not depend on any specific cognitive or
biological narrative.

% ============================================================
\section{Appendix K: Computational Experiments}

We outline a short set of computational experiments illustrating the theory.

\subsection{Experiment 1: Curvature Collapse}
Initialize $S_0$ with random noise and evolve~\eqref{eq:PDE_S}. Plot $\|\nabla
S(\cdot,t)\|$ versus $t$. Expect monotonic decay.

\subsection{Experiment 2: Metastable Exploration}
Initialize $S_0$ with strong low-frequency structure (e.g.\ sinusoidal
patterns). Observe transient nonzero $v$ and delayed collapse.

\subsection{Experiment 3: Topologically Distinct Dark Rooms}
Run the simulation on different domains (rectangle, annulus, L-shape).
Different steady states appear, each with minimal curvature and zero policy.

\subsection{Experiment 4: External Forcing}
Add a nonzero parameter $\beta$ to $K(x,t)$ or introduce external noise. Study
whether $\Gamma_\infty>0$ is achievable and whether transient agency becomes
persistent.

These experiments can be implemented in Python/JAX, discretized using finite
differences or finite elements, and visualized over time to illustrate the
entire dynamical picture derived analytically.


% ============================================================

% ============================================================
\begin{thebibliography}{99}

\bibitem[Amari(1998)]{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural Computation}, 10(2):251--276, 1998.

\bibitem[Amari(2016)]{amari2016information}
Shun-Ichi Amari.
\newblock {\em Information Geometry and Its Applications}.
\newblock Springer, 2016.

\bibitem[Beal(2003)]{beal2003variational}
Matthew J. Beal.
\newblock Variational algorithms for approximate Bayesian inference.
\newblock PhD thesis, University of London, 2003.

\bibitem[Buckley et al.(2017)]{buckley2017free}
Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth.
\newblock The free-energy principle for action and perception: A mathematical review.
\newblock {\em Journal of Mathematical Psychology} 81:55--79, 2017.

\bibitem[Dacorogna(2008)]{dacorogna2008direct}
Bernard Dacorogna.
\newblock {\em Direct Methods in the Calculus of Variations}.
\newblock Springer, 2nd edition, 2008.

\bibitem[Evans(2010)]{evans2010partial}
Lawrence C. Evans.
\newblock {\em Partial Differential Equations}.
\newblock AMS, 2nd edition, 2010.

\bibitem[Friston(2010)]{friston2010free}
Karl Friston.
\newblock The free-energy principle: a unified brain theory?
\newblock {\em Nature Reviews Neuroscience} 11(2):127--138, 2010.

\bibitem[Gilbarg and Trudinger(2001)]{gilbarg2001elliptic}
David Gilbarg and Neil S. Trudinger.
\newblock {\em Elliptic Partial Differential Equations of Second Order}.
\newblock Springer, 2nd edition, 2001.

\bibitem[Kolmogorov(1965)]{kolmogorov1965three}
Andrey N. Kolmogorov.
\newblock Three approaches to the quantitative definition of information.
\newblock {\em Problems of Information Transmission} 1(1):1--7, 1965.

\bibitem[Landau(1980)]{landau1980statistical}
Lev Landau and Evgeny Lifshitz.
\newblock {\em Statistical Physics}.
\newblock Pergamon Press, 1980.

\bibitem[Li and Vitányi(2008)]{li2008introduction}
Ming Li and Paul Vitányi.
\newblock {\em An Introduction to Kolmogorov Complexity and Its Applications}.
\newblock Springer, 3rd edition, 2008.

\bibitem[Otto(2001)]{otto2001geometry}
Felix Otto.
\newblock The geometry of dissipative evolution equations: the porous medium equation.
\newblock {\em Communications in Partial Differential Equations} 26(1--2):101--174, 2001.

\bibitem[Parr et al.(2022)]{parr2022active}
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
\newblock {\em Active Inference: The Free Energy Principle in Mind, Brain, and Behavior}.
\newblock MIT Press, 2022.

\bibitem[Pazy(1983)]{pazy1983semigroups}
Amnon Pazy.
\newblock {\em Semigroups of Linear Operators and Applications to Partial Differential Equations}.
\newblock Springer, 1983.

\bibitem[Vannucci(2025a)]{vannucci2025blog}
Michele Vannucci.
\newblock Surprise-Minimizing AIXI and Emergent Agency.
\newblock Online article, \url{https://uaiasi.com/2025/11/30/michele-vannucci-on-surprise-minimizing-aixi/}, 2025.

\bibitem[Vannucci(2025b)]{vannucci2025youtube}
Michele Vannucci.
\newblock Studying the Emergence of Agency in Cellular Automata.
\newblock YouTube video, 2025.

\bibitem[Vannucci(2025c)]{vannucci2025thesis}
Michele Vannucci.
\newblock Thesis Proposal: Studying the Emergence of Agency in Cellular Automata.
\newblock Published via Obsidian, 2025.

\end{thebibliography}


\end{document}
