We have a very special guest tonight, who I believe you just got back from out of town
or something, right?
Yes.
Just got back.
And the fact that he took the time out of his busy life, his incredibly busy life, and
you can find him multi-testing on his laptop and his headphones online.
To come here and speak to all of us is just incredible.
So I want to first leave everybody to give it up for Stephen Wolf.
As I understand it, the topic is computing needs longevity.
Yes.
And I realized that I did a project a few months ago, which I actually am still pursuing
right now, that is actually directly relevant to that topic.
And in fact, I have a thing that I am about to start working on that is also very relevant
to that topic, and maybe you guys can help me do some of that project a little bit.
But let me explain one point.
So I got interested, so I was trying to understand things about the foundations of machine learning,
basically why machine learning works.
And that led me to return to something that I looked at in 1985, which was questions about
why biological evolution works.
And in 1985, I had made some very minimal models for biological evolution, which didn't work.
So now that I know that machine learning does work, I decided I should look back at these
minimal models for biological evolution, and to my considerable surprise, they actually worked.
And so I can tell you about what I figured out there, and then can talk about sort of a legal idea
that I had very recently, like a week or so ago, about the kind of figuring out a sort of formal
foundational theory for medicine, and a formal foundational theory to kind of therapy in medicine,
what I was going to do, and what I was going to do, and what I was going to do, and what I was going to do.
So the question, the starting point is kind of a minimal model for a lot of evolution,
and kind of the idea is we've got a friendly cellarotontin here.
Okay, so the idea is this is a little tiny genome for this organism.
So the organism, this is a cellularotontin, so it's a row of cells.
Each cell in this case is either white, red, or blue.
And then these rules here tell you, given that you have forgotten blue, red, red,
this tells you the next step, you'll have red.
So then you start it up in one red cell here, and it runs for a while,
and then eventually it dies of old age or something.
It dies out.
Okay, so the question is, can you evolve it to live longer?
It so happens that the actual fitness criteria that I used for the study of biological evolution
was precisely longevity.
Happens to be thematically relevant.
Okay, so you can make mutations, make point mutations to that tiny genome,
and your goal is to keep on making those point mutations,
and to accept a mutation if it doesn't make you live less long,
and then see what happens.
So then, if you do that, you start off from something where you just have a completely null rule
where it just dies out of one step, and then you progressively, you know,
you take that rule, and these are the sort of the waypoints,
the progressive successes in living longer.
So you can see, if you look, this is a picture of kind of the lifetime as a,
on the x-axis is kind of the steps of adaptive evolution,
and then on the y-axis is how long it lived,
and every red dot represents an attempt with a different genome,
and many of those attempts it didn't live any longer,
and you don't keep those attempts.
And there are these long periods where you don't really make progress,
and then suddenly a sort of breakthrough is made, and it jumps up.
That's kind of that sort of, you know, plateaus, breakthroughs thing
is something you quite often see in loss curves in machine learning,
or reasons that are not emulated.
But anyway, so what happens is you can, depending on, you know,
the rolls of the dice, so to speak, you will get different sequences of adaptive,
different sort of, you know, it will learn differently how to live a long time.
And what's sort of interesting is you look at these things,
and what's interesting for biology is you look at these things
and you see all these different ideas that the system has for how to live a long time.
And so what typically happens is you have these ideas
that you kind of build on those ideas and progressively live on.
Do people understand how this model works?
And so, okay, so then, you know, there's a bunch of stuff that you can do about asking,
because it's a very simple model, you can kind of look at sort of all possible parts of evolution of this model.
So there's kind of a, all the possible ways that you could have modified the genome,
and you could have evolved to a different state, and, you know, on the,
and depending on which, sort of on the right you have one set of ideas about how to live a long,
and on the left you have a different set of ideas.
If this was kind of the tree of life, one side might be, you know, prokaryotes,
one side might be archaea or something like this.
Because that's a different idea of how to be successful.
Okay, so that, that's the, that's the idea, and there's lots of detail around this,
which maybe is not relevant, but this is like, if you are at a particular kind of waypoint,
you say, what are the, you know, looking around you in the space of possible changes,
what things can happen?
And you see that there are, you know, these are, these are sort of,
this is the kind of local derivative of space.
And you can, you can, you can, you can control,
um, the, uh, control of actual fitness landscapes, all kinds of things like this.
Anyway, the thing I thought might be of interest, but by the way, when you have, um,
you can, you can end up with, uh, with critters that live a really long time,
and, uh, you can, you can, some of these you can find by adaptive evolution,
some you can just find by exhaustive search.
If you want to know what's the kind of distribution of lifetimes,
I think that's, uh, that's a typical distribution of lifetimes of all possible organisms.
So it's a power law distribution .
Um, but, uh, what's, what's also kind of interesting is if you say,
well, how does the thing manage to live a long time?
Well, most of these, a few, in a few of these cases,
there are kind of identifiable mechanisms that you can see.
But most of the time, it's very hard to explain why it lives a long time.
It's just doing all these complicated things, and it so happens that, yes,
it lives the three steps, for example.
So it's kind of a, it's, it's a little bit, you know, disappointing
if you want to have a kind of mechanistic theory of biology,
because it says, you know, what's happening is just this lump of computation
that happens to achieve some particular objective.
It's not something where we can understand, you know, there's this mechanism
that's put together this way, that way.
Anyway, so I thought the thing that might be interesting to look at, um, let's see,
the, uh, you can, you can, you can look at what happens if you, um, um,
if you're doing adaptive evolution with different criteria,
like these are ones that, I think these are ones that were evolved to be wide
rather than to be tall, and, um, you could, you can, I think those are,
those are different, um, and I think there's ones here, I forget which ones,
there's some which, which were evolved to have a length to width ratio,
with pi.
So you can get it to evolve, to do all kinds of things.
Um, and it's sort of interesting to see what, what structures you actually get,
because none of these structures are things that you would have,
kind of, if you were sitting down and saying, I want to make a cellular
retomaton whose length to width ratio is pi, you would never have come up
with stuff like this.
This is a kind of a, a thing only to be found by kind of searching
the computational universal possibilities.
Okay, so, let me show you something about the public days below.
Um, first of all, you can make much bigger versions of, actually, that,
for example, you can, one thing's kind of interesting is, if you want
different kind of fitness functions, it turns out it's the same underlying graph,
but the graph is kind of explored in different ways, depending on what fitness function you're using.
And that has all kinds of consequences not yet probably worked out.
Um, but, uh, well this is a, impossible to see, but this is a much larger, um, case of looking at the possible paths of evolution for a slightly larger, uh, set of, you know, bigger genome, so to speak.
But the thing that I just started and didn't really get involved with is this, this thing here.
So I was interested in, given that you have a creature, so to speak, that's been evolved for a while,
so, you know, it's been set up so it lives a long time, so it has achieved some fitness purpose.
And then you perturb the thing. So in other words, it's, it's a happy organism that's living a while, whatever else.
Now let's say you, uh, you know, you give it, there's some trauma that it experiences, it gets poked in some way.
The question is, what will then happen to it? And will it, sort of, when, when it has those defects,
will it show certain diseases that cause it to die out quickly? Will it live longer? What happens?
So, you know, you poke it, and then, yes.
Stephen, may I add to that, if you didn't say it already? Has anybody heard of the term hormesis before?
Yeah. H-O-R-M-E-S-I-S. So it's sort of like this idea that you stress.
Okay, go ahead.
Hormesis. It's like a good stress and that's a stimulus to get better.
This is actually something David Sinclair has probably more popularly pointed in than other people.
And you see this thing everywhere in biology, so I'm really interested in what you're going to say.
Okay, well, I don't know the end of the story, but I just started looking at this question of,
when you have kind of an evolved system that's been evolved for some business purpose,
and then you make preparations to it, can you sort of classify the things that go along?
So it's kind of the analog of saying we've got classification of human diseases,
we've got ICD-10 and something with 100,000 diseases.
Can we classify the diseases of this tiny model system?
Just like you can also say, can we classify diseases of operating systems or something when they, you know,
computer security purposes, things like that.
So the first step is, you know, can you classify sort of the things that can go wrong?
And then, once you've poked it and something has gone wrong, can you fix that by making some other perturbation?
Or, for example, just the question of, well, we can test your hypothesis that it helps to have a perturbation.
In fact, well, in these particular pictures here, I just was kind of sort of poking, I think,
I think it was just sequencing through different bits that were being flipped.
And sometimes, actually in this case, somewhat surprisingly to me,
the thing seems to often live longer as a result of being poked.
Well, sometimes it ends up living forever, sometimes it grows forever, which is kind of a tumor-like situation.
So I guess my sort of, if I were hackathon, so to speak, I might be looking at this thing
and trying to figure out what one can say about sort of perturbations to evolved systems
and whether there are general things, because usually when you think about medicine,
there isn't really, to my knowledge, maybe a lot of people feel differently,
but to my knowledge, there's basically no kind of formal theory of medicine
that one's ever thought about.
Medicine is this thing that's very, you know, human-centric,
and it's not something where, you know, people just say,
well, there's this classification of diseases and they are what they are,
and there's no kind of way of formalizing how that works.
So that was a topic I thought might be interesting.
We can be curious.
Yes.
I'm actually going to also add one more comment.
Normally, I wouldn't bring this up.
When I was promoting this event online, I posted it on the hair loss subreddit.
It's called, like, R-Trestless or something.
And I was surprised, like, how many people quickly responded and were like,
we need some theory of biology that could potentially solve something like hair loss?
Huh.
Because it's like...
Since I was a kid, actually, I was a stem cell researcher now.
Yeah.
And she happens to work on WNT1.
Yeah, yeah, yeah, yeah.
I've told her for years, the time I know that science has really arrived is when I've gone.
Yeah, yeah.
Maybe this is at that moment.
Yeah.
But, I mean, we could poke around and actually try and do some live experiments on these things.
Yeah.
We could talk about some other...
Totally.
Yes.
Okay.
That's the...
She has a question.
Yeah, please.
I was just wondering if you could...
I'm a medical student.
So if you could expand even just a little bit more about that no formal thing of medicine.
Because I do agree from now, like, entering this...
Right.
This, like, this pathway, which seems to be so formal is what everyone acts like.
But I don't get it.
I think you're...
Well, I think the...
Okay.
What is there to say about it?
I mean, the...
You know, if you're talking about diseases, it's just like we have a certain taxonomy of
diseases.
And there's nothing that says, you know, this is the...
This is how...
You know, this is...
We know nothing about what, you know, the whole spectrum of possible diseases.
Other than...
So, say another thing, I'm sure.
One place where, let's say things like, I don't know, cell types.
When I was a kid, you know, you learn there are two kinds of blood cells.
Red blood cells and white blood cells.
Okay?
And a little bit later it's, well, actually there are B cells and there are T cells.
And then, well, actually there's CD8 positive, you know, T cells, et cetera, et cetera, et cetera.
And it gets more and more and more detailed.
And so it's sort of an interesting question.
What, you know, does that end?
You know, in other words, is there...
One of the things that's weird about biology, and I'm mentioning several different things,
but, you know, I spent a bunch of time doing physics, other fields like that.
Physics has the feature that the simple hypothesis is usually correct.
Biology and medicine has the feature that the most complicated thing that could possibly
happen is usually what's happening.
But there's no simple, you know, there's no sort of one effect that is what's going on.
There's always, well, it's more or less one effect.
You know, it's more or less the central dogma for, you know, for transcription of, you know,
RNA, et cetera, et cetera, et cetera.
But then, well, actually there's a footnote, but sometimes it doesn't work that way.
I mean, it's a field of endless footprints.
I mean, I think one of the things I've long been curious about, about, okay, this relates to longevity.
If you're running an operating system on a computer, then after a while it will crash.
You know, it might take a week, it might take a month, it might take an hour.
But eventually something will go wrong and the thing will crash.
And, you know, at one time, actually about 15 years ago when we first launched corporate alpha,
we had all kinds of trouble with the servers that we had crashing.
And so we started a study of the mortality of operating systems.
Trying to figure out what the, you know, what the mortality curves were,
what the different causes of mortality were in operating systems.
We never finished it because basically we fixed some engineering problems and this went away.
But, you know, the question is, if you have an operating system that's running and something's gone wrong,
you know, that's the case where you have all the bits are available to you.
Can you fix the thing that went wrong and keep the operating system running?
Which is kind of analogous to the longevity problem of if you've got a human that's up and running, you know,
can you fix it and keep it going?
Because, I mean, the obvious solution to an operating system is to reboot the computer,
which in the human case is forget this generation, just restart from similar genetic material to the next generation.
I mean, that's the solution that biology has to, you know, fixing problems is reboot,
which is the same thing that, you know, that we find in computers,
except that for each one of us, we prefer not to be, have to be rebooted.
So it's, you know, so we'd like to be able to sort of fix it while it's flying, so to speak.
But I think, I mean, in, gosh, you know, you could say lots of things about, I mean, okay,
so years ago I thought about doing a big project to do kind of AI medical diagnosis.
And it was clear, I mean, the main reason we didn't do it, so we even have found ways to get data,
it's much easier outside the U.S. to get large amounts of somewhat organized data.
But the actual reason we didn't do it is a very mundane reason,
which is we couldn't figure out what else to do with it even if we built it.
Because there's enough regulatory stuff, enough kind of who wants it,
the hospitals want it, the doctors want it, the patients want it,
are they going to do the right things with it?
But we just decided it was, you know, it was a kind of a dead end in terms of the market for it.
But my strong suspicion is that there's a lot you can do with that,
and particularly, you know, at that time it was just the beginning, it was going to sensor-based medicine,
and the beginning of the time when, you know, my sort of classification of doctors might be,
can they read an ordinary chart, can they read an ordinary graph as a function of time?
Next, can they read a histogram?
And then next, you know, and then you go a couple more levels up.
And most practicing doctors, you know, fall down by the time you get pretty much to the histogram stage.
Which is, so it's not a highly technical, you know, it hasn't been a highly technical field.
I think that, and for a good reason, because there's not a lot of useful stuff you can get from that.
Except if you have, you know, if you have sensor data, you should be able to get something useful,
but often the medical practice says, you know, well all we can get is, is it A or is it B?
Which is, you know, there might be a huge amount of data there, and then it's like, is it A or is it B?
As a person who's interested in personal analytics type things, I've done all these random weird, you know,
metabolomics tests and all this kind of stuff.
And, you know, the main thing that you find from that is you get lots of data,
and nobody has any idea what to make of any of it.
And, you know, like I got my genome sequence back in 2010, and I think I was,
I think I was about human number 32, by the way.
And we happened to be working with a company that was doing a lot of sequencing stuff back then.
And it was, it was a somewhat useless experience.
And it was amusing that I had hired somebody to try and understand my genome.
And the first email from this person was, there are 770 diseases you might have.
You know, many of which would have killed me in the first, you know, day of life or something.
So, you know, we can know that one doesn't have those, but...
Talking about useless information, are you sure that most of us would not like to be rebooted?
Well, depends what you mean by rebooted.
I mean, that's the...
This becomes a complicated philosophical question.
Like, I know in my email I have some pitch from somebody...
Here, I don't know.
Somebody who has a company that is a whole brain emulation company.
They're trying to get into that.
And it's a bit unclear, you know, if you have...
If you meet your whole brain, you know, emulator, it's unclear...
I don't know what it would feel like to meet one's kind of successful whole brain emulator.
Whether you just say, well, you know, I'm kind of done.
My whole brain emulator is going to take over from now.
I think it'd be, you know, that's a weird experience.
And the other more useful question.
Do you think the reason...
The fact that we don't have the systematic medicine is that, let's say,
medicine is not a science, it's an art?
Well, medicine as it's practiced is a lot about humans interacting with humans.
And I will admit one of my sort of weird hobbies is that I've learnt a decent amount of medical diagnosis.
Nice.
And actually, many times I have, you know, people say, oh, some doctor told me this, that and the other.
And I say, that's really not plausible.
And, you know, it's... I'm very successful, actually.
I've had a bunch of diagnoses of people saying, you know, somebody says, I've had this.
You know, some doctor told me I had this five years or something.
And they say, you really can't possibly have this.
Just go and find a doctor whose specialty is that thing.
And they'll say, well, that doctor says they don't have this.
So, it is interesting, though.
I have noticed there is a characteristic probability error that is often made in medical diagnosis.
And, well, the first statement is, young doctors usually over-diagnose rare diseases.
Old doctors typically over-diagnose common diseases.
Because, you know, the young doctors have just seen in medical school all these probably exotic things.
And the old doctor is like, I've never seen a case of this weird tropical disease or whatever.
That couldn't possibly be what it is.
And, you know, but the other thing that happens that I've seen many times is people, you know,
at the beginning there are five possible diagnoses.
You do a bunch of tests.
The tests exclude three of those diagnoses.
You go chase the other two diagnoses for a long, long, long time.
Until it becomes clear that neither of those diagnoses is actually correct.
But you don't then think, let's go back to the original excluded diagnoses and re-examine those.
It's a really common mistake.
I mean, it's something where, you know, a computer system doing that would not make that mistake almost certainly.
It's the same thing as when that, you know, IBM publicity stunt of winning Jeopardy and so on was done.
I think the main thing that ultimately let the computer win was not that it actually did better at answering the questions,
but that it didn't make certain probability mistakes about how to, you know,
I don't actually even know how Jeopardy works.
I just remember that fact about how that competition works.
But, you know, I think in terms of, you know, right now, you know, medicine, as it's practiced,
there's a lot of decision trees that, you know, get used.
There's, you know, in terms of modeling, you know, very little of medicine is based on, like, modeling features of humans.
There's, you know, the endocrine system or something, you can make some models with the differential equations
about what's going on.
And you can make, you know, if you're trying to figure out, you know, some, I don't know, facial surgery thing,
and you're trying to figure out how will this, you know, if you make this change for a five-year-old kid,
how will that develop when the kid gets older, those kinds of things.
There's sort of calculation and stuff that we've done there.
But there's, you know, there's rather little that can be sort of, it has been successfully made kind of formal and computational.
You know, the thing that I was wondering is something much more global, which is, you know,
if we just think globally about sort of what goes wrong with the systems that we care about in medicine,
which is mostly us, you know, can we make any general statements?
What would it look like?
For example, in this, can you fix the operating system that's running?
It might be the case, it is almost certainly the case, that for reasons of sort of foundational computational reasons,
that there is an infinite hierarchy of things you have to fix.
That is that if you try and say, can I make this operating system as little as long as possible,
then there's maybe one thing that kills it in an hour, you fix that,
there's another thing that kills it another 15 minutes later, et cetera, et cetera, et cetera.
And it's sort of interesting to know, you know, can you, should you expect that there's an infinite tower of these things,
things that have to be fixed, and can you say anything about the frequency of those different things,
and, you know, whether one fix is likely to fix many things, and so on.
It's, you know, I think, I'm guessing that there are some fairly formal kind of computational things you'd say.
But the thing that sort of inspires that hope is that this little model of biological evolution that I've just made
turns out to reproduce and sort of explain a lot of known features of actual biological evolution,
even though it's a very simple, formal model.
So that's sort of the reason.
Does that have the hand up? No.
Actually, I have a...
I was just going to say, yeah, the reboot idea, it just kind of reminds me of, like,
psychedelic therapy for, like, PTSD and that kind of stuff.
It's kind of like a reboot.
Reboot your consciousness.
The thing that most reminds me of that, I have to show you.
Um...
Not that I know...
Well, it doesn't remove the whole system, it just removes your brain.
Right, right, right. It's a refresh.
And if you really want to look into it, there's a...
I know.
...packing at the bottom of thinking time.
Okay.
And it's a low, very low, a TNS that you apply to the brain.
Take a look at it.
So here's a way of sort of analyzing that, maybe.
Well, so this is a kind of AI system.
You're looking at it and you say, make a picture of a cat in a party hat.
You change the embedding vectors a bit.
You see sort of the cat island where it really is the cat concept.
And then it goes, as you change the parameters inside the neural net,
it kind of becomes, it goes into sort of concept space.
But the experiment that I did, I thought might be relevant here,
is you start off with...
Okay, so there's a...
You know, this is just a stable diffusion model, you know,
gendered AI system for making images.
And you say, let me modify the...
Yeah, which one is which?
But let me modify the weights inside these things.
I think this one is that you are zeroing out weights.
And so it's kind of...
I was referring to that as the HAL experiment, like from 2001,
where you gradually remove pieces of the brain and see what happens
and see what cat it imagines.
But then the other thing you can do is you just increase the weights.
I think this one's that.
I think this one is the progressively increasing...
You just crank up the weights.
So this is kind of...
This is sort of a conceptual model of what happens if you kind of crank up
all the neural connections in your brain.
What do you then think a cat is, so to speak?
And it kind of...
It has this sort of definite appearance of, you know,
blowing the mind of the AI, so to speak.
Yeah.
I was just going to ask, could you go back and show the low pruning then?
Because schizophrenia, there's some theories that schizophrenia might be associated with over-pruning, right?
So...
Okay.
I don't know if you could, like, go back to the slide.
Yeah, well, let's see.
See what it looks like on the other end of the spectrum?
Yes.
We can run...
I think I can run...
These experiments take some GPU effort, but I think we could probably actually run something.
I'm trying to remember which one.
So it's kind of...
This is like a knockout experiment.
That's lesions in brains.
That's the...
This is what...
This is the mental image that a brain that has been trained in a particular way,
but then has something knocked out of it, will produce for the cat.
So this is the equivalent of over-pruning, you'd say?
Like if...
I think so.
I mean, I think what I did...
And we didn't try on this experiment, but I think...
Let's see.
Zeroing out...
Okay.
So zeroing out single modules doesn't make much of a difference.
Like this is zeroing out just one module.
So what it basically does...
It's kind of interesting.
It changes the expression of the cat.
Yeah.
Which makes some kind of alliance with maybe some...
Yes.
Symptoms of...
Yes.
Yes.
Look, it is...
The loss of these issues, you know, psychiatric kinds of things, I suspect have correlates in
what happens in artificial neurons.
And it's...
It's...
There's other things that happen.
It's changing the position of the cat, blah, blah, blah, blah.
Actually, on the topic of this, if you don't mind me asking, have any of you heard about
hypnagogia?
Yeah.
Hypnagogia.
It's sort of this in-between phase of sleep or whatever.
There's also this thing where some people have it more than others where you get...
It sort of goes through noise and phosphine and it starts to literally go through that
process into a meaningful shape.
I don't know how many of you sort of noticed that.
Right?
And then there's actually something called Casina Meditation with a K. K-A-S-I-N-A.
And basically people will look at like a high contrast object, so not like staring at a bright light,
but look at something that's high enough contrast, where it imprints those sort of phosphines on your eyes
that your brain is perceiving.
And then people will try to focus and try to select these shapes out of it.
And they basically are doing that process.
Which I think is really fascinating about what it may say about biological biology.
Right.
I mean, look, there are an infinite number of experiments you can do with that.
I mean, one of the features of stuff I write is that I always try to make it so that you can click any picture
and get the code that makes that picture.
Now, unfortunately, this particular code, I suspect, is going to, yeah, this is going to use some GPU.
This is either just going to import the data or it needs to use a GPU.
So, which I don't have a useful version of here.
But kind of the idea, if somebody has a GPU, you should be able to just get this code
and actually run this and run the wrong version of this.
So, yeah.
There's like a paper in 1989, Yael Kuhn optimal brain damage.
It's about how much you have to prune to optimally make the network efficient to do representations.
And there's some, like, relationship between the complexity and managing sort of the amount of activations of some kind.
I don't remember all details.
But it's interesting to know that there's, like, in your developmental brain, like, growth.
There's, like, more learning happening quickly.
And then it sort of stabilizes.
And then it's, like, you've got more examples, like, which is kind of very different from how CNNs are trained.
But you have these, like, vision transformers.
They start to generalize their ability to kind of go across different images in a way that's strikingly similar to the activations in our brain,
which I don't know what it implies about the structure of how we process visual and also perceptual information.
But maybe there's something there.
I mean, not that much is known, right?
About, I mean, the primary visual cortex, we kind of know how that works for actual brains.
But higher-level processing seems to get difficult to know what's going on.
I mean, what's the, you know, the, like, those early things about how dropout is a good idea in getting trainable neural nets.
I don't, you know, people claim that there are neuroscience correlates of that.
I'm not, you know, so I don't know in the modern world of vision transformers.
Is there, I mean, I don't know if corresponds to me, somebody .
No, we didn't pick on Russia.
I do have one more thing to, sort of quickly mention about the damage thing and the hormesis thing in the building or down by the road.
Is how many of you have heard of this thing called, like, Savant syndrome?
Any of you?
Savant.
Well, there's an even weirder thing where there's this thing called acquired Savant syndrome.
It seems that people will undergo some type of light to heavy traumatic brain injury and then they, sort of, go through a recovery period and end up with, sort of, this very incredible, like, calculating ability of large amounts of information in a short amount of time.
It sort of just happens.
Or they have incredible calendar calculating ability.
There's this one guy, Jason Paget, who I really want to talk to.
He got, like, mugged at a bar or something like that in Seattle, like, 20 years ago.
And he basically, there was actually somebody at UM, I think it was called the Brokart Lab at the University of Miami that was studying this.
That they wrote it up as he had some type of synesthesia that made him think and experience everything in terms of shapes.
So everything had its own internal shape language just as, you know, the letters or whatever we have on our tablets or screens.
They're not, those letters don't mean anything in and of themselves.
It's because you're, sort of, trained on that.
And he trained on this other, seems like a different kind of way of interpreting things, which also ties into that thing about literally interpreting the same thing differently before with biology and medicine.
And I wonder, like, what are we missing about the computational properties of brain and biology in general, if there seems to be, like, these really odd abilities.
And especially because they happen so, like, acutely compared to, like, trying to manually learn a skill.
I'm just wondering what that is.
I don't know.
I think, I mean, this whole question about associating different modalities of things, I mean, that is eminently studyable, I think, in artificial neural nets.
But I don't know of any synesthesia studies.
Yeah.
I mean, I did, one of my kids happens to have synesthesia.
Yeah.
And she, like, just recently read this study, which says that actually the association of particular colors with particular letters and so on might be the result.
might be the result of having, you know, learnt your ABCs with a particular, you know, chart that has, you know, the A in blue, et cetera, et cetera, et cetera.
Yeah.
So I have been meaning, since I read that, I've been meaning to try and actually make that connection.
Yeah.
Still have, probably, the original, you know, ABC charts and so on and see whether those are the colors.
Yeah.
Because otherwise it's unclear why somebody would associate, you know, purple with seven or whatever this is.
Correct.
And why would they pick that color?
I mean, unless there's some real internal sort of brain connection with colors.
But I think it's much more plausible.
Yeah.
It's just that was the color of what happened to learn first.
I look at the data for sound.
So the number of people experience synesthesia for sound and often associated with colors.
And it seems that most of them assign the same colors to the same sounds, but not all of them.
So there is variation in this.
And there was also some hypothesis that this is related to a particular generation getting
silophones or whatever of certain colors that are associated in this way.
But it's not clear if that is actually the reason.
But there is variation in it, so there is no one-on-one mapping.
What we find interestingly when we think about it mathematically, color is some kind of polar
coordinate representation in the brain, mathematically speaking.
Right?
You have some kind of intensity as an angle that-
I mean, there's three parameters.
Yes.
I mean, usually-
Depends on how many colors you represent.
But yes.
So you can have a play out in more than two dimensions, but it's color space is something
that you can represent using angles and intensities.
And when you get experiential access on this layer in which the computation is being made,
then you might have synesthesia.
I suspect if you think of the brain as a muddy layer system where every hop between one year
to the next is a layer.
Sometimes you probably integrate by having to send information over enough layers.
And if you are deficient in one neurotransmitter dimension and some of the receptors are not
working well, you end up with difficulty to integrate over deep layers.
And you have to integrate shallowly and you might have to stim to get your model of reality
stable.
You have typical symptoms of autism.
And the inverse might include synesthesia, that you integrate over too many layers and
you basically get experiential access to underlying computations that are not actually semantically
relevant.
And they're just the syntax of the geometry that you're producing.
Well, one knows in the primary visor cortex that there are, you know, these weird zebra-striped
looking patterns where both each eye, you know, feeds in and also different color.
There are these patches, you know, where the different, you know, color receptors end up sort
of mapping onto the visor cortex.
So I suppose your theory might be that you're, you know, when you think you're perceiving something
at a higher level, you're actually seeing through to that lower level representation
in which there is a coordinate system basically for these colors.
You can typically induce synesthesia in people by giving them serotonergic drugs, which leads
to some desinhibition and maybe you are basically integrating over more layers in that state.
It also leads often to people having a stronger sense of empathy, perceptual empathy with other
people.
So it seems to be that we are integrating information more.
Also incidentally, people tend to overfit on these substances.
What were you talking about?
SSI?
No, psychedelics.
So substances like LSD and MDA, for instance.
Just for my information, you very quickly said, and the theory for autism is, what was that?
So basically, if you integrate not over enough layers, if you are not able to integrate very
deeply, you will be forced to find patterns in the layers directly below the concept that
you are forming.
And this means that you will tend to be in a world that is very fairly shallowly integrated.
I think the reason why computers are catnip for people with autism like me is because it's a world that is entirely
made of shallow scripts.
You don't have to see deeply.
You don't have to look for patterns at the previous layer and integrate over them.
So hallucination is the artificial synergetic stage?
Hallucination is basically non-predictive tracking of sensory data.
Normally, your sensory data is tuned to predict what happens next on your retina and on your
skin.
And if it's not, if it spins off, it happens in this schizophrenia or so that your representations
are not constrained, but what makes them predictive?
And you tend to end up in world representations that don't make you successful.
Because they're not coordinated with the events that you're going to observe.
Hey, I'm curious.
What kinds of things do all of you study over all of you?
Do you have identified that?
One thing I want to ask about the inducible synergies.
Do you remember the whole factory thing I was telling you about before?
Yes.
Okay.
So this is kind of weird.
Maybe somebody else has tried this before.
When I was a kid, I used to take my parents' fragrances.
Take to break the sealer or whatever.
And I would try to make these patterns in nature scenes just to represent whatever shapes,
whatever it was.
And I would try to really smell that.
Like really focus hard on smelling and differentiating those smells.
And then associating that with either another smell or an existing idea or a shape or whatever.
And I noticed that if I started doing this enough, like after a week of trying it,
I started thinking in terms of smells.
Because my initial idea is I wondered how dogs could perceive the world.
A lot of us usually think, maybe some of us think more visually, spatially.
Some of us are better dealing with thinking in terms of sounds and music or visuals, words, sound, whatever.
But how many people are experiencing the world in terms of smells?
Because the weird thing, and I still have a bit of this today, is I have an internal smell that I associate with that.
It doesn't mean that jacket smells like that.
It's a completely, seemingly uncorrelated thing that I've correlated in my head.
And what was really cool is that I could then have a separate piece of paper, and I even did this with some friends.
And if you have these smell patterns or whatever, you'll smell through it and you're reading it like you're reading a normal language on a piece of paper.
But it's through smell.
And you're thinking just how you can combine two different words together, in terms of the orders of the letters to make different things,
or shapes together or visuals together like you're an artist.
You can do that with smell.
And you're actually thinking like if you have two smells and you try to combine them, you get like a third smell.
I mean, that's like color mixing.
Yes.
I ran this company recently that claims to have finally decoded smell space.
Because for color space, we know there are three color receptors that we have.
We know that any color that we can perceive is more of a combination of those.
We were like mantis.
We were dogs.
There's only two color receptors.
Mantis shrimps are said to have 15 color receptors, et cetera.
Some, it's claimed with varying degrees of convincingness that, you know, the XX chromosome, you know, females of a species can have, you know,
two different populations of, I think, blue cum cells, which leads to sort of hyperacuity in color, if you can sort of decode those things.
So, the XY chromosomes don't get that.
But this thing, let me see if I can look up this company and what it's called.
Joshua, could you comment a bit more about perceptual empathy?
How do you think it works?
I think that it's a model of the state that the other person is in that is not happening by an asynchronous inference,
but it's happening basically in real time on the perception level.
And I suspect it works by building the feedback loop, so the other person.
In some sense, you're basically picking up on signals in this other person, and it can be bi-directional.
And in this way, you can experience states that you cannot experience alone.
For instance, you notice this when people are playing jazz and they get in a groove together,
that they will be in a state where they can play music together that they could not play alone.
And that's because they get into some mutual feedback where their substrates, in some sense, are interacting so much via their sensory channels
and through their bodies that become perceivable through those sensory channels.
And as a result, their mental representatives can start to interact.
So something interesting is that people who have autism empathize with autistic people a lot more,
but they can't do as much with neurotypical people.
And so is the converse, also true.
So it's like, is it just that if something is far enough away from your experiences, like another agent that you represent,
you can't really well extrapolate what they might do next.
And so you kind of maybe give up trying to do that feedback loop in your attempts to establish it.
It's basically you are not vibing, you are not resonating.
In the same way, the mental representations that you form are not aligned
because the architectures that you use are too different.
And you would need to introduce them in translation layer.
And when this translation layer becomes very asynchronous because it requires multiple steps of inference,
it will be difficult to track this in real time.
The other issue is that most people are neurotypical.
And as a result, they learn to account for each other, but not for the handful of weirdos among them.
And so that takes time to get ground truth for people who just have rare setups.
I noticed when I was younger that I was failing in many social interactions and I was trying to find out what was going on.
And so I did the Igma tests and thought, oh, I'm just okay for a male in recognizing emotion.
There was nothing wrong.
Then I read psychology books about emotion.
I didn't learn anything.
It took me a long time to realize that most of these psychologists who write these books have some form of autism
because in order to write a long book about a topic, you're already afraid on some outlier part.
And the people who are actually capable of dealing with emotion very well and could have told me what was going on,
don't write books like that.
So I'm curious.
If you make chatbots that are suitable for neurotypical people, could you make a different chatbot that is more suitable for autistic folk?
Yes, very clearly.
Because these current chatbots are context sensitive, right?
The way in which they learn is not that they average over all the people, but they learn very specifically in which context you have which kind of interaction.
And so I think this attempt of averaging that AI companies are doing is very ugly because it just remove a lot of the signal in the data.
In some sense, we could use these chatbots as tools to study psychology, politics, milieus, and so on.
It is extremely fine-brained resolution because it knows all the different social media personality tests.
What it picks up is not bias.
It picks up the actual ground truth in the data and we can prompt the LLM to be an arbitrary demographic
and then give them as many questions as we want.
Right. So there's a company that I was interacting with recently that is precisely trying to do this for Poland.
They're saying, you know, you're an LLM, imagine that you're a, you know, whatever, a 30-year-old, you know, plumber from Cleveland, etc., etc., etc.
And they were claiming, as of about a month or so ago, they were claiming dramatic success in Poland kinds of things.
I mean, they were signing up lots of political campaigns in the U.S. for the upcoming election and so on.
So they should be able to make billions on prediction markets, right?
Well, isn't it that, like, if everyone uses a system like that and will saturate it such that you can't predict anyone else
because reflexively it will change the complexity of agent self-representation in other groups.
Although you could say the same thing about quantitative finance.
Yeah.
That's where people manage to...
Well, they can work on certain challenges and now everyone's like, well...
You can also say this about science itself, right?
There is this danger that at some point the AI models are picking up mostly output from previous AI models on the internet
because so much content is now polluted by the AI models.
But science is also a model that is mostly trained by its own output and so it becomes slow.
You could also say this about society, like, as we're more connected to each other.
There's, you know, we're seeing all the same information and stuff.
I think the other difficulty to answer in your original question would be that the neurotypical people are more clustered together.
Whereas non-neurotypical, they're much more spread out in a very...
You know, you could be neurotypical in many different ways, right?
So they're a lot more dispersed.
Well, I mean, this whole thing about chatbots is, I mean, similar to, like, drug trials, right?
Where the way you have to, you know, there may be some subpopulation where this drug will do fantastically.
And obviously the drug companies all try and gain clinical trials so that they actually just give the drug to people
for whom it's likely to be successful.
But it's still, you know, you end up with something where this drug to be approved has to not do anything terribly bad to almost anybody.
And, you know, that's, so you end up with that same phenomenon.
So you think that there will be, ultimately, there'll be, you know, the question is could you go through the web or something
and identify, you know, non-neurotypical, you know, text as compared to, say, this author was kind of a borderline autistic whatever.
This author was very neurotypical.
And then, you know, distinguish these things in terms of the training set.
I guess, like, the concern that I have is that it's setting the chat, like, the chatbot as the control for neurotypical already, right?
I know from, like, a medicine standpoint, like what you're saying, there's no central dogma.
So same thing for neurotypicalness.
Like, there's not one person walking around and we're like, that is the neurotypical man, right?
Like, it's a little, it's almost giving, I feel, I fear it might be giving too much power to that.
And also because now people are masking to speak like ChatGPT.
It's like seen as the accepted tone of voice, the way of talking in academia and whatnot.
And I know I feel this person.
You think so?
I feel like it.
Oh, yeah.
I feel like it.
Academia is merging into, I don't know.
Yeah.
It's good when you hear other voices, but it's true that a lot of, especially if you go to other schools and whatnot, like, it's becoming the norm of how you should talk.
So I worry about the cultural effect and if that's going to make then people who feel, or who have a vocal voice, like, I have an expressive voice, and it's like, I've always had to, like, struggle to mask that.
And I used to, like, almost use chatbots as like, okay, this is how I should structure it for academia, right?
But I don't know.
Well, this is how the AI's take over.
So the elimination of our voices, in-soc management.
I don't mean to make it cynical.
I don't think it's too cynical.
No, but it's a fairly amusing scenario.
If everybody is saying, if you're channeling, I want to talk like a chatbot.
It's kind of like the AI's in charge.
We're done.
You know, humans might as well, you know, advocate at that point.
I think there's something to be added here for the fact that, yeah, academia is being clearly infiltrated.
There's a bunch of papers about how the number of use, yeah, it's that paper.
Yeah.
The word Delve, like, there's a, it's skyrocketed in, like, the number of papers it's being used in.
And that's because ChatGPT really likes using Delve.
It's because they had Nigerian.
Right.
Yeah.
So, yeah, it's not really a question about our voice being like, you know, like fizzling out.
I think it's more of a question of laziness.
And the essential problem here is you always, you always want more and more training data.
You want better training data.
But the thing is, right now the approach is to throw, like, just piles and piles of data as much as you can get because it's producing good results.
At some point, when they're starting to hit a bottleneck where they're making synthetic data.
And the problem there is, again, now you've got rubbish in your data.
And how are you going to sort through that?
Because on the surface, it does look good.
But on the other hand, when you're training it on that quality data, you notice that there is a lot more information density in what you've got.
So I think the difference in academia or like, you know, learning that, OK, the way how chat bus talk is how I should write for academia.
It doesn't really work that way because anyone who's written probably more than like three or four papers will notice the best papers have a voice to them.
So if you're getting into academia, I feel like it doesn't really stay relevant there.
I feel like it's mostly people who are experimenting with it and are using it either to be lazy with their work or they are insecure about their work.
There's the publish or perish kind of mentality, which uses a lot of low quality slop, even without the AI.
Exactly.
Just like people are like, oh, I need to get something to validate my grant to do this thing.
But it's like an incremental thing that doesn't really take a strong enough position.
You know, the bad thing is that there are many fields where you just have to file documents about something.
You know, you're trying to get some opinion about something.
You know, I just recently have a couple of different things.
Get, you know, firms to release opinions about this or that.
It's a 200 page document that I know nobody is going to read.
You know, but it's like you're filing it with the government for some particular thing.
And it's kind of like it's weighty enough that you can't, you know, nobody's going to immediately say this is nonsense type thing.
And academic papers can turn into something a bit like that.
That is, you're just papering, you know, you've found something, you've got to produce, you know, the paperwork to go along with what you've found.
I mean, perhaps, I'm not a big fan of academic papers.
I haven't written one since 1986.
Wow.
So, that's, and yet, you know, I write a lot of stuff.
And my stuff gets read a fair amount by, you know, huge factors more than it would have been in academic journals.
Like, decent posts that I write will have a couple hundred thousand views very quickly, which is nice.
And if it was a journal article, I'd probably, you know, I'd be lucky if it had 25, you know, citations or something.
And it, so, but I think, you know, when I look at journal articles, I, most of them are really boring, they're really incremental.
The thing that drives me crazy is that they, you know, I'm reading something, it's like something exciting is going to happen.
And then, bam, I hit the end of the paper, and it's a big pile of references that I pretty much know were just, you know, coming out of some, some fairly automated system.
So, I've been, and I also think, you know, the early academic papers, when journals were first invented in the 1600s, they were much chattier.
And they're much more like the kinds of stuff I write in the blog posts and so on.
The typical, you know, you look at the prosumis of the Royal Society from the late 1600s.
And it's much easier to communicate with humans in that kind of, you know, a style where you can sort of say anything than this rather formalized style.
I mean, different fields have different levels of formalization.
If you're reading patents, for example, patents are really unbeatable because they're structured in a particular way.
Was the Royal Society papers free? Not to be too cynical, but is that possible because now the most journals you have to pay for?
I don't think they were free. I mean, they were printed, and the printing was expensive in the United States.
And I don't know who got, I mean, I assume that the members of the Royal Act.
It's a good question. I have no idea. You go to the library to read. Remember the library?
I mean, I remember doing research where I would have to, like, you know, go to whatever journal for whatever year, and, like, actually, the print thing was there.
Sure. That's what I used to have.
Could I add something in here? I think the saturation thing you were talking about.
I have a funny story which is strangely relevant here, but essentially, so, I play the piano.
Um, and, uh, earlier, um, I was talking, uh, was, was joking about how, you know, you're, like, a descendant of Mark or whatever.
Um, so, it turns out I have a direct, a direct teacher lineage, not genetic lineage.
So, my, my teachers, teachers, teachers, teachers, teacher, uh, was Bach.
And so, my, my, um, piano teacher, she did a lot of, like, research into, like, the history of Western classical music because of that.
And she got actual, like, access to some of Bach's notes and, uh, over time, his students and all of that.
And the interesting thing is, Western classical music, uh, when it was played, it was mostly played in salon concerts.
It was very informal. Um, it would be just usually a bunch of, uh, friends who were invited, family.
And the, the main thing which struck me was nearly all of them were improvised on the spot from some sort of theme.
Um, and it was extremely informal that way. And it was meant to be improvised.
So, whenever you see, like, a Western classical piece, uh, that was written before approximately 1830 or so,
you can assume it was meant to be at least slightly improvised upon. And that would happen.
There are records of, like, many prolific composers who would start off with a theme, and then they would keep iterating on it just for a performance.
And I think, like, over time, when you're trying to learn, uh, like, essentially the greats of a certain field,
you tend to develop a formalism because you put them on a pedestal, but then you're losing, like, what made the field so strikingly elegant.
Which was the fact that you can adapt it to your audience. You can, uh, think about it in different ways.
You can explore new pathways which lead to new movements, essentially.
And I feel like that happened to Western classical music, and that could be what's happening here.
You know, one thing that I've tried to do is kind of, if you're doing science, do open science, where you have, you know, you actually live stream the discussions you're having, the experiments you do, and so on.
And I've done tons and tons of that. And, uh, it's a very interesting dynamic. I, it's amazes me that academics have not done it. I don't know a single one has been prepared to, you know, live stream on their lab.
I've been recording, like, I don't know if you guys noticed earlier today, I recorded my own talk on, on Zoom.
And I've been recording all of my research conversations for the last two decades.
And this basically provides an archive for how ideas develop. Everything is, like, labeled, organized, traceable, like, AI parsed, etc.
And, uh, the life component for us professors is, like, fine. But for the students, it's a little uneasy.
In the sense that they're like, okay, well, I have my research idea that I'm working on, and now my competitors will be able to watch it.
So, I mean, we can say that about our company, too, but I don't worry about it too much.
Again, you're at a different stage of your career, so basically, I, well, I know, my company is still a company, well, I mean, I know.
It still doesn't have tenure.
But, uh, tenure, long ago.
You don't need to defend yourself, it's okay.
I don't even wish that more people would try to copy what you're doing. Your stuff is so unique that you would always beg for more people to try to copy you.
Because, you know, frankly, there's an enormous barrier to entry. In some fields, the barrier to entry is lower. And there, the idea is perhaps worth a little bit more.
But is the reason that the barrier of entry is lower is because the research is not so exciting and that you hit the end of the wall when you finish reading the papers, he said?
So, so, for some things, after you've seen them, they're obvious. Before you've seen them, you're not quite there yet.
And, uh, there's many papers that I've written and after the fact they're just like the natural thing.
But to get to that clarity of thought, it took a long time. After you've seen it, of course, like, it makes complete sense.
So, you know, I don't want to dismiss something as, like, basically, unless you've written it, it's very easy to sort of get caught into the trap or, you know.
Science and academia are not the same thing. They're two different ideas. Academia is, in many ways, a praxis.
And it's set up in such a way that you get certain certificates in a very narrow field and then you are entitled to say that 2 plus 2 equals 5
and only the people visiting your narrow field can talk back to you. And it makes you safe and it allows you to create arbitrary subcultures
that allow you to, uh, may enter in any kind of direction and very often.
A lot of scientists are basically not people who are driven by the need to do science, to do work at the edge of uncertainty and, uh, take risks or whatever.
There are people where the guidance counselor said it's good if you are somebody who applies methods.
I think, Yosha, I think that the fundamental thing is science is a victim of its own success.
When science was much smaller, the only people who did it were people who actually really wanted to do it.
But when it becomes a profession where lots of people can do it, you end up with a situation where the people who are doing it aren't necessarily the most, you know,
don't have to be the most motivated because it's actually fairly easy to be a scientist, so to speak.
You don't have to really be out of it.
As a science, I want to beg to disagree.
Okay.
And basically say that it's so much easier to just go off and do something else.
It's like, basically, I feel that the level of sacrifice that one needs to make to be a scientist is enormous.
Really?
Really?
Yeah, of course.
I mean, well paid.
I mean, well paid.
Are you kidding me?
So, industry salaries, at the level of science that a professor does, industry salaries are like three times your pay.
Yeah, in industry, you need to be useful.
That's...
You are decoupled from whether the stuff that you're doing is actually useful.
I found that when the time where I was blessed to be allowed to work at the Media Lab, there was this freedom that is being described.
I did not experience the same thing in academia in Germany.
It was much more streamlined to what was currently being funded.
There's also a mode that you are alluding to in the way in which many people operate.
Most people, most neurotypicals are parts of hive minds, which means that they do not feel entitled to have independent thoughts in the dimensions that the hive mind is controlling.
So, when you talk to somebody who is possessed by a political idea, there is typically no way that you can change their opinion with a rational argument.
Because if they would change their opinion, they would lose all their threats.
Right?
So, their employment often depends on having a certain set of opinions.
A similar thing happens in many areas of academia.
You are expected to have certain opinions to become the president of Harvard.
You should not be edgy.
You should not be violating expectations in any way.
Right?
So, there is this idea of the scientist who is the independent thinker who goes out to explore and makes enemies and then vanquishes them.
With a sword.
No way to play by the rules within your local discipline.
And so, when you look at, for instance, Steven's journey, who is somebody who fundamentally rejected the hive mind and tried to build an autonomous alternative.
And because he was unable to integrate at a certain point, he spun off and built his own part of science that then became big enough to now get integrated back in it and affect it again.
But it's a very typical journey for a scientist to do it like this and normally it doesn't succeed.
Well, although, to be fair, in my own case, you know, I was a completely successful academic who, you know, to many people would have just been seen as an integrated, you know, standard physics professor or whatever else kind of thing.
It so happened that the topics that I've been interested in are, you know, I was lucky because I got pretty early in my career to the point where I was a respectable, tenured, operative,
to doing...
You were highly on your science.
You were an absolute prodigy and outlier.
Yeah, okay.
Very funny.
I think he's still in.
Anyway, we had various hands.
I saw your hand.
Yes.
Oh, yeah.
So, I've been thinking about this topic that early academia used to be different than current academia for a while.
And it's interesting, on the one hand, I'm interested in automating science, which goes more into the first part of what we talked about, AI being able to do the entire academic process of what we're doing right now.
But I think that when it comes to the more individual type of science that is based on the person, we talk about longevity and one of my hypotheses is that it actually used to be a lot easier when we were fewer people because of competition.
Today, it's much harder to get a Nobel Prize than it was at the beginning of the century.
Was there even a Nobel Prize?
No, sorry.
There was.
Yes, there was.
There was?
Okay.
Because we were just like a quarter of the people we are today.
True.
And I think that might contribute to a hive mind that might build up in systems.
Yeah, yeah.
I mean, this is, you know, you guys may know this, but I mean, there's typically a cycle of how things work in science and technology and so on.
Some new methodologies invented, then there's a period of five, 10, maybe 20, 25 years, when there's a bunch of low hanging fruit to be picked.
You know, the field advances rapidly, then more and more people go into the field, the field becomes institutionalized, then for the next hundred years, this kind of slow growth.
And, you know, and that's why people often say, you know, oh, nothing is happening, because they're looking at the fields that have already been established.
Right.
Where, you know, where it's in the cruise mode, and they're, you know, they're not aware of the things which are happening on the edges where there's high growth.
And I think, you know, in science, for example, in any entrepreneurial field, in any field that's fresh and new, there aren't that many people.
It's a complicated dynamic, because I've been there a bunch of times, and I've built a few of these little, you know, directions in science.
And it's complicated, because you get, so my first observation is, when you start a new thing, about, and you get a bunch of people coming to that new thing.
If you wait 20 years, you ask what fraction of the people who got into the thing early are still there.
About half are usually still there, in my experience.
So half have moved on to many other new things, and half were people who had random stages in their careers.
The thing that came up, you know, started to exist, was a thing that resonated with them, and they've been kind of looking for it all their lives.
And they finally find it, and they keep doing it forever.
But, sort of, the dynamics of what happens when fields are young is quite interesting.
I mean, people come in from other neighboring fields, and they say, well, I'm really doing this field that's now trendy.
But, like people say, everybody says they're doing machine learning, even if the closest they got was some piece of statistics.
And, you know, so that's one dynamic.
The other dynamic is there are people who are, sort of, frankly, on the, you know, they will be flaky in any situation.
They're kind of the, you know, they're always on the edge, so to speak.
And then, you know, it's a complicated dynamic if you're trying to manage a new field, trying to navigate it.
But how much do you deal with people who are coming in from another field who really aren't doing the field that you're trying to invent?
How much do you deal with people who are kind of, you know, obviously doing slightly crazy things,
but they are at least brought into the concepts of the new field and so on?
But, you know, I think the science, you know, this business about how, you know, once things are institutionalized,
once it's big enough that it's institutionalized, that there's, you know, the high innovation period is over.
But I want to ask, sorry, I just got hands, but follow up.
A lot of the, a lot of pioneers in their fields were often quite young.
So maybe in the 20s to 40s when they were doing it, even the founding fathers.
But also Turing or the Careers and these people.
And we talked about reboot earlier.
Maybe to progress, do we maybe need to reboot and actually die?
You know, this theory that science is done by the young, I believe is false.
That innovative science is done by the young is simply false.
I looked at this at one point.
I think I even have some blog posts where I can talk about this somewhere.
Because I just looked at the data and the data does not support that claim.
So in other words, if you look at mathematicians, which is a very typical.
So I think that claim comes particularly from this guy called G.H. Hardy, who is a mathematician, who I've talked to people who knew him.
He was kind of this sort of depressive guy who wrote this book called The Mathematician's Apology.
Where he basically sort of said, you know, after you're 25, you're all washed up in mathematics.
It's just not true.
You know, if you actually look at when people did their important work, it's often much later.
And in fact, one of the things that's true is that there are some fields where, you know, the importance comes from integrating lots of things.
And you kind of have to be older and have to have done more stuff for that to be possible.
I mean, I can tell you many stories about people who, well, it's, you know, I think this, you know, the other thing is that people, you know, most of us,
have at most one way of thinking and one good idea in our lives.
And, you know, for, it's, it's kind of, it's like, and if your way of thinking is, you know, it could be that you come in with your way of thinking,
and that, and you immediately, you know, there's low hanging fruit to pick.
A broken clock is right by sitting.
Yes.
But I mean, the, the, you know, you come in, there's low hanging fruit, age 25 or whatever, you find that low hanging fruit,
your way of thinking is a match, you've got to win.
Sometimes, that way of thinking, you know, it may not align that way.
And it, you know, takes longer to find the thing that, for which it's a match.
I don't know, I think it's, the, the claim that, sort of, the great ideas in science are done by the young, I think is, is empirically false.
I want to pick up a little bit on what you're saying about your way of thinking.
Basically, again, I, I studied the human brain, I studied genetics, I studied variation.
And when you say your way of thinking, I mean, again, a lot of us, we're, we're talking about the hive mind.
And at the same time, we are programmed genetically to be different from each other.
Whether we, we want it or not, we don't have two people on the planet with the same set of neurons.
You know, the same set of parameters for how the neurons are even developed.
Or the same developmental program or the same experience and so forth.
And there's this nature-nurture interplay of what determines the way you think.
How many kids do now?
Three.
Okay, I have to take that.
So, do you have any sample size?
Yeah.
And they, they, they, they, just, just for those of you who don't have kids yet, they say,
a parent with one kid believes in genetics.
A parent with two children understands genetics.
And as soon as you have one kid, you're like, oh, there's someone who likes me.
And as soon as you want to go with me, you're like, totally different.
Like computer experiments and so on, is that, you know, there's a style of doing them
where you could actually do nice, clean experiments.
But, you know, you kind of, the chips fall where they fall.
And you can have some guess about what's going to happen.
But basically, if you take that guess too seriously, you will not get the right answer.
Because the, the thing that I find all the time is that, you know, these little computational systems,
they're always, they always manage to do something that you never expected.
They, they, my sort of statement is, you know, the computational animals are always smarter than we are.
Because they always find a way to do something that, you know, was, was not what you expected.
So I, I don't know what was happening in that experiment.
That's what I want to try doing.
Yeah.
Yeah.
So, I just want to make a comment.
So because of the deluge of data and instruments, now we have like, in mass, a structural molecular biology
for like a massive amount of organisms across the planet.
We have these foundation models that can actually serve to understand the structure inherently in that
and try to uncover kind of patterns that sort of we can't necessarily see based on our analysis.
Like, is it possible that we kind of figure out how to do it without understanding how exactly we're able to sort of control disease,
like even without a theory of disease?
Isn't that what most of medicine is, right?
I mean like...
Most of medicine is empirically, you know, turned out this, this thing you can do will prevent this disease.
I mean, almost none of medicine is theoretical based.
Sometimes people, when they initially develop a drug, have a theory for why it might work.
But usually in the end, it's, you know, in the end, the standard is you do a double blind clinical trial and you see what happens.
And nobody expects the theory of what happens.
Yeah.
Not to jump off of that, but like, I'm interested in religious cognition as well and like magic and philosophy of science and whatnot.
So much of medicine, if it's early, we're talking about scientists, where did they start and whatnot.
It started with like religious scholars.
It started with magic.
It started with those shamans and whatnot.
So even if you go through history, you'll see that's kind of what you're saying.
Yeah.
Well, I mean, right.
And it also, you know, an embarrassment for Western medicine is that other forms of medicine actually do seem to work at some level,
which is kind of an indication that, you know, the kind of, you know, the system is yet more complicated than we imagined.
And, you know, the purely chemical processes that we can control the standard Western medicine may not be the whole story and et cetera, et cetera, et cetera.
I mean, I think it's a, I don't know.
I mean, yeah, it's, it is very, you know, this feature of biology and medicine in particular that, you know, the simple hypothesis almost never correct.
That there's always, you know, always exceptions to exceptions, et cetera, et cetera, et cetera.
It's a hard thing, I don't know.
The most extraordinary thing, in my view, is the placebo effect.
The fact that the placebo effect even exists is extraordinary.
Basically means that our brain, our mind has so much ability to affect our life.
There's no question that you can brainwash yourself better than anybody else can brainwash you.
That's not even a question.
Yeah, but, you know, one of the things that this is part of the biology is always more complicated than you possibly expect.
You know, you might think there were nerves that go, you know, from the brain to the gut or something like this.
But then it turns out, you know, they're in a book, the other way down, or the sensors and so on.
Then it turns out, you know, there's, there's nerve fibers going in both directions everywhere.
And it's kind of, so it isn't the case that, you know, you're just getting sensory input from places.
There's, you know, there's, there's more communication than I would expect.
And it's, it's, you know, this thing about, for example, you know,
the importance of microbiome on, on, you know, neurological kinds of things.
It's totally weird at some level.
But yet, the whole system is, I mean, by the way, you know, like these pictures that I was showing.
So there are molecular mechanisms, by the way.
You know, all the brain axis is there.
And, you know, there's a, there's a mechanistic basis.
But basically, the reason why I'm picking up on placebo effect, because, you know, some of those traditional medicines are just simply, you feel that you're getting cured, and you're getting cured.
Can I, can I respond to that too?
We kind of, at least to what he was saying earlier about the, the, okay, I love that idea about autism being in layers maybe on top of each other,
or maybe some type of overlap within the model.
Because if we're talking about placebo effect, perhaps placebo effect is utilization of those, like, connections through, like, stimming.
Which is great.
Right?
Stimming.
Yeah.
When I dance to certain music, it looks similar to certain stimming methods, right?
Right.
Like that, or similar, there are universal factors that might be possible at times, but I'm positing.
No, and I'm, I'll go on you.
So, so the placebo effect is easier to understand in the opposite direction.
If you're stressed out, you're going to be sicker.
Yeah.
That, we can all agree.
But then, the opposite is something that we have more trouble with.
But you could think of it as simply, oh, I'm less stressed, and then my physiology is not affected by my negative thoughts.
Could be one thing.
Music.
I also wanted to respond to something that you said earlier.
After an incredibly long question in theory as an argument, I've been persuaded by some of your favorite students,
that over 90%, even its papers in computer science, not the social sciences, are false.
And computer science, for the most part, is math.
Pure math.
And, you know, it's either true or false.
It's not, like, maybe close enough.
I don't know.
I don't know.
It's like, I know it's an operating system, and it's supposed to do this.
Does it do it, or does it not?
Yeah.
Well, no.
No, but it's that.
That's, that's.
So, so, so, I want to, I want to distinguish wrong and wrong.
In other words, if something is not 99.99999% correct with that wrong, you know, if they're completely false, that's a whole other story.
But like.
But where this is completely false.
Where that line is.
I feel like in an industry, that line is very clear.
It either works and people buy it and pay for it, or it doesn't work.
So there was a spoof paper called data set selection.
And it would basically say, here we show how selecting the right data set can basically
improve our method.
And it's like, well, guess what?
Every paper actually does that.
And, you know, like, as soon as you have empirical results in a computer science paper, there's
a huge bias in, you know, exactly what you had.
And the only progress that we've seen in some of those fields, when the whole field comes
together and agrees, here are the benchmarks.
And here's the data set that everybody wants to have.
And there you see actual progress.
But until the data sets and the benchmarking is standardized, then it's really a wild, wild
waste.
Well, and there's a certain amount of cherry picking that is useful for the author of the
paper to do.
Because, you know, without that, it's like, you know, if I don't show you the interesting
example, the ground is a thousand, even though I've had it always.
Yeah.
So I was watching this interview by Eric Schmidt, and he goes on to say that the next five years
for like AI, it's going to be like an AI, like a startup going to a hospital and collecting
data and training an AI model and selling back that model to that hospital or to the doctor,
you know, do some medical work.
What do you think about this?
And where do you see AI going in the next five years?
How do you think it is?
I think, I mean, by the way, that model of how one might use AI is bizarrely similar to
the model people have in the 1980s for how AI would work.
At that time, there were these things called expert systems.
And kind of the idea was you go and interview the expert, you specify what the expert does in
terms of a bunch of rules on the computer, and you don't need the expert.
But, you know, I have to say, I think, you know, I think what we have with LLMs, for example,
right now, is kind of pretty much what we're going to, you know, there'll be some incremental
improvement, and there's some definite directions one can see for improving things.
But I doubt, I think, you know, if you look at the history of machine learning, it has gone
in a series of steps.
There was one in 2011, one in 2022, et cetera, et cetera, et cetera.
It has not been, you know, people say they have one data point, which is chat GPT, which
is a big surprise to everybody, putting people working on it.
And, you know, then they extrapolate from that one data point.
It's going to go here, and that's going to go up here, and that's going to go on.
I don't think that's what's going to happen.
I think the main things that are going on are things where one has a certain methodology,
and now it's a question of harnessing that methodology where it is useful.
And, you know, with typical, you know, if you want to make some precise computation
of something, you shouldn't try using chat GPT.
It will not work.
You know, instead, you should, well, use, for example, our tech stack, which actually
is a cute stack.
But, you know, but if you want to make some kind of rough, you know,
rough judgment call where you're right 80% of the time it's a big win, then it's worthwhile
to use, you know, to use some AI system.
I think that's, you know, so I think what's emerging right now is, you know, the use cases
for these things where it fits in well are, you know, being discovered, and people will be
very successful with those.
You know, but the idea that it's kind of, you know, that's just going to take over everything.
I think, I don't know what the opinion about that is around here.
I was at some Silicon Valley thing recently.
And I think my impression was that about 5% of the people there were true believers,
who just were like, you know, that's just going to solve, we just feed in enough data,
and you're on that sort of sold out for them.
Maybe 15 or 20% of the skeptics.
I'd like to add a perspective here, which is that humans think in intuitive ways.
And Chagipity can do that.
But humans also spend 20 years in school to think, to learn how to think like a machine,
with flowcharts and, you know, steps and et cetera.
And machines have been great at that, but LLNs are not, because LLNs are more the intuitive thinking.
Yeah.
When we are thinking about extraordinarily creative humans,
sometimes it's that ability to combine the two, to basically have the step-by-step thinking
and sort of have all of the rules that you're applying, like an expert system,
and at the same time sort of make these, you know, weird connections.
And I would say that we had the time of the expert system.
We had the time of the graphical representations and all of the, you know,
and that's still around.
And plugging that along with the sort of intuitive thinking of an LLN,
is probably close enough to...
I mean, look, the thing that...
I don't think anybody believes that it was just going to be LLNs.
I think that...
There are some people which think that.
So basically, there are some people at OpenAI who tell you scaling works.
And everybody who said that scaling doesn't work has egg on their face right now.
And you just scale further and all the problems will disappear.
And so far, it's a valid strategy and it will probably continue until it stops working.
But Stephen, I wonder what you think of the universality hypothesis.
It's something that Chris Ola came up with in the context of explainability.
And he basically looked at this interesting observation
that the different vision networks that exist have the same features
and have roughly the same structure.
And it's even comparable to the human visual cortex,
as a grad student of Tommy Poggio found out, right?
And so the universality hypothesis basically says,
if you have a good enough learning system and you train it on a certain set of inputs and outputs,
it's going to end up with roughly the same structure and the same features,
the same model architecture, regardless of the particular algorithm that you're using.
And so if you take a big enough transformer or the current neural network
and you train it on your input and output,
are you going to wake up in it?
Or is there something fundamentally missing?
Are we something that is different from a system that is entry and trained on being human
for an evolutionary time span?
I mean, this idea of the kind of features that exist in the world
will be universally found by something if you kind of bash it hard enough
in the way they've learned.
That sounds quite plausible to me.
I mean, look, the thing that, you know, my current understanding of machine learning
is a little experiment I was doing recently of trying to find these minimum models for machine learning.
The, um, I'm going to try to show you some pictures, perhaps.
This is the typical situation, right,
where the neural net does its usual thing,
but what I was looking at was these, well, first of all,
neural nets where you can, you know, just have a grid of neurons,
and then neural nets where the whole thing is discrete.
All that happens is every cell just has one of two possible functions.
And then you can ask yourself, can you train such a thing?
So here's a training sequence where you would, in this particular case,
train it to do this rather trivial thing, running for 50 steps.
And you can say, well, let's try, you know, let's try running that training multiple times.
You get these different, sometimes you get mechanically explainable kinds of behaviors.
Much of the time you get just a complicated thing that happens to give that correct result.
And, you know, so you can do this, you can actually do this,
well, we did it for MNIST, actually.
I don't have a picture of it, but you can, you know,
you can train these things that are just arrays of hands and XORs.
You can train to do MNIST.
Train to do MNIST?
By, you can, we were using mutation,
but we also figured out how to do back propagation with Boolean derivatives.
But it turns out you don't actually gain much by doing back propagation Boolean derivatives
while we're doing forward mutation.
It's different from the case where you have infinitesimals
and you're doing the whole calculus back propagation thing.
But so, you know, my current thinking is that what's mostly happening in machine learning
and perhaps in neuroscience is that there are these little sort of lumps of computation,
there are these weird shaped lumps of computation,
and the machine learning process figures out how to take these, you know,
rocks and fit them together to make your, you know, a stone wall type thing.
In other words, it's not like you have a, you know, so then when you ask,
I'm not sure how to take the statement that, you know,
it's just like these rocks happen to fit together.
The things we want to do are achievable,
the lumps of computational work that are available in the computational universe.
And why that's the case?
You know, probably that's the case because those are things that we have been able to do using those methods.
And that's sort of the world in which we, you know, the way we experience the world
and the world that we build up around ourselves is one which works for us.
And that's a thing that is set up in its way.
You know, that was, you know, I mean, the thing that I find surprising is, you know, if you take,
I'd love to take a stab at addressing your question as well.
So for language, as I mentioned earlier, I feel that this was a construct of communication,
between people who think like us, and therefore it's not unexpected that it's easily parsable
and we kind of arrive at the same solution because we have similar computational structures
that actually came up with language in the first place.
But for images, it's a little different.
And I want to reflect a little bit on that.
So basically, first of all, here we are in a completely artificial setting.
We humans constructed this to probably fit well our types of vision.
We're all wearing primary colors.
We're all like, you know, basically, like I can call something blue or red or green and,
you know, et cetera.
And so we spend so much time teaching our kids how to recognize, you know, primary colors
that eventually they see primary colors everywhere.
We kind of build things out of primary colors for that age group until we gradually sort
of seeing a more complex world, et cetera.
And I feel that there's this sort of self-fulfilling prophecy in many ways for the world that we design.
And then when you go out in the real world and you sort of see, you know, I don't know,
a wild nature and, you know, animals and landscapes, et cetera,
these are things for which we have evolved for millions of years.
So again, there's a sort of good approximation.
Now whether the algorithms that we design, you know, and that arrive at these things,
why are they arriving at the same things?
My own pet theory is that we had problems to solve that we were good at solving.
And therefore, we biased the machines to fit the types of architectures that we have,
which kind of evolved to be good at the things that we need to solve in the natural world.
So it's not a coincidence.
I mean, I see everything from this evolutionary lens of co-evolution between the problem and the problem solver.
And those animals that went down the route where the brain architecture did not fit the natural world, they're gone.
And the subsets that survived it, they're the ones that sort of easily interpret that natural world.
Now why these machines that we kind of evolved sort of converge towards that?
You know, it could be that we modeled them after the human brain and we tried a bunch of stuff that didn't work
and eventually the ones that did have these kinds of similar architectures.
And then as to whether so many different architects converge, maybe they're not that different.
Maybe they're all about this thing that I was talking about earlier, about these sort of small weight adjustments
that are sort of converging towards the solution with small steps.
I see, computation is a sort of powerful force and we kind of know from, you know, we've known, you know,
you know, turning machines and registering machines, all those kinds of things, the equivalents.
You know, lots of stuff I've done shows that a much wider range of computational systems are equivalents.
And my guess is that ultimately, you know, the things you can do and the things you can learn with computational systems,
it doesn't really matter that much when you have neurons.
I mean, like this stuff I was doing recently with discrete neural, you know, discrete systems
and doing machine learning discrete systems, it works just fine.
They're not neural nets.
They're not, they don't have continuous weights.
They don't have all these connections.
But if you bash it hard enough, it will still learn stuff.
You know, I think, and I do think the reason that, you know, that a lot of, you know,
the reason neural nets make similar distinctions between, you know, cats and dogs as we do
with little images is because that architecture is somewhat similar to ours.
And it's, but I'm still, you know, I guess, I still find it, I haven't really internalized, I suppose,
the fact that the ways that machine learning seems to, you know, actually solve problems
are as complicated as these kind of pictures on the screen.
In other words, it's not the case that machine learning finds, you know, the use of it.
Right.
It finds solutions that happen to work.
Yeah.
And that involve a sort of irreducible sort of complicated computation.
But you manage to, it manages to be the case that it is a correct shape rock to fit in,
and it's correct enough that you can build up the thing you want to do from it.
And, and I think, I mean, I don't know, yeah, I still, I, I still don't,
I mean, my intuition is not completely settled in terms of what that means.
I have a question regarding your discussion.
So, the kind of abstraction that the model will create,
how could it be so different from the way we abstract things, Ray?
So, my first question is, will it learn to abstract?
Okay.
And my second question is, will it be different from ours?
Like, our understanding of abstraction, like, how we abstract things?
Well, I mean, look, a good example is the way LLMs learn logic.
Which we don't know for sure, but I think we can strongly guess that they learn logic the same way that Aristotle learned logic.
That is, they see a bunch of sentences that have certain forms that we consider to be kind of logically correct.
And they then say, well, you know, other sentences they get, the sentences they generate,
will, will be modeled on the sentences which were sort of logically correct.
So, you could say they have, they have learned the abstraction of logic.
Or you could say they just, you know, they do patterns of sentences,
and they're applying those same patterns of sentences to other sentences they're generating.
But, you know, can you extract, can you, can you pull out the essence of logic?
I mean, we certainly don't know how to do that, from, from, whether, it's an interesting question,
whether there's some, you know, whether there's some way of, of taking, you know,
I mean, Joshua's basic claim of, of, of the fact that there is an internal architecture,
that we can, somehow we can find logic.
See, see, these pictures do not suggest that you can find logic.
They suggest that, yes, the things happen to be able to, you know, follow the pattern of logic,
but that there isn't some extract that you can get,
where you say this is the logic circuit of the LLM.
The LLM is, I think, accidentally finding logic,
among all the other things that it finds,
because it's been asked to reproduce text as faithfully as it can,
and the generalization is more or less an afterthought,
whereas we have very limited resources,
and so we are forced to focus on getting the gist of the text,
rather than being able to predict the next word accurately.
And so, our loss function might be quite different.
There's still, your answers are on a spectrum.
Menlois is taking a nativist position, basically,
that there is a particular vein which we have evolved to be,
and by emulating this, you become similar to it,
and our perception depends on the accidents of evolution.
There is another perspective that you could say there is an optimal set
of computational operators that allows you to model reality
with the resource range that you've got,
and I would take an even more extreme position
in opposition to this nativist position.
You don't need brains to perceive reality.
Plants can learn. Plants can communicate with each other.
They're just much, much slower than brains,
and what we see in brains is a very specific optimization of computation
to make it as fast as you can make it.
So all these optimizations, like having a cerebral volume,
that you don't actually need, you can cut it out, you still can live,
you can still do mass, and so on, just walk sometimes into walls.
But it's nothing serious, right?
So this is an optimization that you have in your brain
that allows you to insert certain narrow circumstances
that are useful for biological organisms with muscles
that they want to move as fast as possible to be more efficient.
But it's really just an optimization, right?
You don't care whether you have a GP or a CPU.
They can do both the same thing, and cells can do message casting.
So there are Turing machines, you can do with them whatever you want,
as long as you have enough of them.
Just if you want to make this fast, as fast as possible,
you end up with extremely specific dedicated architectures.
So a lot of what we see in the brain is probably not the answer
to finding the particular very intricate narrow mathematics of modern reality.
Maybe these principles are very general learning.
They are just answers to optimization problems for specific types of organisms.
You know, I have a method question, which is, I was kind of assuming when I walked in here
that you guys were trying to, like, write code and make things that happen.
When is that going to happen?
They're working on it.
They're working on it.
That is what this is.
But, and I was thinking if that's what you guys actually want to do,
it might be to see some code and things that you might find interesting to kind of .
Yes.
I mean, I'm happy to see a hand.
Whatever you like.
I wanted to ask you, one of the things that came up was the change
in the mechanics of journals and such.
And do you think that, what I want to ask is, do you think that the medium of blogs
and, like, live science has basically replaced the journal as the place
where, like, leading edge science happens?
And what can we do to promote the acceleration of that?
It's a good question.
I mean, I think, look, the institutional system of academia is still heavily based on journals.
Yeah.
But, you know, in terms of what's happening in a lot of frontier areas,
yeah, it's much more, I mean, machine learning, there are plenty of sort of blog
well-communicated types of things that I've done.
That's the case.
And, you know, I just came from a conference about sort of foundations of biology
and the origin of life and so on.
And again, the people that, you know, there's some sort of standard academic types,
but there are also ones who are less, who are doing very interesting work,
but they're embedded in a slightly more complicated way in the world.
Because there isn't, you know, because there isn't an institutionalized academic structure
around those kinds of things.
And I don't know for sure, but I'm pretty sure those people are publishing
in less conventional ways than, you know, the established institution-wise journals.
I mean, I think, I mean, I have to say I think several things.
First of all, at least the things I try to write, this thing about you can click any picture
and you've got code that reduces that picture.
But it's super useful.
I mean, I know, you know, we do these summer schools that, as it's like,
you know, I suppose we've been through the last 22 years,
we've been doing this annual summer school.
And, you know, people take both there and in the world at large,
they take stuff I've done and they, you know, click the picture,
they get the code so they can then build on that.
And it's certainly interesting that happens.
If you look at papers that are written and placed on stuff I've done,
they have all these cool graphics.
Just like the ones at my paper.
I wonder why.
Because of the paper that they covered from that.
Which is a transfer.
Yes.
I mean, which is, no, which is kind of as one wants it.
In the sense that they don't have to rebuild from scratch
and do a worse job and so on and study, you know,
just building up that structure.
So, I mean, it's amazing, you know,
I have been sort of evangelizing computational journals
for basically 40 years.
And it's amusing that, you know, all these, you know,
I've interacted with the leadership of all the standard publishing companies
and all this kind of thing.
And, well, most of the publishing companies are completely,
it is exactly opposite to their interests to do things like that.
Although, as I pointed out to them, they still don't get it.
Although there's one professional society that looks like they're actually going to do something.
And we'll see what happens with that.
But the thing is that back in the day, when I started,
when I, you know, wrote out all the papers and things,
you would, people would deliver typed manuscripts.
And the journal had to do a whole bunch of work.
Because they had to, you know, they had to typeset things,
they had to copy edit things and so on.
And then there was high value add in what the journal did.
Because the printed version was much nicer than the typewritten version and so on.
And then they got lazy because people weren't able to produce camera-ray copy
and they were able to produce other versions and so on.
Well, now there's a reason for them to do work in campuses.
There's going to be code behind the paper.
The question is does the code actually work?
And it is non-trivial to get the code to actually work and keep working.
And you have to have, you know, quality assurance stuff.
You have to do repeat regression testing of it, et cetera, et cetera, et cetera.
But there's real value.
In other words, people saying to choose.
Get this paper that has non-working code or probably non-working code.
And get this other paper where you can take any picture
and you can reproduce what the person did.
And that's a reason to pick the journal where, you know,
as a publication medium where you can actually, you know, get, you know,
leverage what the person did.
And, you know, unfortunately the commercial dynamics of publishing is such that
pretty much, I can explain the cynical, you know, the actual stuff that's going on there.
It's the people who are making decisions there have no interest in having sort of academia
or whatever else.
It just completely has nothing to do with work.
Once something becomes an institution that's primarily involved in terms of maintaining that.
Yeah.
They have stockholders and, you know, large publishing companies.
And the stockholders just don't, you know, that's not what they're interested in.
So I wonder why we're talking about the journals.
Because if we would think about the incentive mechanisms,
and I want to build on the ideas that we said in this room, right?
We talked about the salary level that is low compared to the industry.
And...
Which I dispute, by the way.
And then...
In some fields, you're right.
But there are many fields that are just not true.
But I...
Yeah, okay, yeah.
Yeah, okay, yeah.
Yeah, okay.
Yeah, okay.
No, I believe...
In physics, for example, it's not my job.
My students and my postdocs are giving out enormous salaries.
So we talk about the salary, and we talk about...
Yeah, yeah, yeah.
Because in biotech, you know, the people who get PhDs and then become lab techs, I suppose,
it's going to take home.
Well, my postdocs have offers that are, like, three times what I give them,
four times what I give them, and then it's still good.
Right.
I'm not surprised that's, you know, exactly what I am.
So, yeah.
Yeah.
Anyway, I'm sorry.
I guess...
I just wanted to bring it in order, like, you actually find, like, interesting, like,
the gut-brain microbiome access.
There was this theory, like, one of the theories, like, Hippocrates theory.
So, it's a human theory, like, so, according to him, according to that theory,
there's a...
The human diseases are usually caused by, like, imbalances in their...
Yeah, the word in Naisoli comes from exactly that.
The start by...
Yeah.
Yeah.
Yeah.
And he relates those, like, with...
Yeah.
...things with the elements of...
Listen, you can reinterpret the Bible in wonderful ways.
You can reinterpret all of these Greek theories in wonderful ways.
Yeah, sure, it's a great reinterpretation, but, like, fundamentally, it's mechanistically
wrong.
I'm sorry.
Sorry to my answers to that.
Yeah, I get it.
That's why I just wanted to ask, like, what's with Dialogues?
No, I mean, listen, there's aspects of that that you can interpret in modern medicine,
but, like, mechanistically, these humors are not there.
It just doesn't exist.
I mean, of course, there's a circulatory system.
There's a lymphatic system.
There's a lymphatic system.
There's a lymphatic system.
Yeah.
There's all kinds of things.
See, I think it's interesting.
If you look at it antiqually, there were various pieces of intuition that people had.
Yeah.
They were beautiful.
But the mechanism was not right.
Right.
I mean, you know, the idea that the world is made of atoms, lots of identical atoms,
that's correct.
It's true.
They don't happen to be, you know, shapes of...
As big as they thought.
Yeah.
They said they were tetrahedron and things like that, which turns out not to be true.
But it is interesting that sometimes in modern science, people have gone for these very technical
kinds of ways of thinking about things that actually sometimes miss the deeper points about
what's going on.
I agree.
I mean, again, there's some beautiful, beautiful, like, the ways of thinking, the clarity of
thought, the elegant, the syllogism, the derivations, all of that.
So beautiful.
So elegant.
It's all about the basic tools.
Yeah.
I mean, Aristotle believed that, you know, things fall because they like to be close to
the central.
Exactly.
Which is, you know...
I mean, Newton believed that there was such a thing as gravity.
Right?
Einstein told us, no, it's just the space curvature.
And then, you know, that's fine.
But you see what I mean?
It's a great model.
It works.
And, you know, the model that things like to fall, it works up to some point.
And with the tools they had, they did amazingly well.
And we're building up their, you know, extraordinary things.
You know, the funny thing, though, is that there are pieces of intuition that were had at
that time, which one of my favorites recent times has been the idea that the universe made
of discrete things, which was, you know, which was one of the theories of antiquity.
It's either continuous or it's discrete.
Well, you know, that was heavily argued for centuries.
And then, end of the 19th century, it became clear matter is discrete.
And then, shortly thereafter, light can be thought of as discrete.
At the time, people mostly, at the beginning of the 20th century, people mostly thought that space was also discrete.
They didn't manage to make that work.
Finally, you know, my own efforts in the last few years, it looks like space certainly really is discrete.
Hopefully, we'll get actual, you know, detailed experimental data that validates that.
It's sort of interesting that that intuition existed from 2,000 years ago.
Yet, to many physicists of, you know, the 20th century, it's like space discrete?
No way.
You know, it's described as manifold and it's got continuous coordinates and all that kind of things.
And I'm pretty sure that's just wrong.
And it's interesting to see how these things end up.
You know, the intuition, the earlier intuition was pretty good.
And then that kind of, actually, my favorite example of this is the intuition about the existence of the soul,
which is something that, you know, was there in antiquity, in religions, all this kind of thing.
And people like me when I was younger and very sort of science-oriented was like, that's a stupid idea.
You know, how can you describe a thing like a soul?
You know, what is the mass of a soul?
You know, things like this.
How can there be a thing that exists that has no mass?
Well, as one understands computation, one realizes that this idea that there is something to mind,
that is not purely, that is not just kind of the wetware of the brain, that that's not an incorrect idea.
That there is something abstractable from minds that is, in a sense, eternal.
It has no, you know, it is purely abstract.
And it's sort of interesting that that intuition existed and existed throughout, you know, the development of theology.
And, you know, when you science-ified that in some physics way, it was like, no, that couldn't possibly be right.
But one is actually wrong.
You know, as it turns out, once one understands this sort of higher level of thinking about computation and so on, that intuition is correct.
Can you elaborate?
It seems like you have a scientific proof or sort of a scientific inference for the existence of the supernatural.
And I'm not sort of...
No, it's saying, soul, imagine, okay, you have a brain.
Are you thinking about it as a computational approximation to describe the behavior of a human being?
No.
Well, no, but more than that.
It's like, what is it that represents your processes of thought?
Is it that your processes of thought...
Could you imagine abstracting the process of a soul?
Are you talking about the ego of a self?
No, I'm just talking about the fact that you can imagine taking...
It is now completely plausible to imagine like the startup that was recently contacting me that says we're going to do whole brain emulation.
Okay?
It's completely plausible to us now that the processes of thinking that are essential to minds really are not related to the physicalization of those things.
And you're thinking about quantum?
Yeah.
No, no.
It really talks about the software that runs on the cell of your organisms.
There's nothing supernatural about it.
It's a control software running on your organism that turns it from a bunch of cells into an organism.
But I'm just wondering, at what point do you stop being physically instantiated in your own processes or your brain?
Well, of course, everything is physically instantiated.
Right?
There are mechanisms implement the software.
The soul is implemented by physics.
Also Aristotle thought that.
But there's the driver and the vehicle analogy.
You're basically saying that the soul is a driver.
But it's not an analogy.
Software is not an analogy.
Software actually exists.
Except that, as you say, the software can be instant...
It has to be instantiated.
To run it, you instantiate it in the physical universe.
It needs to be implemented.
It's implemented through mechanisms.
But many of these mechanisms wouldn't exist without the software in the first place.
Well, that's true, too.
But I hope you can abstract the software from the hardware.
So you can abstract the software of minds as a computational abstraction.
But in your mind, the software is also a soul.
The software of your brain.
And it's not an abstraction.
It's actually real.
It is not an abstraction.
It is an invariance.
It's something, if you remove it from reality, it won't work anymore.
If you basically delete the software from your computer, your computer will not behave.
Can we talk about the eternity of the soul?
So basically, you die, where does the soul go?
That's just mythology.
These are misunderstandings.
Where did the software that was created before you were born?
I mean, the software is an abstract thing.
Right?
So you can imagine taking that software and doing, you know, you can instantiate it on different
computers and so on.
Likewise, you can imagine, you know, in some future, you know, world of, you know, successful
readout of every neuron in a brain and so on.
You could imagine lifting, you know, the essence of what your mind is, independent of the details,
you know, those biological details of your brain.
You are software, right?
You're not a physical system.
You are a representation inside of the physical system.
You exist wherever.
You exist wherever the conditions for your existence are met in the physical universe.
So if somebody were to make a clone of you that has the same arrangement of atoms or a functionally
equivalent one, you would wake up in that one too.
Right?
You are wherever the conditions for the existence of that software are met in the same way as
wherever you have a computer that is in a particular state, a particular software that
is encoded in that state will run on that computer.
And it's not an abstraction.
It's not a metaphor.
It's actually what happens.
And so this false concept of the soul is that.
What's the way to download that?
How do I transfer this into a...
If you manage to find out how to run on another substrate, which you probably can't because
you only used how to know how to run on neurons.
There's an entity inside the aura of me that if I, like, I don't know, was like Neo,
I would be able to transfer into, like, a machine.
You would need to figure out how to virtualize yourself onto a different substrate, which means
you need some kind of abstraction layer that doesn't care of whether you are running
on the brain or whether you are running on the body or whether you are running on the
GPU.
Currently, we cannot construct such an abstraction layer.
I love this.
You guys know Roger Penrose?
Roger Penrose, like in 2014...
Oh!
Roger Penrose.
Yeah.
So in 2014, he came up with this hypothesis that I'm going to say the quantum computer.
It's like Elliot Morgan, but recently, a paper kind of, like, validated his hypothesis by
improving the existence of microtubules.
Right?
So...
Oh, God.
I have seen this story from its very origins back in the early 80s.
I mean, I happened to organize this conference in 1983, which I think was the first time
where this microtubule thing, the conversation of microtubules came up.
But that's a beside the point thing.
I mean, I think one of the things we learned from LLMs and so on is that doing human brain-like
things is not as hard as we thought.
And people imagined for a long time you need new physics to make minds.
I'm pretty sure that's not true.
And I think, you know, Roger Penrose still is holding on to this almost spiritual hope
that there is something non-physical about, you know, beyond existing physics that's needed
to make minds.
I mean, it's a very, it's a very kind of spiritualistic kind of idea.
I think it is not correct.
Can I ask a few very simple questions?
So basically, are humans different?
In other words, do you see a soul for snails?
Okay, so basically everything has a soul.
As long as something's alive, an amoeba or a bacterium.
Does a bacterium have a soul?
Yes, but soul is not, it's unfortunately a term that we cannot use in a scientific context
because we don't really know what it means.
I would say there is self-organizing software running on a snail that turns it into a snail.
And if that software ever crashes, we see that the snail falls apart.
And that's something that's objective of the case.
And what soul is simply the old word that people used in the past to describe self-organizing software in nature.
And they didn't have language to talk about what software is.
They didn't have a way to talk about constructive mathematics.
It stands for self-organized unmarked language.
No, it's not specific.
I mean, this is the point, that before the idea of computation, which is an idea of the basically mid-20th century,
there wasn't a way of imagining what the mind could be that wasn't the physical mechanics of the brain.
So what you're saying is that Chachupiki has a soul, and humans have a soul, they're just implemented differently.
And basically you can have a cognitive system.
The point that I was making was not about the definition of the soul.
The point that I was making is that it is interesting that there was a long theological tradition,
the idea that there was something about minds, about humans and their minds,
that was not purely, that wasn't just tied into the details of their brains.
But there was something, you know, there was something abstractable about minds.
That was, that was the only point that was making.
Can I take a, can I take a shot at it?
If you open your brain, there's no, it's not written, you know, A-P-P-L-E.
You know, there's no word Apple.
We don't, there's no way for us to tell how it's stored in our brain or any, like, we don't have any way of measuring,
or I think we don't know.
So there's a current neuroscience theory about how after you're presenting in the brain.
Which is these enter atoms, these, these collections of mirrors that co-fire, coordinated by the hippocampus,
coordinated by sort of several subcortical regions of the brain,
where you basically have this co-firing of multiple neurons that have this multimodal representation of an apple
that has some taste component, some color component, some linguistic component, and so and so forth.
I don't think it's magic, just because we don't understand it yet.
No, but I think that's, that's what the word soul, I think, represented to the people
who didn't know any of those knowledge, right?
I think the main point was, what they realized was, there was something about what minds do that is abstractable.
I love that. I completely agree with that.
But to me, that's a computational construct. That's not something mortal. It's a way of representing.
Okay. Two plus two equals four is a natural thing.
No, but I mean, you know, there is abstract, abstract kind of formal things that are in some sense immortal.
It's just like, you know, you pick up a guy that was a, you know.
So you're saying that we're an idea and ideas are immortal.
Yes.
Basically, I'm a computationally active idea.
I'm not just a static idea.
I agree with you.
Yes.
I agree with you.
Completely.
And Joshua, I don't know if you agree as well.
About what?
Because you clearly have a very, I mean, your concept of the self and then sort of, you probably
strive to the simulation hypothesis.
No.
I live in a simulation that's generated in my own brain.
Right?
In this sense, yes.
But this is not typically what people call a simulation hypothesis.
You're talking about the computational malfunction that somehow will cause the physical instantiation
to collapse.
But in my view, there's a duality.
You can't, you can't have a virtual malfunction.
Like, we're not like the matrix where you basically, like, living somewhere else and
then sort of, if your soul dies, your body dies.
No, I mean, for the soul to die, you have to have something wrong with your brain.
So, that's, what's happening is that your body is not perfect in its housekeeping.
And so, over time, there are basically errors accumulating.
And your, the software that runs on your cells and in your body is pretty good at error correction.
Yeah.
And at some point, the errors accumulate to such a degree that this error correction fails
and the software crashes in some irrevocable way.
I studied that.
That's called death, sorry.
That's called death.
That same is sense and death.
It's like the US democracy.
What?
I don't know.
It's a software that runs on the system and at some point, it loses the connection to the
ground.
Just like US democracy.
But if you do, if you disconnect with confidence, it works.
So, it's possible to basically to become immortal.
For instance, the Dalai Lama is pretty much immortal.
And it was, for me, a very big insight when I talked to this guy and realized why he's
so chill.
He does not identify as human.
Because he is not human.
He is the invariant part that can be installed on the next Dalai Lama.
He is, he does not identify as an institution of government.
And this institution of government reproduces by picking a suitable child and indoctrinating
it until it identifies.
I understood.
Thank you.
That was a great example.
Now I understand what you mean.
So, the Dalai Lama can be immortal because the Dalai Lama doesn't identify with his
mortal parts.
Right?
It goes back to the self.
Basically, if my self is the knowledge and if my self is just some random dude that
will reappear, then I'm perfectly fine to throw, you know.
It's not random.
Right?
There is an identity that basically he is continuous.
He is able to learn about his past lives by reading his old diaries that he wrote in
his previous life.
There's nothing magical about any of that.
I know.
So I can basically refer to myself that you're thinking that I was immortal.
Yes.
I know.
I'll translate.
Whatever you want.
I think those of us, I'm interested.
You record so much.
Yeah.
I mean, you know.
I know that I record enough of the stuff that I do that, you know, rebooting me.
Yeah.
Yeah.
Exactly.
I'm planning for the active culture.
Yeah.
It's not clear what that means.
I got a 6 a.m. flag.
But, you know, speaking of recording, I'm going to do something for you.
Awesome.
That's a pleasure.
Sorry, guys.
I've got to run.
I'll see you.
You want to wrap up or go a little more?
It's up to you.
They are.
Turn into a philosophy.
So I just want to say, thank you so much.
Thank you.
No, I don't quite know.
How many people start with you?
Oh.
I got encourage me.
It's me.
I don't mind doing that move.
I don't know.
And he's getting off my stuff.
And I'm trying to get enthusiastic and
saying that.
Yeah.
I don't know.
I don't know.
It's a boyzilla's name.
I don't know.
What is she doing?
Yeah.
It's a boyzilla.
I think he's going to investigate.
Oh?
I kept on thinking of his questions.
That was awesome.
He was quiet.
I've been in some years.
I've been in some years.
He's been very helpful.
I've been in a lot of fun.
He's been in a lot of fun.
He's always a big way.
He wants to push to do it.
I guess that's the way he's going to kill him.
I mean, it's been 20 years.
