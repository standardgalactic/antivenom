Unifying principles underline, no pressure, evolution, computation, cognition, and AI.
Evolution, computation, cognition, and AI.
And how we can leverage them to design the next generation of thinking systems.
So what are these principles that underlie evolution, computation, cognition, and AI?
So I'm a genomicist.
I've been working in genomics for many decades now.
And I've been studying gene regulation.
I've been studying sort of the basic principles through which genomes function.
How genomes encode regulation.
Because the analogy of the genome as a hard drive breaks down in the sense that the genome is a self-regulating hard drive.
All of the commands, all of the operating system, all of these things are sort of encoded within the genome itself.
And you could ask, how is the genome able to navigate the landscape of solutions to these complex problems?
And what's really phenomenal here is that we encode it within the genome, which needs to replicate at every generation from a single cell,
to then recreate the entire body plan of humans, within that is in fact encoded the entire circuitry for our brains.
So there are some fundamental similarities in evolved systems, such as the brain and such as the genome, that have a lot of similarities with what we have as AI right now.
And it's perhaps not a coincidence that humans make the same types of mistakes that AI seems to make.
For example, if you ask, I don't know, a 10-year-old kid to draw a person, they'll probably draw the wrong number of fingers, or the fingers will look really awkward.
And guess what? AI has the same type of trouble as well.
If you sort of show a picture of a dog or a muffin, I don't know if you've seen those, they often get confused by humans and by AI alike.
So it's kind of interesting that the same type of architecture that we evolved to deal with our physical world and that we somehow, through this repeated improvement that evolutionary processes are all about,
has kind of converged to the same type of inferences that AI has.
So I think that, you know, that poses a lot of questions.
And sort of, as I think about, again, evolution, computation, cognition, and AI.
So we talk about evolution.
I didn't talk so much about the genome, about how it approximates the functions.
So basically, you know, there's an evolutionary landscape upon which you're, you know, exploring the world.
And then there's the actual encoding of what you've learned about that.
And at the basis of that is tinkering.
At the basis of that is not the circuits that we like to draw when we engineer systems.
That have, like, one big wire and a bunch of if statements and a bunch of asserts and stuff like that.
Instead, what we have is these tiny little weights that control the expression of genes from hundreds of regulators binding together and thousands of signals converging.
And it's not unlike the weights of our own artificial neural networks.
And if you look at the way that the brain works, again, you know, backpropagation is not exactly the brain, but it was modeled after what we thought the brain was doing.
And a lot of the operation that we have now is not exactly what the brain does, but, you know, it is, again, modeled after the brain.
And you could even say that the transformer architecture, this whole attention mechanism, the ability to reinforce edges that appear to be associating with, you know, similar objects or, you know, relevant objects, etc.,
is not unlike the same type of synaptic pruning that we have within our own neuronal networks and this reinforcement.
So I see a lot of common principles there.
I see the principles of, instead of encoding something with, like, you know, if-then statements,
encode something much more organically, much more dynamically, much more scattered in these, in their weights.
Another commonality is that of abstraction.
So as you start thinking about computations, you start thinking about computer programs,
or you start thinking about sort of what is the foundational principle behind programming and software engineering,
that principle is the principle of abstraction.
What is abstraction?
Abstraction basically means that I have multiple layers of representations.
And at the base layer, I have, you know, very simple commands.
And I can encapsulate those in building blocks that I will then use for the next layer, the next layer, and the next layer.
So there's a lot of power in doing that.
And that's not unlike what even our genome does in being able to sort of just black box some operations
that are then building on previous machines.
For example, if you look at a human as a system, we need to fight off an infection.
There are specialized pathways and modules inside every one of our cells.
The cells themselves are, you know, units of computation, units of function.
They function together as systems.
Again, my kids are now learning about the immune system and, you know, the respiratory system
and the bone system and all of these things.
Basically, these are intertwined systems within your body.
And if you have mutations in the DNA, you don't see any humans being born, like, inside out.
Or, like, I don't know, with 20 limbs or something like that.
And even just, you know, being born with multiple limbs.
I mean, these are limbs.
These are not just, like, weird cross-cutting, like, fingers sticking out from anywhere.
So there's instructions in even the body plant formation of development that are convergent,
that are sort of encapsulated in the way that individual pixels will not be altered in an image,
but instead it will be objects.
So if you start messing with the internal representations of a neural network,
of a convolutional neural network trying to understand an image and represent an image,
that image is going to have weird, complete shapes coming about rather than individual pixels messed up.
And a lot of that has to do with compression.
The fact that as we see objects, we're immediately compressing them down.
And every now and then you'll see an image that's, like, damaged.
But you're able to restore the damage and sort of observe the stuff behind,
which is part of that lower-dimensional representation of the world.
And all of that, again, is based on the same principles of abstraction,
of hierarchical representations, of modularity,
which underlie, you know, genomes, evolution,
and all of these other things.
Computation, cognition, and AI.
So, again, it's not just that our brain and AI appear to be similar.
It's that there seems to be something much more fundamental about representing the physical world
and representing concepts and images and even thoughts
that sort of is easily approximated using this type of operation.
And if you take one thing out of this presentation,
it's going to be centered around that concept of representation,
of representation learning,
of being able to not think of modern-day AI
as simply a continuation of the neural networks that we had in the 60s,
but to think about modern-day AI as,
yes, we have a fully connected network in the end, if you wish,
to approximate some functions,
but it's all about that first part of representation learning.
Okay?
And I want to perhaps jump right into this slide here
where I'm basically talking about these three vectors.
So, on one hand, you have your data, and that's X.
Then you have your labels, and that's Y.
And a lot of the revolution in deep learning in modern-day AI
happened by this picture.
And this picture has two components.
On one hand, you have the boring, fully connected neural network of the 60s.
And that I agree with.
It's still there.
Perhaps no longer, but in that picture, it's still there.
But what's much more exciting about this picture
is this Z vector, this Z dimensionality.
What is that?
This is something where the system is extracting representations from your image.
It is not looking at individual pixels.
It builds up from the pixels with filters,
which we call convolutions,
checking of, oh, is there a line here?
And you scan that image for a line this way,
and a line that way, and a line that way, and a line that way.
And then you have a first layer that doesn't look at the pixels anymore,
but it looks like the first layer of abstraction.
And then on that layer of abstraction, you now see,
okay, do I see two of these nearby?
Oh, maybe that's a corner.
And on that corner, you now see shapes and circles and eyes.
And then you scan for eyes.
These are filters on maybe the 10th layer.
And this is not unlike what happens in your visual cortex.
So basically, your primary visual cortex is receiving
an inverted image from your retina of the space around it
that is basically pixels.
But immediately after,
you have all of these convolutions running operations
that effectively extract features and representations from this.
And some of those representations we have evolved for.
In other words, there's a bunch of lines, you know, in the world.
Maybe we can hard code the concept of a line in our genome
so that at the next generation, we can already recognize lines
and we don't have to wait until our neural network learns them.
But other concepts, such as language, for example,
are not hard-coded.
And instead, they are learned at every generation.
At every generation, you basically see your kids, you know,
sort of staring out, listening, you know,
they're sort of figuring out, they're training their edge detectors.
They're training their, you know, basic building blocks
upon which then all of their learning will happen.
It will not happen at the level of pixels.
It will happen at the level of representation.
Is everybody with me here?
Awesome.
So that's what cognitive cartography is about.
What we basically said is,
hey, I don't want to look at the X
and I don't want to look at the Y.
Why?
Because the Y is basically categories
that we humans know about already.
This is stuff that's like training data.
And the power of machine learning today
is that frankly, we don't need human annotators anymore.
Why?
Because I can basically go into that image
and hide a box
and say, what is in that box?
And if the neural network is kind of,
you know, moderately, not completely dumb,
it will basically say,
oh, just follow the gradient of colors.
And that's fine.
With enough parameters,
we will learn that images typically follow
the gradient of color
and we'll be able to fill in the missing box.
And eventually, if you start giving it
more and more and more and more parameters,
it will recognize that to get better
at filling in that box,
it better build an internal representation
of the entire physical world.
It better understand that boats
have, you know, I don't know,
sails on one side
and water on the other side,
that cars have like a steering wheel on one side
and, you know, a fender on the other side
and so on and so forth.
So, this is not unlike the training of JGPT.
So, when you train these large language models,
what are they doing?
They're basically doing next word prediction.
And many people dismiss JGPT as,
ah, it's just next word prediction.
Who cares?
It's not intelligent.
I'm like, yeah, yeah, yeah,
but that's how they train you.
That's not what it does.
What it does is the emergent behavior,
once it has figured out
that the best way
to get at next word prediction
is to understand
syntax and grammar
and language
and semantics
and prose
and style
and, you know,
the language of speech
and, you know,
every human language
and then concepts
like liberty
and, you know, justice
and stuff like that.
And that's how you edge out
the extra 2, 3, you know,
percent of performance.
So, with a sufficiently large number of parameters
and this self-supervised learning architecture,
you can basically get at representations of the world
that are immensely complex.
And what we have in our everyday life
is cognitive systems,
every one of you,
was trained
using a different subset
of the English language.
Many of us learned it
as our second language,
some of us as our third language.
And we have relearned
how to speak English
to the point
that we understand each other
and that we understand
the difference
between the concept of,
I don't know,
liberty
and freedom.
And if you were to use them
in a sentence,
you would not use them interchangeably
because you've understood
from the context
in which these words appear
there's semantic similarity
and there's subtle differences.
My kids don't understand this,
but as they grow older,
they build sophistication,
not only because their exposure
to language is longer,
but because their brain,
their hardware,
actually continues to grow,
continues to get bigger,
the connections themselves,
the underlying hardware
continues to evolve.
So, concepts
that when they were babies
were blended into one
are now starting to come apart.
And this is, again,
in many ways,
the way that
the last generations
of training
were basically partitioned
in the language.
And even though
every one of us
has been trained
on a tiny, tiny, tiny subset
in a different subset
of the English language,
we're all able to communicate.
What does that say
about the English language?
It's actually kind of simple.
Understanding how to speak
human language
is actually very simple.
there's a lot
of repeating patterns,
and there's a very
small number of patterns.
The fact that machines
speak English
is not surprising.
But I've built my career
around a language
that we humans
don't speak.
That is not a language
that we agreed upon.
Again, English
is evolved as a language.
It has, of course,
roots in many
Indo-European languages,
and it has a lot
of simplifications.
It is a very derived language.
I grew up speaking Greek,
which is a much more
ancient language.
So basically,
the grammar is much more
complex.
There's much more rigidity.
There's many fewer
root options.
So for example,
the word like
hepatic liver
is completely nonsensical.
It's like two roots,
one from Latin,
one from Greek
that mean the same thing
like the liver liver.
Like, really?
So the fact that English
has all of these
different roots
to draw from,
and all of these
grammatical structures,
and the pronunciation
makes no sense
if you only speak English.
But if you speak two or three
or five European languages,
then pronunciation
on English makes
complete sense
based on where
each word came from.
That's it.
It's so simple.
And sort of tracing
that pattern of thought,
pattern of evolution
of language itself,
again,
as an agreed-upon
communication medium,
has a lot to do
with how our brains work.
We didn't arrive
upon English
because, I don't know,
it arrived on tablets
from Moses.
That's a different language.
But we learned English
because, you know,
that's how people
started communicating.
And eventually
they would make
the same mistakes,
things didn't quite sound right,
they would flip some syllables,
they would flip some sound,
and then things would kind of
roll off the tongue
a little more easily,
and so on and so forth.
So that's an evolved language.
But for TMA,
that's a language
that our brain
is not very good at speaking.
If you look at protein structure,
that's a language
that, again,
our brain is terrible at speaking.
If you look at chemical structures,
if you look at gene functions,
if you look at cellular
transcriptional space.
So that's what my day job is.
My day job is trying
to understand
a language
that none of us speaks.
And to do this,
we've basically developed
a lot of this
cognitive cartography,
looking at this latent
z space,
this latent vector representation
of constructs
that make no sense
initially.
So in the same way
that you can go from pixels
in an image
and build your way up
towards more abstract concepts
of shapes
that you're scanning
in the image,
you can go in a genome
and basically scan
not just individual letters,
but now words
and patterns
and motifs.
And you can build
from those grammars.
You can go into
chemical space
and start understanding
no longer
a linear grammar
like language
or one-dimensional
linear grammar
and of course
the attention
edges between them
of the different words.
But now
you can understand
beyond this linear
and 2D version
that we have for images,
you can start
understanding graphs
and the representations
that come from graphs
build on every atom
in the context
of all of the other atoms.
And that context
is what gives
the atoms meaning
in the same way
that the context
of the other pixels
give the pixels meaning
and they become shapes.
So you can now
start understanding
the building blocks
of chemistry
by understanding
how every atom
is influenced
by every other atom
in a graph neural network.
Everybody with me
here is a problem.
And you can do
the same thing
with protein structure.
You can basically
now start building
the backbone chain
of the protein
and then the meaning
of the function
of the protein
comes from
the combination
of amino acids
that are sort of
falling together
and that together
we have a white label.
We know which chemicals,
I don't know,
kill E. coli
and which chemicals
fight cancer
and that's the white label.
But now being able
to now trace that
into
which part
of the representation
of chemicals
is responsible for that.
Which part
of the protein
representation
is responsible
for the function
of the protein
and for different
functions of the protein.
That's a much, much harder,
much more interesting problem.
And again,
we've done that
for patient records,
we've done that
for knowledge graphs,
we've done that
for scientific papers
and single cell data,
et cetera.
And we've also
in the last few years
been doing that
for the combination
of each of these
data types
and human language.
And that's where
it gets a lot of fun
because you can now
start training
in a multilingual system
that is able
to understand
patient records
and English,
protein structure
and English,
knowledge graphs
and structure
and English.
so you can start
conversing
with that system.
Everybody excited
about this?
Yeah.
Good.
So we have built
such a system.
We basically built
a system that is
using geometric
deep learning
to try to get rid
with some of those
things.
So we're able
to now start
speaking to that
system which
understands human
disease,
it understands
protein structure,
it understands
protein function,
it also speaks
English.
So we can start
designing next
generation drugs
against specific
proteins,
against specific
pathways,
against specific
entities.
And what's really
exciting here is
that we can
also use
this technology
to peek
inside the
machine,
to look inside
the brain of
the machine
at those
representations.
And that's
what allows us
to now start
representing
these proteins,
representing
these structures,
representing
these knowledge
drugs.
So before I
sort of discuss
how you can
start with
a chemical
and with
every atom
understand how
its neighbors
are influencing
it.
And at the
first layer of
abstraction,
you're only
looking at
your own
representation
and the
representation
of your
neighbors.
But then at
the next layer,
your neighbor's
representation is
influenced by
their neighbors.
And therefore,
you're learning a
much more holistic
view of the
chemical.
And at
every iteration,
you're improving,
you're revising
your own
representation for
every atom.
And then all
of the nearby
representations get
revised by you
just as their
neighbors.
You can do the
same thing with
a knowledge graph
where every node
is a gene.
And it is
connected to
many other
types of nodes.
So we now
have 20,000 nodes
for genes.
And we have
another 17,000
nodes for
diseases.
And every
disease node
will be updated
updated based
on which genes
it interacts
with.
And every
gene will be
updated based
on which
pathways it
interacts with.
And therefore,
a lipid pathway
will influence
a bunch of
genes which
will then
influence
Alzheimer's
disease if
they're connected
that way.
And Alzheimer's
will become
perhaps more of
a lipid
metabolism or
transport disease.
And, I don't
know, cardiovascularly
will become more
of a, you know,
lipid storage or
you name it.
So the beauty
of all of that
is that you're
letting the
knowledge graph
speak for itself
and influence
each other and
then learn much
more complex,
much more
multi-modal
representations.
Is everybody
with me here?
Awesome.
So we can now
take this very
high-dimensional
latent space
of what was
learned and
start projecting
it down using
dimensionality reduction
techniques like
this new map
to basically map
it into two
dimensions and
actually for us
humans visualize
it.
And we can
do that for
the combined
space where
here you have
drugs and
genes and
diseases and
effects and
phenotypes and
anatomy and
so and so
forth.
And you can
see that the
different data
types have
actually been
pushed apart
from each
other because
they are
fundamentally
different data
types.
And then we
can go into
one of those,
for example,
genes and
expand it more
systematically.
We can
basically see
how that gene
representation
tracks with
whatever the
genes are
actually doing
which was
known completely
from scratch
based on that
knowledge graph.
Is everybody
with me here?
And we can
go a step
further and
now start
using this
multimodal
learning approach
combining
language and
structure and
knowledge graphs
and we can
basically build
these joint
representations
across these
different data
modalities.
For example,
we can combine
together gene
expression patterns
and phenotypes
from the
individuals that
carry those
cells and
then understand
how asthma
or Alzheimer's
or genetic
predispositions
influence cells
in their
transcription.
You can do the
same thing by
having now the
dots be
individuals,
patients.
And you can
understand what
are the axes of
variation.
In this particular
case we're looking
at principal
component analysis
which is much
more interpretable
and these axes of
variation turn out
to be on one
side pathology
and on the other
side cognition
in the case of
Alzheimer's disease.
And we can now
start relating that
with what are the
genes that vary
along those
dimensions.
And along the
cognition dimension
vary a bunch of
genes expressed in
excitatory mirrors.
But along the
pathology dimension
instead you have
microglia and
oligodendrocytes.
So you can kind of
start uncoupling
the drivers of
this variation.
And you can just
go to town with
this.
You can basically
now say,
okay, let's take
multimodal
representations of
anything.
So let's start
with one modality
right now and
let's think of
human language.
And you can take
for example every
poem or every
love letter or
every, I don't
know, declaration
of independence
and map them
into a cognitive
landscape.
Okay?
And this is what
my, you know,
first slide was
going to be about,
about this
cognitive
cartography.
Being able to
map the latent
representation of
proteins, genes,
poems, you
know, documents,
scientific papers,
and so on and
so forth.
Then understanding
that landscape,
being able to
map it the same
way that we map
physical objects.
and if you give
a paper to a
student, that
student, if
sufficiently dedicated,
will be able to
understand that
paper.
But if you give
that same paper to
their professor, the
professor will just
read the abstract and
immediately know what
the paper is about.
They immediately will
know where that paper
sits in the landscape
of knowledge or that
field.
and that's what
knowledge space,
that's what
cartography is all
about.
So any beginner can
collect data and
facts, but a true
expert weaves them
into the very fabric
of knowledge space.
That representation of
knowledge is what
sets apart the
expert from the
novice.
And of course,
this is a beautiful
picture of Albert
Einstein.
That's not,
totally not a quote,
but Albert Einstein,
do not let random
quotes move in.
But that's the
power of map
design.
The style of maps
is that we as
humans have been
using them in our
heads for thousands
of years.
We are embodied
creatures.
Remember the self
in Yosha's talk
was in the world.
We are embodied
physical creatures
and we have
developed a lot of
intuition for
physical spaces,
for topology,
for where the
landscapes are.
When we go skiing,
how many of you
have gone skiing?
Good.
How many of you
have gone skiing
and used the map?
Good.
So when you arrive
there, you basically
see that there's a
representation, a
simplification of the
landscape.
The landscape itself
is extremely complex.
You're not going to
map every tree.
That's a meaningless
map.
What you're going to
do is map every
path, every landmass,
every road, every
obstacle, every gondola,
and so on and so forth.
So that's what maps
are about.
That's what we want
from cognitive
landscapes.
We don't want to just
say, oh, here's all
the points.
We want to say, and
here's how they're
organized.
Because maps
simplify, maps
abstract, maps
summarize.
And again, that's all
about representation
learning.
This is all at the
root of how humanity
works, how cognition
works, how AI works,
how evolution works,
how genome works, and
everything else.
So in that context, we
can now go and map
these cognitive
landscapes.
And we've basically
said, okay, let's
start with one modality.
Let's take every gene
in the human genome,
take its description
as if it was some
sonnet, and then
map their descriptions
in cognitive space.
And guess what?
You are rediscovering
the organization of
biology.
of course 20,000 genes.
It's remarkable.
And then you end up
with, you know,
central biogenesis,
ubiquitin regulation,
onophagy regulation,
apoptosis regulation,
and all these things
are actually near each
other, which makes a
lot of sense in the
cognitive space.
You can also just,
not do that in biology,
you can do that in the
New York Times.
So we took three million
articles from the New
York Times, and we
just mapped them in
that same latent
representation.
And they were able to
navigate now three
million articles.
I don't know the last
time that you saw
three million articles
on the same page, but
it's not something that
we typically do, but
it actually makes
complete sense, because
the AI representation
of knowledge is
actually matching the
human representation
of knowledge.
And notice that we
can give names to
those entities by
summarizing the
concepts using these
latent embedding
representations.
We can also go to
time with this.
We can basically have a
blast.
At the top left here,
I'm basically showing
you every single paper
that was written by an
MIT author, along with
every grant obtained by
the same scientific
lab.
And while we're at it,
the physical two-dimensional
map of campus, where
those papers were
written.
So you can actually
start seeing the
connections between the
cognitive world and the
physical world.
In the middle here, I'm
basically showing you
how knowledge changes
over time.
Where you can basically
see those same
scientific papers, but I
can scroll a bar that
basically says which
papers were written in
which decade.
On the top right over
here, I'm basically
showing you 100,000
loans with quantitative
data about their
phytospore and their
grade, along with the
cognitive projection of
the corresponding
description of those
grants, that allows me
to now start saying
which grants are fair or
unfair, given the
context.
At the bottom left, I'm
basically showing you
every class taught at
MIT with their
relationships and their
prerequisites and how
the knowledge flows
across these different
graphs.
And on the bottom
right, I'm basically
showing you data from
my own lab, from every
meeting with that, and
who's reporting on what
and how people are
progressing over time.
And as I mentioned
earlier, these
maps don't need to be
language only.
In this particular
case, I'm showing you
the joint embedding of
protein, structure, and
function.
And these maps don't
need to be just, you
know, cute little
diagrams.
We can actually go and
click on those maps
and go and learn about
this latent landscape.
So basically here, we
have a latent embedding
representation across
protein structure, function,
and knowledge graphs,
420,000 human genes.
But what's really cool is
that I don't have to
stop there.
I don't have to just
simply leave them out as
printed.
We're in the age of AI.
So I can basically use an
AI assistant to understand
how the path of, say,
these genes, you know,
relates to their function.
So I can basically, you
know, create these bags,
and from the bags, start
asking, you know, for
example, I can name this,
I don't know, path one,
path one, and then I can
create a new bag, and
select some more genes.
And, you know, now call
this, I don't know, FHF
complex or something.
And then I can, you know,
sort of create another
bag, and, you know, I can
basically now ask, how is,
I don't know, FHF complex
different from, you know,
I don't know, path one.
And the AI assistant is
basically receiving these
documents as context.
So this is basically
retrieval, augmented
generation, or rags, but
on steroids, because it is
visual.
I'm able to directly ask
the AI about the documents
that I'm interacting with.
So that's what I want you
to sort of get away with,
the fact that these
limited spaces can
actually be directly
explored.
You know, you can use a
lasso tool, you can
basically use a hierarchy
to go and understand how
all of these relate to
each other.
And, for example, these
are all of the genes
associated with protein
phosphatase function.
And I can search for, I
don't know, lipids, and
I can find, you know, all
of the genes associated
with lipids, but instead
of just looking at a
giant list, I can see how
they fall in the context,
in the landscape, in the
cartography, and I can
sort of organize them this
way.
So we've built this for,
you know, almost anything.
So, and, you know, let's
see, so for example, who
has heard of Lex Friedman
here?
Lex Friedman?
Yeah, awesome.
Cute fan.
So Lex has had a ton of
conversations, including with
Yosha, including with me,
and we can now go and, you
know, search for Yosha, you
know, for example.
That's easier to spell.
And you can basically find all
of the conversations that
Yosha has had with Lex.
And we can see sort of where
they fall, and you can sort
of organize them based on,
you know, their, I don't know
if you recognize that, how to
deal with loneliness.
Yes?
And you can kind of click on
that, and basically go and,
you know, find a corresponding
space, and you can kind of zoom
in, and you can see here
that at the center of Lex's
podcast lies, huh, huh, the
meaning of life.
And, you know, that meaning of
life, when you zoom in, you
can kind of learn more about
existentialism, about the
simulation debate, and free
will, and virtual reality,
and exploring life and
mortality, and so forth.
Who wants to use this?
Raise your hands.
Yes, awesome.
Who wants to map some crazy
space to this?
Raise your hands.
Awesome.
Yeah, I do too.
So, this is called Mantis.
It's a little, like, tool that
we built with mostly high school
students and freshmen and
sophomores here at MIT, and
we're looking to apply to
almost anything, to map
cognitive landscapes, to
understand human thought,
to understand how we
represent knowledge, and we
basically created this
panoply of tools, including
this narrator that I
mentioned earlier, but also
agents that you can deploy
and sort of carry out
computation, an orchestrator
that receives the results
from those agents and they
compute, this compute
Jupiter Sandbox, so that
when you ask for something
more complicated, it
generates the code on the
fly that then goes running.
You can go and edit the
code, and you can go back
and forth.
You can export the
clusters and the tree and
the names.
You can create variables on
the fly.
You can analyze basically any
kind of data with it.
So, embrace embedding
spaces, embrace cartography.
Our ancestors have been
using it for a while, so
the way that the Unidas was
able to fight off the army of
searches and save the
Western world back in, you
know, well, like 2,000 years
ago, was by understanding the
map, understanding where in
the landscape to intervene.
And these cognitive maps are
basically made for the things
that we have evolved for, for
hierarchy, abstraction,
collaboration.
Oh, I forgot to mention that
we also have this collaboration
button that allows you to now
see how multiple people are
browsing the map at the same
time.
Just like you can share your
location with your friends when
you're exploring the landscape,
you can do the same thing
here.
And what's really fascinating
is that the relationship of
documents to each other in
our brain and in the LLM
actually matches.
The pairwise distances are
actually very similar.
If you start looking at brain
activation maps and LLM
activation maps, they're
actually very, very similar to
each other.
So it really allows you to
peek inside the brain of the
machine.
And I'm going to leave you
with this cartoon here with
Dilbert, who basically says,
oh, Zimbo the monkey has
designed three commercial
products this week.
You best find out his secret.
He's using his tail.
He has a natural advantage.
I feel the jaws of evolution
on my throat.
Good, great.
Did you see him cut and
paste?
And then the monkey explains,
your big mistake evolution
wise was inventing computers
that are easier to use if you
have a tail.
In an ironic twist in the
Darwinian saga, you guarantee
the extinction of your own
species.
Dilbert is of course saying,
stop working while I'm
talking to you.
I can hear the evolutionary
clock, tick, clock, tick,
tick, clock.
So what's happening here is that
we as humans have created
data landscapes that are
unsuited for us.
We basically relegated the
integration of millions of
documents to ChatGPT and to
search engines and to
whatever, I don't know,
Facebook's algorithm will
basically give to us next or,
you know, YouTube Shorts or
you name it or TikTok.
And we are possibly consuming
whatever the AI decides for us.
What I'm hoping this type of
technology allows us to do is
bring the humans back into a
loop and not fear spaces with
millions of entries, with
millions of documents.
And we've used these to map
people, to map patients, to map
contacts, to map events, to map
books, to map chapters, to map
documents, to map your hard
drives, to map your email, to map
your LinkedIn, to map almost
anything.
And we're looking for partners.
We're just building this, you
know, from nothing.
So if you guys want to
collaborate, reach out.
My name is Manolis Kellis.
And you can find me right with
Canoli, so you'll never forget
Manoli, Canoli, only Greek.
And again, AI has a natural
advantage in unmapped spaces.
But with the proper tools,
humans have always prevailed.
So here's one for humanity.
There we go.
Time for one really deep
question.
Let's take two shallow
questions.
You guys, like whoever wrote
their hand first.
I saw your hand way, way
earlier.
Yeah, so super interesting
thought, thank you.
When you represent something in
future space, if that data
in the future space is as much
as the data, then you're
missing the point, right?
Of course.
How do you minimize that
future space to be truly
representative?
Yeah, so these are living in
very high dimensional spaces.
So basically, papers live in
1,500 dimensions.
We compress them to two
dimensional for humans to see
them in a function that
preserves the distances of all
the nearby points.
And therefore, we're actually
preserving the super, super
high dimensionality in the
distance functions of pairs of
points to each other.
And that's what leads to these
meaningful landscapes.
Next slide.
Great.
Do you think that social media
outruns are building your own
representations and give them
reward systems?
Absolutely.
Absolutely.
That's how they're serving
the stuff.
They're basically saying, oh,
I'm going to serve you
something for a pre-review here.
The only thing we're doing is
we're making it explicit.
We're giving you a data science
workbench to work with it.
And then you need to use this
then to model or then view
that reward system?
I would like to be able to,
when I browse my social media,
say, oh, you know, just show
me all the, like, I want to
see my browser history in
latent space.
I want to see, you know,
where I've been.
I'm like, just get rid of
that cluster.
I like this one and that one.
And sort of show me, ask for
that one.
Wait, your browser history?
Everything.
Oh, no.
Oh, you heard about
input data, my friend.
My mom always told me,
don't worry about two types
of decisions.
The decisions that are easy
to make, because something is
very different, you just make
them easy.
The decisions are hard to make
because those two choices are
kind of similar to each other.
Don't worry about those.
So, basically, when you have
non-determinism, it's usually
between places where, I don't
know, turning right and
turning left over here on the
ski slope is probably the same.
So, I would argue that the
differences in representation
that I have and you have are
probably non-consequential.
They're probably different, but
non-consequential.
And therefore, at the limits,
because the human is back in the
loop, you don't have to worry
about those.
Yes?
Can you elaborate more on the
parallels between the brain
activation maps and the LLM
activation maps?
So, the way that I learned
English is probably the same way
that you learned English, by
reading a bunch of stuff that
made no sense at first.
And then, eventually, I started
seeing how the word, I don't
know, the typical example is
president and king and dictator
and, you know, something, are all,
and leader, for example.
They're all living in the same
space.
Why?
Because in next-word prediction,
the context that allowed me to
predict the word president was
actually quite similar to the
context that allowed me to
predict the word king.
And therefore, president and king
got a similar coordinate in this
latent 1,500-dimensional space.
And that's how we probably learned
language, and that's how the
machines definitely learned
language, with this next-word
prediction or hidden-token
prediction that basically allows
you to learn latent vectors for
every word.
You now combine these words
together with attention models
that basically allow you to now
transform the original space
into a revised space where the
word apple is no longer in a
fruit basket, but it's instead in
a hardware store.
And by understanding that
hardware store, I'm probably
using the wrong word, like
apple store.
The apple store basically moves
the word apple from a
representation of a fruit
basket to a representation of
a computer.
We're in the time.
Yeah, okay.
By the way, actually, if
anybody wants to hear more of
his incredibly insane and
genius ideas, I mean that in a
good way, by the way, he's
going to be on Kirchai Mongul's
theories of everything podcast
very shortly.
We're actually filming that on
what?
Tuesday?
Yeah, in like a couple
days, and that should go up in
like a week or two, so you'll
get more in-depth, more detail.
You're going to feel like you're
inside of his mind.
Tune in for that, and we'll
share it online.
Give it up for Manolo
Skeller.
I would like to see an inverse
Turing test at some point.
Imagine you have a jury of LLMs that
try to figure out whether what
they're talking to is an LLM or
just a human impersonator.
And it's something that fits
nicely in the scope of a small
hepatotone.
And it would be really
interesting to see if we could
basically plug in the jury of the
current LLMs, prompt them in
different ways, then have
different candidates, which are
humans or LLMs, and see what
happens.
Let's do it.
