Hi, I'm Connor. I'm a mathematician, now working on empowering people to collect and use their own data.
Hi, I'm Amy. I'm an AI engineer intern at the startup.
Yeah, we wanted to explore how AI could be used to enhance cognition, reasoning, connectfulness, and mindfulness to help us answer previously unanswerable questions.
So yeah, we have this questionnaire for you guys. I'm going to give like a quick like two minutes.
No, you don't.
A quick minute, if you guys could answer it, the question as best you can, like a one-sentence answer as quick as you can.
Alright, cool. Can the essence of consciousness exist without a physical body?
Probably not. Okay, interesting. Probably not consciousness dynamic. Interesting, it shows the same one.
No, it's an emergent property. With the data we leave behind, yes. I answered 12.
Okay, great. Same thing. That's the question.
How would dramatic life extension change family dynamics and relationships?
Wow, okay, they all chose the same answer. Dramatic changes in intergenerational dynamics and perception of relationship length.
Huh, okay. Interesting.
No, I guess no someone answers. And finally, what is the meaning of life?
To love and be loved. No meaning in itself, but only meaning you give it.
To love and be loved. To love and be loved. Huh, okay.
Raise your hand if you were one of the winning answers.
To love and be loved. Winner, winner, winner. I do a prize as well if you want.
Cool, yeah. I don't know what happened in the first one.
I think we answered unknown or something.
Someone answered unknown? Oh, did anyone answer unknown?
No? Huh, okay. Well, I don't know. We confused all the AIs somehow.
Cool, yeah. So we asked all these different AIs. And, yeah, so kind of what we're trying to get at here, which didn't work very well, is, you know, AI bias versus AI opinion.
Right, so if you ask an AI, you know, what is the meaning of life?
It's going to give you some response of, you know, all of these different philosophers, all these different ideas, what do, you know, they say about the meaning of life?
But it won't actually give you, like, an objective, definitive answer.
So kind of the idea here was to force an AI to take an objective stance on these very subjective, non-answerable questions by just having it judge human answers.
And, yeah, it's kind of interesting because, you know, the reality of the majority of our current AI interactions is, like, these prioritization systems and optimization systems.
You know, whether it's, you know, ordering your email or whether it's ordering your feed and news, you know, how you receive news.
You know, all this news in the world, what is the information that you see?
And an AI is having to objectively take all these things into account to decide what are the first things, right?
So it's having to take the subjective kind of goals and throw them into this objective output.
Overall, you know, as we hand over more and more responsibilities to AI, and we designate more and more of our subjective reasoning to these AI systems,
we kind of believe that it's increasingly important to not only monitor the bias in these AI systems, but also to monitor the opinions of them, right?
So if we ask this question, you know, the bias might be, okay, what are the philosophers that it reads, that it culled from?
You know, what are all of these different sources of information?
You know, what cultures is it trying to take into account?
But the actual, you know, opinion of it is now going to be, okay, what is the final answer that it chooses out of all these answers to give to you, out of all these human answers?
And, you know, we believe that really, we should use this tooling to empower human thought.
You know, Dr. Kellis, who was presenting Mantis the other day, this idea of like, using AI to have humans be able to look at this huge, broad spectrum of information,
and to try to, you know, condense it into concepts that humans can understand.
And, you know, we kind of question is, if we have more and more information from all these different cultures, all these different groups, you know,
can we empower people to use this information to answer these previously unanswerable questions?
You know, if we can empower people, then could an AI do it?
Is it possible to have a God AI that takes in all this information and is now able to somehow answer these questions in a way that, you know, is generalizably accepted?
Or even not generalizably accepted, maybe it's personally accepted.
So in a better way for each of us individually to understand and to answer these questions for ourselves.
And, you know, this kind of also gets to the question of God AI versus decentralized AI.
Where, you know, could you have this God AI who can answer this question?
Or do you need to have all of these individual AIs who are, you know, have your subjective, you know, perspectives built into them in order to help you answer these questions?
Yeah, thanks.
Judge questions, quick.
Judge questions.
You mentioned that your AI is not generative AI in the sense that it's not generating the answers, but rather judging human answers.
It's like in the best ones.
Mm-hmm.
Okay, so in that case, you would be bypassing the fact that AIs can't actually reason at this stage, just predicting the next token.
But it still has to reason in order to reason to what its answer is, right?
Like if you actually look at like the back of its entire response, you see that it does justify its reason for why it chose the best answer for the meaning of life or any of these different answers.
Are you looking for a specific one or an LLM in general?
Just the LLMs in general.
Yeah, they do, they have, well, they come up with an answer and there's a reason they come up with the answer.
But the answer is usually intractable.
I mean, in the actual, like, neurons, you wouldn't be able to predict it or backtrace it.
Well, they're not explainable.
Yeah, you could.
You could use some explainability.
As far as I know, I mean, I've heard that perplexity AI is one that actually does give you like a source and justifications.
So I was wondering if you had interacted with that, for example.
Yeah, I mean, yeah, I think it is kind of a similar concept, right, where, you know, perplexity is going in and taking these exact sources and now just trying to extrapolate from that to find the answer.
Yeah, I think overall, I, yeah, it is still reasoning.
The AI still is having to reason because it's having to prioritize, right?
It's saying what is the best of all of these answers and there's reasoning behind that prioritization.
But you kind of get around that it, you know, normally it'll try if humans are very opinionated and humans, we all just gave our opinions.
But generally, when you try to ask an AI for an opinion, it's not really going to do it.
So this is kind of a way to try to hack around that.
Does that make sense?
I just wanted to bring up one thing.
Yeah.
Like every single question, even like the most simple prompt is you're projecting a priming stimulus that is from whatever you guys think is a good question to ask it, to choose between like different options.
So I mean, like it's impossible to give unbiased even usage of these because even the language that you're using English, you know, will draw out different responses.
So I think, I mean, it's, I think a good idea to have sort of a range of questions, but, you know, what would people actually ask might be very different from this kind of, you know, any leaderboard or anything.
Definitely. Yeah. And I mean, we're just this group of people, which is obviously a very small specific subset of people.
So super biased off of that. Yeah.
To your point regarding reasoning, 01 previews an example of a model where we're actually not seeing the full rationale.
It's actually put through a filter and that gives us the big picture since it can be used to back deduce how the model was created.
The question I'm wondering for you is, even for models that don't have direct access to the web, you put in the same question twice, you don't get the same answer twice.
Yeah. And I wonder if you've done a, or are considering doing the equivalent of sensitivity analysis, same verbiage consecutive to see the range in one model.
Because all you have is one point estimate. You don't know how to have a distribution. It could be, it's a very wide range and you happen to get the left's tail.
Yeah. Yeah. No, definitely. Yeah. I think that'd be really interesting. And obviously you can also like change the temperature of the model and that kind of does that.
I think kind of the point of this was that, you know, when you're, you know, on a new speed, when you're doing all those different things, the output you get is binary.
You either see something or you don't see something and kind of, yes, the AI, if you ran it twice, it's not deterministic.
You're going to get a different answer that it's choosing at its best, but this is the one that I gave in this case. And how is that going to impact you?
Well, if you're comparing between different models, it would be more representative to get a sense of the variance.
Because if you test each model once, it could be, it was on the left distribution for model A and it's on the right side of the distribution for model B and the relationships actually flipped.
That's like the ecological bias. So suggestion for future steps.
Cool. Yeah. Makes sense.
Awesome.
