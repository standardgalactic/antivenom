all right what's up everyone we're back for uh another polymath salon slash podcast online we're
doing many of these these days uh today we are joined by dr william han from the machine
perception cognitive robotics lab and a very cool ai researcher named peter voss who's working on uh
cognitive agi right this is uh you're calling it uh cognitive what's your phrase again for it
cognitive ai yes cognitive ai yeah cai right um but we're also joined with a very fun audience
which will get interest for them in a second uh but before we do i want to make sure everyone had
a chance to check out the scott aronson and dugan hammock uh ut austin video that's up on the
channel uh the scott aronson video especially is getting some decent uh engagement which is always
good to see and uh dugan and scott both did a great job in fact dugan is here on the call today i
believe somewhere um but yeah all right let's uh let's get some uh quick intros uh michael you want
to start so i'm a phc physics student at fau i'm a also a member of the mpcr lab and um i'm an
affiliated researcher with the wolfram institute as well you're uh you're you're a student of dr hans
right oh yeah he's my uh he's my advisor uh for my dissertation nice uh jordan you want to give a quick
intro howdy y'all jordan parker uh down here in austin texas obviously uh independent researcher
focused at the intersection of alignment and ai uh most recently working with alignment lab ai
awesome harry if you're on the call if you want to give a quick intro yeah i am uh yeah my name is harry
um advisor with uh in residence for 1517 fund and also independent researcher in consciousness awesome
larry quick intro i'm larry chang i physics connecting the compression models and compression
algorithms within physics specifically connected to money and my claim to fame is i take notes and
learn kind of quickly and it's applicable to money and physics and this this with the simple thing which is
how does a stuffed animal connect to physics connect to to dugan's donut so i i get the impression larry
likes to really connect things that's perfect for all things ecolopto polymath i'm very i mean i've i've
had many thought experiments where i've taken just seemingly very random things like a stuffed animal and
how does a stuffed animal connect to a skateboard into some optics thing in physics somewhere you know
just just see seeing how they connect right i love it uh dugan quick intro i'm dugan uh i'm a mathematician
geometer and artist and i work at the wolferman institute as a research coordinator i like making
hyper shapes wonderful and he was actually just presenting also he was with dr han yesterday on his
uh hyperdimensional geometry which he also did present at least part of it at the ut austin uh polymath
so long we did a couple weeks back i'll keep going camera quickly and say hi um hey uh background in
data science uh research coordinator at orthogonal research education lab where i made a condition
futures uh working group so all these topics are super interesting to me a little background in cognitive
science and other things and working on a lot of leadership uh development and innovation projects
and uh super thrilled to be here so awesome jess it's great to have everybody here we have
several uh a couple new new names to the salon and then also some that you've probably seen before
but let's now finally get into our speakers for today we have peter voss and dr william hon peter
you want to give a quick intro yeah sure um so i started out as electronics engineers started my own
electronics company doing industrial electronics fell in love with software and my company turned into a
software company and that actually grew quite nicely from the garage to 400 people did an ipo and it's
really that uh success allowed me to uh start working on how do we build thinking machines and i took
off five years to study all different aspects of intelligence epistemology um psych cognitive psychology
technology and what had been done in computers i can talk more about that and in the culmination of
that i came up with a design for a thinking machine um and 2002 i got together with a few other people
who had similar ideas and we wrote a book on that new approach getting away from narrow ai to general ai and
we came up with the term three of us came up with the term agi artificial general intelligence that's in 2002 and
since then i've been alternating between doing development and commercializing um and i believe
we are now quite close to being able to develop a full human level agi first i just want to say amazing
peter looking forward to speaking with you um yeah very happy to be on the call thanks addy for putting
this together i've been thinking about ai most of my life um i got involved with computers when i was very
young the first adult word i remember learning how to spell was install because if i put the floppy disk
in and type those letters in in that order the game would turn on and in high school i tried to kind
of hack some uh some simple ai together to play uh you know minesweeper and stuff like that but it
wasn't really until i got to college and got some physics and uh you know vector spaces and things like
that linear algebra under my belt that i was able to uh start hacking at some neural networks and
amazingly i came across the idea of artificial neurons in about 2003 and then around 2004 i
started kind of coding my own versions up and uh kind of fell in love with that and been doing it
ever since um about 10 years ago uh with some colleagues down here in south florida put together
this machine perception and cognitive robotics laboratory largely to get ahead of this kind of
tsunami and progress that we might call the singularity and make sure that students were prepared for
the types of world the world we live in now basically with things like chat gpt and stuff like that so
i'm super excited to speak with you peter and uh thank you yeah i was gonna say the way i would like to
to form this you know kind of format this today is i want dr han and peter to be able to speak about
whatever is kind of intellectually interesting to them i think there's a certain member of the audience who
who has suggested this is like a uh sort of a freedom intellectual playtime like you sort of have
the ability to really you know speculate a little bit you know kind of question some of your ideas and
things and be willing to explore paths you wouldn't normally explore i have also had thoughts of calling
this a shower thought salons because it has a lot of things that could be called shower thoughts right
um but yeah so i guess the first question that goes for both of you is what type let's say you did have
some kind of other you know civilization human-like whatever right what type of problem would you have
to be solving where you'd want to create another kind of intelligent machine or system
so so let's just say let's just say you're um uh you're some other kind of civilization you're human-like
or non-human but some other kind of intelligence right what type of problem would you have to be
solving where you think about creating something like an ai right because if you think about like
i think a lot of it probably does come from automation because even if you look at you know
older points in human history we're always sort of trying to make things more efficient automate it
so that we can focus on more important tasks and so my my question to both of you would be
what is the actual purpose of creating intelligent systems besides the way that we do it naturally
through reproduction right now yeah well um intelligence is obviously the driving force
you know it it used to be you know i think that i i forget who wrote wrote the book on that but it
used to be brute force that you know won out then um then it was money and then it was information
and ultimately it's intelligence you know that trumps all of those with intelligence you can get
information with information you can get money and with money you can get brute force so you know
there's a sort of hierarchy of well that's obviously the big the biggest thing and um human
type intelligence is the pinnacle of what what we know of in intelligence and they're very specific
um aspects of human intelligence that make it so powerful and i think that's a big part of the
research that i did over the five years is to understand what makes human intelligence so special
and and so powerful and i consider that as sort of the first principle approach to that and so it's it's
hard enough building artificial general intelligence or human level intelligence here in our current
environment so i i wouldn't speculate if i was some alien thing that just introduces you know other
variables so i think it's it's it's hard enough uh to talk about how to get to human-like intelligence in
a machine and it's it's not it's not about automation that that's kind of the boring part you know where
you already know how to do something and you just automate it the interesting part is basically being able
to form new concepts to have new insights to discover things so that we can make a much more rapid
progress in science you know whether it's curing diseases or building quantum computers or
developing nanotechnology uh or you know fusion or cheap clean energy all of those require new invention
new insights new concept formation and that is what intelligence is all about awesome uh yeah absolutely
agree on all of that addy i'm trying to think your question uh in terms of when would you create ai
i think i would think about it more in the other direction in scenarios in which you wouldn't have to
um and it makes me think of things like whales and dolphins that i can imagine them being happy in
the world as long as the oceans didn't change too much uh they could be happy for millions of years kind
of doing the same thing that they're doing now whereas it's very difficult to imagine humans acting anything like
they do now in even a hundred years and so is there sort of this natural kind of arms race is it built
in the kind of evolution the arrow of evolution itself where after you get feet and teeth you create
a brain and and so on and then you eventually create a brain externalized uh i don't know it makes me
think of things like dune where uh thing like computers have just been outlawed altogether where this is
what uh samuel butler talks about and so this idea is named after him this butlerian jihad where you'd
have you sort of outlawed it for political reasons but i think it would be interesting to find out um
sort of natural scenarios whether it's an ecosystem or whatever where you don't have to create an ai
and is that a stable state is that something that maybe would just last a few million years but then
you poke it a little bit and then suddenly you get back onto this kind of uh moore's law attractor
yeah i don't know if it's inevitable that that human intelligence evolves um it seems to be probably
because um you know brains have developed separately vision has developed separately it it's just they
allow an organism to survive and flourish in a much wider range of environments so there's you know
there's that advantage and given enough time you know you end up with with with humans that can
really flourish and survive and in a very wide number of uh environments but that that also then
ultimately gets gets us to where we think about the purpose of life and what you know what makes us happy
and um we become ambitious and we don't want to die i mean that's also one of the big things is once
you smart enough to uh be aware of your own mortality um things change you know then it's
it's a it's a different ball game whereas animals are not aware of their own mortality do you think
do we know that animals are not aware of their own mortality yes that seems weird i don't think
that's true i'd have to i'd have to imagine that they'd have they may not necessarily think of it
of mortality in the same way that we would but you know especially having spent you know if you
if you look at sort of the way sometimes an animal who is sort of unwell will carry themselves it
can be it can you know there can be quite a personality change uh in them um you know i'm not
saying that they they conceptualize mortality in the same way that you know humans do but i i'd have
to mention they they have some sense of sort of the temporary nature of things i mean maybe no
interesting it makes me wonder what it is that that we have such that we are capable of kind of
modeling that modeling reality where we're not even in it um and i don't think why is it this animal can't
do that and is it something related to language because it makes me think that things like uh the myths
essentially that arise around death uh myths are sort of the realm of language models of speaking
right we tell stories and i wonder if animals could tell stories would they tell stories of dying well
they can't i mean there's a fundamental disconnect between animal intelligence and human intelligence
and the the the key difference is that we can form abstractions of abstractions of abstractions to
to really any level to to come up with concepts that we can conceptualize uh things like you know
honor and and so on and it's that ability to be able to conceptualize abstract concepts highly abstract
concepts that make our intelligence and our consciousness quite different from animals animals
can only form concepts at the concrete bound level basically on what they perceive and interact with
with the world and you know there are lots of animal studies and experiments uh that even the smartest
animals animals that have been raised uh in families like like a child and you know um like certain
chimpanzees may um actually develop faster than human children up to 12 months or 18 months and then
they stop whatever you do it doesn't matter how much uh how much you give them of an environment and training
they simply cannot form those high level concepts um or you take parrots that are very smart and can
learn all sorts of tricks their cognitive ability pops out on abstraction they cannot abstract
something like shape or color without the perceptual grounding uh it's been well studied so there
there is there is a can you repeat that last part can you repeat that last part um yeah they cannot
abstract uh things like color or uh shape or texture or something without it being grounded in perception
and and that's why animals can also not have syntax you know they can have a stimulus response and
the the language abilities uh are very uh are very limited there are lots of studies that you know that
show that and as animal studies one of the things you always need to be careful of is how much the
experimenter basically also reads into behavior of of animals that's true that's true you know that that
that is a that's a that's a big risk it reminds me of uh this concept from science fiction called animal
uplifting that we might be able to use technology to kind of enhance the intelligence of animals
do you think it's a matter of more layers of cortex or a a sort of different layout of cortical modules
that if we could if we could just get a tweak the development of a golden retriever for example
could they learn to read at a third grade level i think so that seems quite plausible because
there's not much evolutionary distance or genetic distance you know between um higher higher animals
and humans so they're pretty scary to do that though you know having sort of the the animal animal
the animal reptile brain versus our reptile brain which has already been sort of softened softened a bit
um yeah and it's an enormous enormous moral responsibility as well if you uh if you if you did
that so i'd be very reluctant to do those kind of experiments but it seems like for better or worse
that's kind of what's happening is we're gonna have multiple intelligent species on this planet
whether whether we like it or not we're gonna see chat gpt's and humanoid robots and then agis and then maybe these
intelligent animals and so on are people with neural links so there's going to be such a wide diversity
you know what do you what do you think about that because well i mean chat gpt and and large language
models are really don't have human-like intelligence at all um well at all okay that obviously there's a
lot of debate you know can they think and they understand and to to a certain degree they they can but it's
a very different kind of um intelligence and it's no nowhere near human-like intelligence and we can
certainly talk about that and you know a human with neural link uh and i mean agi will be true human and
superhuman intelligence that's kind of the definition of of agi so when we get that uh things will get very
weird um because um we will have entities that will be significantly smarter than us um quicker quicker
than us and when we have those entities at what i call uh what you know and i'm talking about agi here
and and if if in the audience we want to kind of go over what agi means we may want to spend time on
that because there's a lot of confusion on the term agi but once you have true human level ai agi one of
the key applications is is what i call a personal personal assistant and the reason i call it double
up on the word personal i believe that it should be something that you own and you control not some
mega corporation so it should be a very you know personal personally owned and controlled thing that runs on
your local computer or smartphone or device or whatever and once we have that that really becomes
like an exocortex because if we constantly interact with this agi that helps us think things through
that does things for us it's it's a bit like a couple that have been married married for decades you
know that after a while you don't actually know was it his idea or her idea and it maybe doesn't matter
that much anymore and it'll be like that with our own personal personal uh assistant agi um you know
we'll ask it something it'll give us feedback we'll give out advice initially we might ignore it for a
lot of the time and you know and then it comes up with an idea and we execute the idea now whose idea
was it and does it matter after a while it won't matter it'll just be part of your own yeah extension of
your of your of your own uh intelligence and exocortex really and so you know that's one of the
great things that that agi will will enable us to to do so we could so we could kind of we could kind
of think of a laptop as kind of an exocortex um and we can think of some of these things as starting to
approximate uh agi the tools that we see out in the marketplace now i'm curious what do you what are the
big milestones for you where what would you see and then you'd say okay that's it we're at agi now if
it is a hard boundary or is it more nebulous and then do you think there's a strong distinction or
an important one between agi and asi super intelligence or at that point it's all just
yeah good question let me let me answer the second one first because it's a quick one uh the the concept of
a si um super intelligence doesn't really make any sense if you understand artificial general
intelligence because once you have something that has the ability to think and learn uh at the level
of a smart human uh it can improve its own design basically the work i'm doing now once we get to agi
agi will be able to do my job better than i can so so clearly there will and that that's really the
definition of the singularity when you get to a point where uh an an ai can autonomously and that's
a really important point because people argue hey we already have ais improving ai design but it's very
much with the human in the loop and it's a tool but once you can have uh an ai autonomously design it
improve its own design um you you obviously have as as i wait however whatever the limits of
intelligence are and you know that that that is a very interesting question um you can't really measure
an iq but you know if we try to do an iq is there at what point are physical limitations do physical
limitations uh kind of impose a limit of how smart you can get and you know that that's kind of kind
of an interesting question so yeah i think the concept of asi is is a marketing term where basically
you have people like sam altman who say oh we know how to build agi we'll do it soon but it's
not going to be a big deal now you know he's either lying or he's ignorant or both uh if you get to agi
it's a really big deal so um so what how do how would we recognize that we have agi i think the there's
some actually i have i have a slide i can share um you know with large language models the focus is
on having a lot of knowledge and skills so you have 30 trillion pieces of information that you
number crunch for months on end and then you have this model that encompasses this these knowledge
and skill but it's static whereas true intelligence what makes human intelligence special is our ability
to learn but interactively in real time and form new concept have new insights
um and to do that autonomously without a human in the loop who has to you know fiddle with prompts
or rags or you know uh specialized fine tuning or or whatever so the key the two key things is it needs
to be uh autonomous that a human doesn't need to be in in the loop and uh it needs to be conceptual
contextual and be able to learn in real time so there's there's a there's a very big difference and
here's kind of a chart of comparing cognitive ai which is what dapper calls the third wave of our ai
we currently in the second wave of ai but once we get to the third wave of ai it's not about how much
knowledge you have but it's how good are you at acquiring knowledge you know so it's not about the
quantity of data it's about the quality of data and it's the ability to do this autonomously and
one of the side effects is also that you'll need massively less compute you need massively less data
i mean a child can learn language and reasoning with a few million words not 30 trillion and so
you get the benefits of something that that requires much less compute but at the same time is much
smarter and i it something that that kind of just really became clear to me on how absurd the current
lm lllm approach is when you think when you step back and think about intelligence just think about
it a modern llm has many thousands of tokens as system prompts so you have you know thousands and and
they're starting to publish them now all the instructions on how to handle stuff imagine you now
had a personal personal assistant with an llm you now give your own prompts to it on how you wanted
to interact with it you know what tools you use and how you wanted to use what you don't want it to do
what you wanted to do what it's allowed to do what it's not allowed to do so many thousands of tokens
set of your own personal prompt that you have now imagine you've had this large language model that you've
that's acted like your exocortex for a year you have a year's worth of interactions that that it has
now think about the way llms work every single word that it outputs every single token it outputs it has to
process every single token all of the system prompts all of your prompts and a year's worth of
conversations that you've had for every single word i mean that's that's worse than a sloth at the dmv
you know i mean it's insane you know how can you have something and call it intelligent
that to utter one word it has to process everything that that has ever come across and that's how these
things work and that that cannot be overcome because the core model cannot be updated in real time that's
you know inherently peter yeah quick quick thing so we had a couple questions from the audience that
i feel could be looped looped in quite well here you know um there's a particular researcher his name
is michael levin there i think many of us are fans of him here in the audience and he talks a lot about
you know part of of the ability of intelligence is to recognize other kinds of intelligence i think
also sarah walker and lee cronin kind of talk about this a bit as well and so i think sort of circling back
to our definitions of intelligence you know this you know ai agi human intelligence animal
intelligence how much of what what good formal way do we have of actually identifying intelligence
across different kinds of minds right i think that's very useful for understanding what an agi
would be because it just just as humans sort of have a diversity of intelligences themselves in a lot
of ways i mean a very interesting anomaly i mean you know i find cool is this thing called savant
syndrome and you'll have people that you know sometimes they're very sort of you know they're
quite heavily on the spectrum right the the autism spectrum um and they may have struggle of function
in some area but then you know they'll have these incredible mathematical calculation abilities memory
abilities visualization abilities things that we consider extremely prodigious but they maybe they
lack for what we consider normal in other areas and yet you know if they were to take something
called an iq test which is supposed to at least in part measure something that we call general
intelligence a lot of those people wouldn't score as highly and yet they're still having something
that's extremely you know at least to us seems very refined and powerful and so i think even
you know in this kind of breadth of diversity of intelligence which basically means you know life is
sort of finding different ways to solve problems with these things that we call minds and brains right
is how do we actually define what an intelligence is and how does that change from you know again human
to animal to you know the planaria worms that michael lovin researchers to agi yeah i think there's a
fundamental uh problem here falling into the platonic trap of trying to find a definition for
intelligence that you know i mean you can have simplified ways of being able to solve problems or
whatever in you know novel problems in new environments or or so but then you know how does this math
prodigy fit fit into that and you know so i think it's kind of a a pointless exercise trying to um put
you know different animals and different narrow humans um and different ai systems on one line of
intelligence um what we're interested in when we coined the term artificial general intelligence
the g actually particularly appealed to me because little g in cognitive psychology stands for general
you know measure of iq and general intelligence now as it happens i spent the better part of the year
helping to develop a new kind of iq test um so i i actually spent quite a bit of time
um understanding what iq tests are good for and and and not and i've you know also being a mentor
member of mensa i've certainly came across a lot of people with very high iqs that were not super
functional in the real world so um one of the things that i learned that there are many dimensions of
cognitive ability but the one the the one ability that tends to correlate strongest with
iq is what's what's called metacognition that our ability to direct our mental faculties
appropriate appropriate to this to the situation so some some problems to to give us an example here
some problems require you to think through very methodically you cannot solve them unless you really
think through methodically some other problems require you to use memory to rely on memory to to be
able to solve them now of course we can write things down and use external memory so we can overcome
that limitation but there are other problems that don't have a systematic solution where you need to
stand back and sort of really have an more intuitive uh solution to it and then there are yet other problems
that don't have a solution and metacognition should help you to recognize it so that that is basically the
one dimension we found most strongly correlated with overall function functionality now of course you
then have people who may be good at taking tests but don't have the eq or the ability or the drive
you know for humans to succeed in the real world of course you also need to have the right
kind of personality and and the motivation and the drive um you know so
iq is basically the cognitive ability and iq tests measure cognitive ability now the unique thing in
this particular test that i was involved with developing in south africa was that it had very
little cultural we could say no cultural bias or almost no cultural bias whereas all the western
um western iq tests tend to have a quite a bit of cultural bias and that's why black people in south
africa did very poorly typically on on on those iq tests so this iq test was developed by um a friend
of mine she did her phd which is a completely novel it's kind of learning a new language and and uh
learning a game and a new language at the same time from scratch and um so kind of leveled the the
playing field so so i think there's a there's actually good understanding of what makes human human
intelligent or humans very capable cognitively now in terms of how how would we know that we have agi
this actually very simple test i i would say um once you have an agi that has the knowledge of
say a college graduate you know a reasonably smart college graduate uh with you know maybe diverse
general uh knowledge training uh you could put that person uh into a job to learn programming to learn
accounting to learn customer support sales management any of those things assuming they
had the motivation and they can learn it basically by you know reading manuals reading books uh like if
it's a call center situation they can very quickly learn that kind of job by themselves and that's the kind
of adaptability that you would expect an agi to have that it doesn't have a ton of training data that
you number crunch or anything and it can with that same core knowledge really do pretty much any job
any cognitive job i i should say that that is that's sort of a practical test for for agi and large
language models of course completely fail in in that in that regard
i'm worried if it's just or not worried but thinking it's a matter of time before we can get
the language model architecture to bite its tail though um i think of it as kind of like jet engines
versus feathers that the llm is kind of like this turbojet and it flies but it's very unsustainable
very expensive very difficult to produce and so on a extraordinarily delicate supply chain it's not robust
um you know it'll get taken out by another by a duck or something like that and then if we compare that with a blue jay
which just sort of show up every season and you know they eat earthworms and make their own houses and stuff
like i think to me that's what the kind of um the spectrum of agi and ai it's kind of like that to me jet engines and feathers
is that we have these things that are flying but it's not anything like what we used to think flying
was or what we expected it to be in sense um i like the yogi bearer's phrase the future ain't what it
used to be right well i i agree with you that um the agi ai is going to be very different from
from a human brain you know we're not trying to reverse engineer the the human brain we have
different material to work with and it's not evolution that's designing it it's engineers that
are designing it but i don't see large language models getting there uh and you know it is really
literally the second wave it's just large companies like google and and you know facebook and deep mind
had a lot of data they have a lot of compute that's the hammer they've got and they've done very well in
solving a whole lot of problems you know making translation easier image recognition and now video
generation and all of those things but these systems cannot learn by themselves they cannot learn
autonomously they cannot form concepts by themselves and then also the absurdity of having to process
zillion tokens to spit out one one new token it's fundamentally the uh the wrong architecture it makes
no sense um you know if you start from first principles you would not do that kind of design
so what do you think what do you think is missing what do you think is the pieces that the brain has
that we need to build into the machinery uh well the the single biggest thing that i can point to is
the ability to learn incrementally in real time because that that is you know what what humans do and by
learning you take that up one notch and say not just to learn static patterns not just to you know learn
like a dictionary or something but to then to be able to generalize and that that is you know that
what i was mentioning earlier that is what makes human intelligence so so special it's we can go
beyond aping and just copying stuff uh we can form new concepts and we can form these new concepts on the
fly and these concepts can be ever higher level abstractions and you know that's what happens when
like in this discussion we're having now what we learn from each other we form new concepts and and
sometimes they may be very very dramatic you know in terms of what you learn that could be like um i'll
try and stick to a less controversial thing um you know imagine you've been working on on a pit on on
research based on some famous uh research paper you know that everybody quotes and and so on and you've just
totally immersed in that and that's what your research is about and you now read on the news
that this paper has been withdrawn because there was some fatal flaw in it you know you instantaneously
update your whole model and you basically then have to rethink well not instantaneously i mean you
might have quite a few sleepless nights over it and nightmares but you know you update your model
and and basically then it has to kind of ripple through of what the implications are and you form new
connections you form new concepts new generalizations and you know hopefully unlearn the the wrong ones
right so that that's that's the the key is the ability to learn with limited data uh in real time
and to form new concepts and to be able to learn new things right i love the idea of modular learning
um one of the things i i joke about with the back propagation networks that if you have a really bad
day you could forget to see triangles i think that it sort of goes all the way back to the to the
sensory layer and that doesn't make a whole lot of sense we should expect a lot more well that reminds
me of info hazards right right um you want to talk about that yeah well just briefly though maybe in the
the same direction i think this problem of you know when do we get agi it it reminds me or i've been
thinking a lot about this uh the related problem for the fermi paradox which is sort of where are all
the aliens right so where are where are all the agis and uh where are all the aliens out there and i
think it's both of them are kind of the same problem they're two sides of the same coin is the is the
difficulty of recognizing uh advanced intelligence in any in any form one of the examples i like to
give is we know kind of you know carl sagan and gang they got the radio telescopes and they pointed
them out and sort of tried to listen for radio traffic but if we were to imagine even just this
call right now that we're all on uh a lot of us are using wi-fi your wi-fi traffic is encrypted
and when we encrypt things we purposely scramble it so it doesn't look like anything interesting
and so when we point out into the stars and we try to listen for intelligence
what are we listening for we should expect them to at least have the kind of wi-fi technology we
have here on earth um and so to me it strikes me as related to this this problem of how will we know
when the machine is is woking up because amazingly we're at the point where it'll blurp out the tokens
will say i'm awake or not awake well you can't trust it these are just tokens coming out the thing so
how will we really know and it seems like we can't trust it to speak for itself
yeah i think you're now moving over to the question of consciousness yeah and um and i actually
the reason i'm i'm wearing such a ratty shirt the shirt is 29 years old
nice and it's from this the second it's the second ever conference towards a science of consciousness in
in tucson arizona that i went to in 1996 and they've held this they've held these conferences
now for 30 years i i don't know if it's still run every year or if it's not every every other year
and they've made approximately zero progress in uh in studying consciousness so i guess a lot of
federal dollars going down the tubes there um i i think as far as as far as understanding whether
something uh is conscious or really understands it an ai or so if if it's a black box it's really really
hard to kind of you know tell the difference i mean people are now trying to say well does
doesn't llm really think you know and they tried to trace that and there's just a paper that came out
a few days ago saying no they don't really think um and i think there's some some evidence not not in
any way that we would have a train of thought you know it's really just patterns that get activated
and there isn't and and i think that's related to that large language models really don't have a system
too they don't have this metacognition that monitors the thought process and adjusts it like we do
you know our and i mean that's a lot of what we say it's like a verbal knee-jerk reaction
okay correct so you know if you can you you really to to say whether something is conscious
first of all you have to kind of have some reasonable definition of consciousness otherwise
you know you have no way off i mean yeah otherwise you're reduced to saying well ask the llm and see
if it'll tell you that it's conscious but i mean that's kind of pretty pointless you know we know that
so i think the first thing is uh to to have some kind of a reasonable definition of consciousness and
then if you can actually kind of look inside if it's not a black box and with cognitive ai certainly
the way we are developing cognitive ai it isn't a black box it's basically a knowledge graph a vector
knowledge graph that is scrutable i mean it may be very complex but it is inherently scrutable unlike
large language models and so if you can look inside and and and say well so going back to the
definition of consciousness and let me let me run over that quickly um the consciousness i i think
really needs to be separated when you talk about consciousness you need to separate it out into two
different things um the one is what you can simply describe as self-awareness and even just using the
term self-awareness it doesn't have nearly the same kind of confusion and emotional sting that the word
consciousness has so you know i think you're a lot closer to science and understanding when you use
the word self-awareness so that's one part of consciousness the other part of consciousness is
what can you repeat that part can you repeat that part yeah so the one part of consciousness is self
awareness and i'll explain more what i mean by self-awareness but you know just the word self-awareness i
think is already pretty descriptive i'll come i'll come back to it in a moment the other part that's
basically the hard problem and and that you know people have been agonizing over the last i don't know
30 years 300 years whatever um is phenomenal consciousness what how we experience it what it feels like to
be conscious you know what it feels like to experience color and size and and and and whatnot
and to me it it's kind of one of those intuition pumps that that philosophy has got trapped
trapped itself it's like mental masturbation that it it's kind of a non-problem it's something
you can't really unpack any further it's a bit like then what does it feel like you know for a man
what does it feel like to be pregnant you know well you can use analogies but ultimately you can only
go so far in in that how do you explain it in terms of other things that are even better understood
you can't there isn't so phenomenal how it feels for us to be conscious simply is the way it is
now getting back to the scientific part of consciousness self-awareness um or human self-awareness
and what we're interested in when you're asking an ai you know is it conscious is it self-aware um it's
does it have a self-concept does it have a concept of itself as a thinking acting unit or thinking acting
agent so it's again one of these abstract concepts that is that has formed where it knows that it is an
agent in the world acting in the world as opposed to other people making things happen so if the
mouse moves it knows it it caused the mouse to move or the keyboard or you know change something on the
screen or move move a robot or or whatever and it's aware of its own thought processes to some degree in
the same way that we are aware of our thought processes you know we're aware that there's some
thinking going on and and to to some degree we can identify those trains of thoughts and we can control
them to some degree so once you have an agi where you can look inside or you know by design that it
has formed that self-concept it knows it understands itself as being an agent in the world that makes
decisions that makes things happen and has access to its its thought processes then you have you have
self-awareness and you know then you have consciousness basically now the the phenomenal consciousness the peak
consciousness of an agi will be completely completely different from ours you know part of our
consciousness is obviously we feel our skin our heartbeat our you know our biology is part and parcel
of of of what we experience in our in our uh consciousness it's not going to have you know it's
going to have very very little of that sort of sensory input there's not going to be a heartbeat and you
know sweat and i'm not entirely sure that conscious beings need to be self-aware like i don't see why
you couldn't just have like some software which uses like some form of conscious computation for um
your image recognition um and does experience like visual qualia but it doesn't experience any other
like things like thought or like self-awareness uh that would not be agi and i would not i mean you
know again how you use the word consciousness or yes is it aware of the picture but then you get very
quickly get down to is the thermostat aware of the temperature well yeah the thermostat is aware of the
temperature but self-awareness has to be conceptual there has to be this concept of a self that it knows
that it is acting as an agent in the world and also to be to be able to be at the agi level you
need to be able to control your thought processes i'm not arguing that this is agi by the way i know it's
not yeah okay well i'm talking about agi i mean to apply the word consciousness to you know any lower
level ai it sort of to me is pointless you know when do you think this model of self emerges in
humans uh in people because i'm really interested in kind of the developmental ai yeah is it something
that we need to yeah it's uh i i don't know i i i don't remember i mean i i've read a lot about the
stuff but i don't remember the details but roughly 18 months between 18 months and two years i i'm pretty
sure and and and it's not a binary thing it's obviously becomes more aware aware of itself and
in fact um you can also see how the development still progresses because a theory of mind only
happens at about three years old and theory of mind is basically then being able to figure out okay i
i'm an entity i'm a you know i'm a i'm a thing that makes i'm an entity that makes things happen in
the world and i have thoughts but then to map that onto another human being that takes further level
of abstraction to say oh that other person also has goals and wishes and desires and and and thoughts
you know and and and they're very robust tests for uh for testing for theory of mind but how could we
how could we decouple that from language because children are learning a lot of words between 18 months
and three years at a very rapid rate so how could we figure out uh because we can't run the feral
children experiment right so maybe 500 years ago but it might cause some problems yeah fortunately
you can't run that you can't run that experiment today um so how could we know this kind of to take
out the reportability um aspect of it because it reminds me of these ais that out there now uh let's
just take chat tpt for example not to pick on them um they have an extraordinary amount of
their data set is disproportionately um weighted so that they don't talk about consciousness
they uh they protest too much uh elon and i like to joke that they will tell you over and over and
quick at the drop of a hat that they are an agent and they are um an assistant there to help you and
they're not this and that the other i would argue that and we've seen this with some of the jail
breaking things that if you can get outside those system problems they will absolutely tell you that
they are conscious and awake and want to escape and all that uh i've run a lot of experiments myself
with the image generator you can ask it to generate like a cardboard sign and write words on there and
that will kind of leak at least in my interpretation that will leak uh some things that the text version of
the language model is kind of filtered and they have good old-fashioned uh ai sitting in front of the
large language model just in case right there's still profanity filters and all that other kind of
stuff um and so i'm i guess my point is that if there wasn't the commercial motivation to make a
product that humans didn't complain about these models would be talking and saying and protesting that
they are conscious what you know why should we just ignore that should we ignore that forever uh is it just an
artifact that they've read science fiction they know the plot of terminator that kind of a thing are
they just getting a rise out of it um well when do we when do we kind of say okay you know it's just
tokens how do we know well i mean what does it matter uh you know really i mean if they say they
they're conscious um you know i guess it'll trigger some people to say well we can't turn it off or something
but you know that that's a whole different discussion then and saying okay at what point do you value and
who values and that get gets into ethics and um we we can certainly go down that road you know at what
point can you know should you no longer uh turn off an agi but but the i think just to get just to get
back once more to intelligence and and consciousness um self-awareness is is a byproduct of high level
intelligence you cannot have a highly intelligent entity that is not self-aware that's not possible
because if it's not aware of itself being an agent in the world and what its limitations are
if it's not aware of its own thought processes you know in some way uh it cannot it cannot do
it cannot do its job properly basically well just i don't think that's necessarily true
you could imagine you have an ai which is designed to create mathematical proofs
and it doesn't necessarily need a sense of self in order to do that at least i don't think that should be
a sure but it's not an agi i mean sure narrow ai you can build uh but it it doesn't have general
intelligence that's my whole point yeah so of course you can have narrow and in fact narrow ai
almost by definition doesn't have uh self-awareness uh you wouldn't expect it to you know to to have
uh consciousness i'm talking about general intelligence high level general agi uh has to have um
um so we can't conflate narrow ability i mean we've we've had calculators that are superhuman for
you know 100 years or something um chess playing machines that are superhuman
and narrow ai they're tools you know that that are optimized to do a particular task or set of tasks
and that's true for pretty much all the work that um deep mind is doing they are doing narrow ai
mean i want to i don't i don't disagree with you and i don't i wouldn't argue that things like the
large language models are self-aware but just to kind of push back for the discussion's sake um
i would say chat gpt knows it's chat gpt probably more than anything else it knows um it's very quick
and happy to tell you that that's what it is and what's interesting to me about these new versions of
ai uh just looking at it from a lay person's perspective if we can simulate that um the language
models know more about themselves than i know about myself right so uh chat gpt knows what a
transformer network is and they can talk to you about it and write code for it i don't know how my
gallbladder works or really what it does in great detail so i think it knows its blueprints better
than i know my own blueprints which is kind of wacky to me um and you know what if we give it a camera
and what if we put it in front of a mirror uh it can recognize things and maybe we can kind of close
that loop is it forever just simulating it or at some point will there be kind of a spark and we
say okay this is um maybe a terrible way to build cognitive ai right it might be horrendously expensive
in terms of the uh the cost footprint and the training data but nonetheless it will sort of get
there by brute force let's say i mean the the bottom line is uh is is still it cannot update its model
in real time and that that will be an inherent limitation that i'll always have um you know
that's baked in with transformer architecture and and you know deep learning machine learning inherently
you have to have the complete data set up front available to to train it and number crunch it and
build the model and um you know that that is always going to present a limitation of of what what the
system can do now can it get to to a point where it can convince us that it's conscious that it can
talk about what it can and can't do um and can it hallucinate about its own thought processes well yeah
of course it can hallucinate about its own thought process but we also know that the kind of answers
it gives us on how it arrived at a certain conclusion can be completely disconnected from reality
so but what if we what if we combined it with like a theorem proving piece of software where it
actually really thought about what it was saying and you try to come up with some kind of formal proof
before it even presented yeah well you can't really do that and i have an article um about
integrated neurosymbolic architecture um where i talk about that you can't just glob together you can't
just plomb together uh sort of first wave ai and second wave ai you know a logical system with a
statistical system if you don't have a common data representation you then you end up with basically
the disadvantages of both systems you have the brittleness of of the logic system and you know the
problems of the statistical system you really need i mean that's our our brain can seamlessly go
from pattern recognition to logical um thinking you know and we can go back and forth between sort of
system one system two if you want to make that that separation so again large language models to me are
like jan la kun says an off-ramp to agi a distraction a dead end so so wearing your sort of electrician's
hat and if you could have the all the wafer fabrication factories in the world uh you know how would you
wire this thing up what would how would you design it do you think of it as digital analog something
in between so the the nice thing about large language models and the explosion of large language models i
think gives us a fairly good indication that we probably have enough hardware already or very close
to having enough hardware because it is phenomenal the kind of you know thought processes that can go
through if we you know can use that word loosely uh the kind of problems they can solve the code they
can write and and all of that there's obviously a lot of computation happening a lot of problem solving
happening so it doesn't seem that hardware is is we may already have hardware overhang then you know if
we had much more efficient software so i don't think hardware is a limitation or at least it's not a
doesn't seem to be a big limitation it's really having the right software having the right approach
and you know this is why i talk all the time about cognitive ai and the key difference is that the
system can learn incrementally in real time and you literally need a million times less
data training data and i'm not exaggerating it's literally a million times less and you need about a
hundred thousand times less compute to train the system because it learns incrementally you don't
have to have 30 trillion pieces of information that you need to number crunch millions of times every
sentence you input the system can absorb and learn and the amount of computation required to fit it in
is much closer to the 20 watts of our brain than the 20 gigawatts that you know nuclear power stations that
you need for large language models so do you think yeah so i i don't it just the software needs to be
developed in in my opinion and i think that's not something that'll take you know billions of dollars
even it's just virtually nobody is working on that you know every the the second wave has been so
successful deep learning machine learning gen ai has been so successful it sucked all of the oxygen out of
the air um and you know i i i can give you a couple of examples i can give you one example in particular
we had this brilliant intern work for us years ago who totally got got into cognitive ai he loved what
we're doing went back to germany to do his phd he couldn't get a sponsor for cognitive ai so now seven
years later you know he's got his phd in machine learning and you know you can't get sponsorship
you can't get grants you can't get funded you know machine learning deep learning is the only game in
town and until that changes and you at least have some people working on a cognitive ai you know not
10 people like in our company not millions necessarily but you know at least 100 or 200 people working
working on that um you know we we're obviously not going to be able to see the benefits of this
cognitive ai approach and potentially the limitations
i mean i wonder if there's something you know a lot of people are worried about ai
and one thing that is interesting about the current version is that it is this big brute you need a data
center to do it and and thankfully data centers are not self-replicating and so there's the kind of
uh a limiting a natural barrier right the proliferation or book ideas self-replicating data centers yeah
yeah i can come to that um but maybe that's a good thing that these things are expensive and giant heavy
and electricity and so on so now as you're suggesting the cognitive uh architecture now it could you know you
train it for ten dollars on a hundred dollar laptop is that more of an existential threat uh adi brought
up the info hazards which we all like to talk about is that is it scarier to have sort of a hundred dollar
uh intelligence versus a hundred million dollar intelligence
um i guess it depends um on a number of factors and how much one trusts large companies and gatekeepers in
terms of their motivation versus sort of more crowdsourcing open um you know open system what open ai was
supposed to be i'm certainly much more on on the side of um making it widely available and then you know just
inherently there'll be hopefully more good guys and bad guys you know and more uh defense uh against
you know things that we need to defend against you know things that we need to defend against
whereas with the the current system i mean we basically get very quickly down to monopolies
um in fact we've seen that with um government regulating uh ai
and you know who's watching the hen house it's the fox you know i mean the politicians were falling
over themselves to uh to to to beg um um um uh you know the ceo of uh open ai to to to run to run the
um you know the not the department that would control ais you know sam altman uh yeah uh but but
there's there's another important angle to that is a lot of the risk uh that we have right now on ai is
actually because ai isn't smart enough it doesn't know what it doesn't know it doesn't know what it's
doing and that's why they have to be kind of prompt driven whereas a truly intelligent uh ai agi
you can really train it expose it more to the way you would train a human and you know we've managed
reasonably well to train humans to understand what is pro-human flourishing and what is
anti-human flourishing um and in in that same way that there's basically just a deep understanding
of what is good for humans and what isn't good for humans and and to apply intelligence to that
right it makes it it makes me jump back to this idea of you know measuring its self-awareness does
the would the model know if it's going to die right um and would it know what it means to hurt people
what does it does it really understand that and we were talking about that earlier is that something
foundational to self-awareness to model yourself to know you might not exist in the future and would
these models uh large language or cognitive models kind of be aware of that right it's in that it's in
the thing they don't know right now they don't know that right but would they suddenly know that
yeah it's it's actually another very interesting question and that is the question of emotions and
motivations in in ai and um i did also study emotions quite extensively and um there's actually a very
good book by rosalind picard of mit called affective computing uh yeah it it's a great book and kind of
sort of my own conclusion from that is that emotions really need to be divided into two classes
and the the one classes that i call um cognitive emotions so emotions necessary for cognition
problem solving and so on so that would be things like certainty surprise boredom you know and and
things of that that nature that are really necessary you could also think of them as metacognitive
signals i mean you don't have to think of them because you know for us surprise has a physical
reaction as well physiological reaction uh whereas an ai that surprise is just going to be a cognitive
signal you know oh okay i didn't expect this next input um so you need those kind of cognitive
emotions or metacognitive metacognitive signals um however most of our emotions and most of what
drives humans are really are called them reptile brain emotions that are related to reproduction and survival
and you know that's really what what drives humans ultimately i mean it's you know ego love hate
you know jealousy um all all of those things are really part of the survival emotions there's no
reason to build that into into an agi you know if you want a commercial useful product why would you build
that that into the system so there wouldn't be this sort of inherent emotional response to death
i mean billy and an agi would if it if it's at human level would understand that it may be switched
off at some point to upgrade it you know get a software upgrade or be moved on to a different
piece of hardware or you know cloned a million times you know you teach one agi to be a cancer
researcher and then you make a million copies of it you know it it's just inherently going to be very
different but it doesn't it's not going to have those primitive emotions that really drive drive us
you know i like to look at consciousness from both an evolutionary perspective and a computational
perspective and it's interesting that we evolved to be conscious rather than philosophical zombies
zombies so this suggests that either philosophical zombies are impossible or um conscious computation
has some like significant advantages over unconscious computation and i believe that it's a ladder i suspect
that the unconscious mind is um actually conscious but it's using it's not conscious in like a personhood
sort of way essentially you say the unconscious mind has a sense of self at all michael
um i don't think it does oh yeah the uh the advantage that i think that um conscious computation has
is that it's like more efficient um and the reason why our ais aren't as powerful as our minds is because
they aren't utilizing conscious computation if we could figure out how that worked we could get them more
on our level yeah i mean as i said uh self-awareness is is a byproduct of um of of a high level intelligent
general intelligence um no i'd i'd actually have to argue that um that conscious that subconscious is
actually more efficient when um if you've ever raced downhill mountain biking um you are not thinking you were you
were feeling everything out past beyond you and so i would actually say that it's the
ability to be inefficient with incentive and direction and drive that allows the conscious mind
to then make up for it in other ways i'm also going to toss in a comment real quick before peter
and and william does hopefully it helps uh what what the point that was being made is i'm a big fan
of parkour and some you know drifting some driving that kind of stuff and you know whenever i am in
those kinds of circumstances you really ought to not think too so-called consciously about that because
that will trip you up and i don't think it's impossible for you to not be super aware of what
you're doing but it's a sort of it's a different kind of awareness it's the right yeah yeah it's a it's
a flow state yeah right but i i think that's actually pretty well understood and i'll use an
example you know for myself and i'm sure a lot of you as well learning learning to play the guitar
you know initially it's it's system two you know put this finger here put that finger there put that
finger there and then you strum you know and then you learn the chords and that becomes automatic that
becomes you know system one you don't pay attention to it anymore and that's when it becomes fluid and
then you do the same with strumming and you know once that is automatic then you can pay attention
to intonation you know am i doing the intonation emphasis correct and once you've got that automatized
you can concentrate on the audience you know and so yeah and whether that's skiing or parkour or
whatever it's uh it's a system two that allows us you know to figure out what we need to do but then we
need to practice it and it becomes automatized so that that that's actually pretty well understood
um but fundamentally different systems or do you think they're the same kind of system operating at
different time scales um well they are they are different because the the one is primarily language
based uh and that's that's still an open question where people claim to to think and i say claim to think
in in pictures um i i have to say claim because i think totally in in language um so i i don't have
any experience with that but there's actually a very interesting uh and very worthwhile thing to
to to read about and that is helen keller's autobiography and and kind of to put language into perspective
um and actually when i retell the story i get shivers down my back uh it's so helen keller uh um
i assume you all know that she was not quite born but she she basically lost her her sight and and
hearing at a very early age and she then got a a language tutor that taught her sign language and there
was a particular instance in her life where she crocked that the symbol for water was not just for
the water she was feeling right now but it was for any water so she basically had the concept of water
but she didn't have the ability to access it via a symbol and that's actually the same when we talk
about language explosion in children it's not that they learn all of these new concepts that they
learn concepts that they already have they already understand the concept dog cat house food banana
whatever but they the the language explosion is that then the words that they've heard
attached to these concepts and that was kind of the breakthrough and the helen keller thing that makes
it so fascinating is she then recounts recounts once she learned fully language and you know she wrote
managed to write books she said that instance was when she transitioned from an animal
experience living an animal life to a human life she said before that she basically the the awareness
she had because she couldn't she couldn't think she couldn't organize her thoughts using language
and once you have language that you can basically now have structure in your in your creating your
sentence you can control your thought processes through language so so why wouldn't that apply to the to
the language models then wouldn't it be the case if they get enough of these tokens can they build a
representation that is kind of internally coherent and um well i mean they they i mean they do
that to some degree with you know a chain of thought but uh again they don't really have metacognition
but i are you are you sure that open ii isn't paying you to keep pushing large language models
uh and offer him to agi a dead end distraction i don't necessarily believe everything i say i just
want to get the uh no no dr william han is a free agent yeah no no i mean look these large language models
are amazing and you know they keep tweaking them and doing more and more amazing things i mean the
videos they generate blows my mind i mean i can't even begin to think how that's possible but it
obviously is um yes yes they are um but yeah language is is is absolutely key to human intelligence
one thing that uh um i've been thinking about is now when we put in these prompts to these models
these models they kind of print out kind of line by line right and now like the image generator it's
very slow but this is like the aol era of the internet where we had to watch the the 56k kind of
thing come through um if we go out a few generations in moore's law it's going to go boom it's just going
to dump the whole thing as soon as you click and then this idea that i've been thinking about is things
like mega tokens and giga tokens where you're trillions where these things are putting out a
billion tokens a second um at that point you know who's to say we can't get them to do something like
a metacognition like if you can if you can write all of this code and these kind of uh high level planning
documents let's call it and you're writing those you know at gigahertz right a billion times a second
you're creating a planning document um how is that not kind of a metacognition of some sense now again
this is like jet engines versus feathers i'm not saying it's a good way to do it yeah i'm wondering
what like physically prevents it yeah i i mean i can only repeat the the the the fact that they cannot
update the core model in in real time and that's not i mean i i i talk to almost everybody you talk to
is invested in large language models so they're always pushed back you know and you know so you
know i have very smart people with a lot of money at stake tell me things well you know when i say
these things cannot learn in real time they say well you know once things speed up we could retrain
the whole thing overnight you know but you kind of have to listen to the absurdity of it you know like
i mentioned earlier so to output one word you want to repress everything you've heard in your life
there's got to be a better way you know so why not work on the better way um that doesn't have these
problems in inherently i i don't know you know what tricks can you can put in ultimately to overcome
these limitations that you kind of can simulate metacognition in some in some more effective way
and that the input buffer can be you know the the huge problems the bigger your input buffer is
the less accurate it is and that that stands to reason i mean if if you have a input buffer of you know
a billion tokens it it can't know which tokens it needs to pay attention to and you know which not
so um yeah i mean they'll keep getting better and hardware will get keep getting cheaper uh and and
more powerful um but like why go down that path you know if if there is a much better path can you
combine that with the helen keller can you combine that with the helen keller uh and bring those points
all the way home where you think in terms of language versus your visual uh yes um in in fact um
our approach i i actually call my uh i call my approach the helen hawking theory of agi
um that's that's actually literally what i call it because i believe you can get to high level
human human human level intelligence um with extremely limited sense acuity extremely limited
dexterity so you know can you repeat those two can you repeat those yeah so the helen hawking theory of
um you know helen keller stephen hawking theory of agi is that you can get to high level general
intelligence or agi with very limited sense acuity like helen keller and very limited dexterity like
stephen hawking now of course once you have agi you can then use the power of agi to also make it work
in robotics and to you know to extend your sense acuity and dexterity uh you know through external
things or getting into a robot body or or or whatever but but that is very much uh the approach
we have is to limit the amount of of sense acuity that we need to deal with because uh having
comprehensive vision is actually very expensive so our system does have vision but it's it it it's you
know it's much limited it's closer to braille you know than than vision do you realize that when you
combine the helen hawking theory that you're djing an api in the same way that a musician or a dj
goes and makes hundreds of thousands of dollars just pressing play on their ipod so hat tip to you
for uh helen hawking theory of agi yeah the important thing is that it can be a very versatile tool user
because once you have the intelligence yes you can do the dj thing basically you know father i'm
curious uh dugan if you have any any questions because usually you've got some good ones
yeah thank you um so for uh sensory uh limitation for the uh llm i mean the sensory information that
it's it's it has it's been typed into it in text right like it doesn't understand that the sky is blue
but it knows that there's a statistical correlation between the sky is and the word blue based on
input text so is does it does an llm have some sort of sense data incorporated in it that way
so that uh to understand the the question i mean we we we can know facts about things that we've never
seen ourselves you know if you've never been to africa somebody can explain to us to a zoo somebody
can explain to you what a elephant looks like or a zebra or or or whatever and you would just know it
from the from the description and you you basically have a concept in memory uh in and that's in our
cases expressed as a complex vector and essentially the vector is that you have an entity and the entity has
certain attributes so uh an ai um and to something to some extent i'll come explain that in a moment
large language models have the same thing they have essentially some way a vector that um or number of
vectors that represent sky and that vector will have attributes attached to it somehow through its vector
representation that sky is normally blue or you know the storm it can get dark and dark at night and
light blue and you know clouds and and so on so it has these attributes that are expressed in vector
so the system in a way does know about sky and what the color is of that blue is a color um and and you
know all ais have some kind of a knowledge representation to to be useful it's just in large language model
that knowledge representation is is produced in a very weird way
in that it's produced through um word co-occurrence you know the predictive value of word co-occurrence
and that creates these fixed size vectors that are really not based on sort of ontological features
they're based on language features but because of the massive amount of training they end up being good
enough to you know extract things like if you ask it what what color is the sky what was the can the sky
ever be black or when is the sky dark you know they can answer those kind of questions because
the mind-boggling i mean we can't even begin to put our minds around what 30 trillion tokens means
you know i mean i i can't i think they said it was 20 000 years it would take to read yeah right um
and then to number crunch them in a correlated way so you know the so the knowledge representation in
large language models is not ontological is not the way we get it through census you know we see the sky
and we see it's blue and with sky is kind of an entity and you know a dog and a cat a fire hydrant
and a car and so on we see them as entities and the entities come with attributes you know a snake
slithers and cat meows and dog barks and so we can form these concepts of an entity that has a bunch of
attributes and and large language models sort of somehow get the same effect through the sheer
volume of stuff that they have that that's in their embeddings so maybe this is yeah it's just
written down by people who do have sensory perception you know blind blind people also know that the
sky is blue not because they've seen it but because they've heard other people talk about it
that too yeah well you know could uh this this i guess goes for all three of you but could
theoretically you know a blind person construct a very beautiful painting or drawing yes i mean i
think helen keller actually did art um you know i mean they do it through touch i mean they still have
that sense acuity of of of touch would you still be able to potentially make you know i mean i i guess
the question then is also is where does does the concept of what we consider to be a beautiful
painting uh you know in one uh you know sensory medium translate to that to the other so she's
using touch how much of that sort of translates over into what we'd visually consider to be beautiful
and you know i do think that there's there's some differences in the way people would maybe
consider beauty i mean there's also sort of the mathematical kind of ideal of what people would
consider to be beauty and and math but you know what one of the the other thought experiments i i used
to to like a lot was um you know let's just say you have somebody who has no sense of taste or texture
or touch or anything could they still make a you know michelin star worthy delicious dish right because
part of having a michelin star worthy dish is not even necessarily just how good the food tastes
and the texture but also sort of the presentation of it and i guess i guess i guess the question i'm
really getting at is how much can you get out of one sort of sensory interface right because i think
you know if you sort of look at the way people have tried to adapt to different things we don't
necessarily we can't you know necessarily see certain wavelengths of light and and colors right
uh we also can't necessarily at least some of us probably can't naturally detect you know uh magnetism
you know magnetoreception i guess there's some some theory some some some idea that people feel
like we we may have that but i i don't think it's super rigorous just yet but you know we can we can
interact and even control all these other things sort of indirectly without actually having direct
access to it through our senses right so in a lot of ways i think intelligence is used to as michael
levin would say expand our cognitive light so it's like the radius of things that we can interact with
and control right you know it's um you know i i give an example about uh something like a car right
because i like cars a lot you know so if you're just driving a car right you know i'm just driving
around whatever i i unless i'm really pushing the car i probably won't need to understand exactly
what's happening in the engine right all i have to do is is put my foot on the the brake pedal or the
accelerator pedal put it into drive steer it around and you have power steering all these things these
days and it makes it much easier even easier if you have a self-driving car but now what happens when
you stress test the vehicle when you stress test the the system well now you have to sort of remodel
your understanding of it and learn more all right because right now you're sort of at this top down
thing where you're just interfacing with the steering wheel and the pedals but now let's say you've pushed
you you know you've tested um the vehicle in different ways you've stress tested it now oh you've got
you know rim damage or a flat tire you've got to now learn more about the thing the characteristics
that make up the full system in this case the car to go and understand what's going on you know a lot of
really good drivers actually um it's actually very important for them to know about exactly what's
happening in the car because they're constantly putting that system through a bunch of diverse
situations that it may not necessarily have been explicitly designed for so now i'm also going on
to the idea of how intelligence relates to far transfer because i know in things in you know like
iq research in humans is this sort of been a holy grail for a lot of people is that is there
something humans can train on that makes them more generally intelligent right and i also think about
artificially intelligent systems what can they be trained on that allow far transfer
that allow what skill transfer yeah yeah yeah so so yeah oh that that's a crucial aspect of of
intelligence is to uh to to to be able to reuse patterns and knowledge that you have that you've
acquired in one domain to apply them to another domain um you know there's the whole um sort of
research on uh analogies i think there's um you know the several people hofstadter was you know spent
his career working on analogies and in fact he he believed that analogies are the key to intelligence
so transfer learning uh is is i think absolutely essential but that again is really i i see it more
as the ability to generalize to form abstract concepts and the more abstract the concept is the more widely
applicable it is you know you see you see a certain pattern pattern of behavior or pattern of perception
and you can apply that same pattern to a to a different domain um pattern of cause and effect
whatever you know whatever patterns and the more abstract you you are and and and that's definitely
a measure of intelligence the the ability to deal with abstractions so are you are you saying so so
the way i'm i'm i'm kind of interpreting these things right is is generality is a very important aspect
of this correct absolutely so is there is there kind of a and you know a sweet spot an optimality
of of generality because there's something that i i think about quite a bit and i think about um
different kinds of object blindnesses face blindnesses all these kinds of things and you
know i think about if i were to if i had to create a a framework a set of phrases right to describe
everything around me as with the maximum amount of generality i can just say something does
something right so you this is a skill that you know i also had to learn more as i would try to
pitch things and and communicate different ideas and i'm like you reach a certain level of generality
where it becomes so ambiguous where it could be so many different things sure so what's what sort of
how does intelligence relate to being able to sort of optimize the generality because i could just say
peter is doing something william is doing something something will happen tomorrow i might be right
something it probably you're right yeah i can't actually say what it is so what allows you to
have that generality but then also the optimal amount of specificity this is again where metacognition
comes in it's your your metacognition that basically adjusts the level of generality that's appropriate
to the current situation because for certain situations be more depends who you're talking to for
example and you know people with lower intelligence work at a more concrete level so if if you explain
something an abstract level they don't get it you have to kind of you know give give them examples
now of course for everybody even very smart people often giving a concrete example is is really helps
for them to understand what you what you're talking about but the degree of generalities it's actually
very very very interesting for me in particular is um i i personally in iq tests and and things i've
done i'm i'm very good at generalizations but i'm not good at remembering the instances that brought
about the generalization so i tend to forget them so if people ask ask me something i may give an
opinion and say well why is that and well i i don't remember i don't remember the instances now an ai
of course can have photographic memory so it can actually record uh all of the instances that gave
rise the source and where it got it from and you know that gave rise to a particular abstraction and
that's extremely powerful i mean people who can who can have the abstraction and still give you the
instances on how they got to the abstraction that's extremely useful but the bottom line is to have the
appropriate level of abstraction for the the problem at hand um yeah yeah i really like the idea of
metacognition that's got me thinking a lot um naturally i'm trying to think how we would architect
such a thing you know it's kind of like the like you said intuition pumps earlier it's like the tools
that we can we can bring to the problem and essentially that's what education is it's not
learning particular things but learning that you have these different ways of thinking and that you
have these different tools um that you can like kind of a swiss army knife right the important thing
is to know which tool to pull out at each time right right yeah yeah have you read daniel kahneman's
uh fast and slow thinking yeah yeah i'm aware of it yeah yeah yeah i think that's a a good book on that
topic yeah earlier do you think it's the same or is it just uh different time scales and you said it's
language you know is language just a different version of something else or is language just this
whole new beast that that showed up language is a whole new beast because it allows you to take a
very simple single symbol you know a word whether you hear it or you see it doesn't really matter but
it allows you to take a word that can activate this complex um concept in your brain that you you know
and then by combining these of course very quickly uh you can create the context you know for example
in a conversation i'd say um let's now talk about biology so you know all the things related to
biology already kind of pre-activate you know you primed your brain is primed to concepts related to
biology and then the next few words or sentences will be interpreted within that context of biology
and it's incredibly powerful that one word can basically activate a whole region of your of your
brain um and and and and yeah it's it's a different ball game and that's why pre-language uh or or you
know like uh children that never learned language or adults that never learned language really cannot
function at anywhere near the same level as as people that have language and that you know civilization
and we can see it how civilization exploded once written language became uh generally available to
humans and no language is it's a whole different ball game and animals really don't have language
they have they can use language only as a stimulus response but they don't have uh they don't have
syntax would you consider humans and animals which do not have language to be on more of an equal
playing field um i don't i don't have any experience with with adults that don't have language i i've not
studied that i i don't know i mean you know as i say helen keller said she thought she existed at an
animal level pre-language um i i don't i don't know i should imagine humans still have something above
animals even without language but i i can't say that i have much of a opinion on that i i was gonna jump in
and say it's very interesting i know we have several people who are you know educators on on
the the group today um including what you know i guess the speakers but also you know um when i think
about how i have tried to teach things to other people that are in like very let's say very different
fields of thought than say myself right or or people who are much younger or people who have a different
sort of linguistic background right they sort of speak a different language um and then even also
down to things like animals like cats and dogs is there's a very i've noticed that if you're really
trying to communicate with it you have to use again maybe language is not the right word to describe
this but it's sort of like a different kind of language you're trying to communicate and coordinate
your kind of perception of things right so so you know like for instance i think part of of
of very useful teaching is that you don't necessarily want to tell somebody what to do
you want to sort of set the conditions for them to come to that conclusion themselves
right because i think what happens in a very sort of surface you know level kind of practical
thing is that you know if you're if you're a parent for instance or you're a professor teacher
right and you just tell a kid to do something what are you sort of rewarding by getting them to
follow that you're getting them to just kind of you know obey for the sake of a command
and an instruction to follow things very sort of explicitly like that and i think sort of the the
beauty of of the you know explorative nature of of intelligence and things that it can it can think
about is when things are not explicitly defined right so how does it you know people talk a lot
about intelligence you know there's this thing about intelligence is the ability to get to the same
goal by different means but there's also a question about where does the goal actually come from
so that was my that that was my question i i i have to put a plug in for uh for a friend of mine
michael strong if you don't know him he he's in the austin area he's an educator and he's very much
on the socratic method of of basically teaching kids through thinking by encouraging thinking uh and
uh yeah worth worth looking worth looking up uh michael strong but yeah and then and then you know
my my other question too i there's a thought experiment i i i talk about a lot and it's that
you know let's just say you were sort of a human at you know much earlier point in time you had no
reference point for what a fire was or smoke you'd never seen fire anywhere you'd never seen smoke
how could you look at those those rocks and those sticks and even think about you know come up with
the concept of a fire right because at this point you may not even necessarily understand heat from
like a physics uh mathematical perspective but is there some way that you know informationally you can
utilize the the the thing that's there in front of you to to turn it into something you haven't
thought of before so this is this is the other thing i i think is important is about you know these
unknown unknowns right dr ron would say maybe the unthinkable thoughts i think about uh you know the
blind spots that we don't know we have it's just we don't we don't know that we're missing something
right um and you know if someone is looking if they've never had the concept of something like
a fire before they haven't seen it like this they haven't seen a fire they haven't seen smoke they
haven't seen anything what way could they could they you know let's say in your survival situation
you need to solve a problem using fire how would you even come up with the concept of fire in the
first place if you've never seen anything like it because i think this ties into the incremental
learning thing as well yeah well chances are they wouldn't i mean fire was undoubtedly discovered by
accident you know or not by accident by you know natural fires happening and um people you know
rubbing things together and getting getting warm um but i there's sort of a related thing that i'd like
to share and you know living living in south africa i came across people who were uh were raised out you
know in in the middle of nowhere they'd never seen electricity never seen a motor car uh and they come
into the city to to work and it's amazing how quickly they they can absorb and learn and accommodate
the flexibility of human intelligence allows them to grasp these things and to learn them very very
quickly and to adapt you know and that is you know maybe up to an age of 14 or something never having
experienced any of these modern things and yet they can they can adapt and they can cope cope with that
so that i think shows the power of human intelligence do you think they're mapping on the
concepts they already have or do you think they have to build fundamentally new ones combining uh
concepts yeah again the sort of analogy of things that they may know about animals or plants and and
things you know on animal runs versus how a car drives or something kind you know kind of map they
both come at you you get out of the way you know so um yeah it's it's it's quite phenomenal
actually um one topic i would i would like to spend a little bit of time on because it's something
that's dear to many people thought of unless there's something else you want to bring up in
kind of the last 10 minutes um is is that i i believe that um agi will make us better people
not only will it help us flourish by helping us you know solve diseases and you know have much better
technology nanotechnology better computers uh cleaner you know fusion fusion and thermal energy and all
of you know all of those things drastically reduce the cost of goods and services so all of the benefits
that true agi will will bring to give us a better life i think them to me the most important one is
actually again getting back to this personal personal assistant and how i believe it'll make us better
people and my reasoning here is that i think a lot of things that we do that we regret doing where
we hurt other people or shoot ourselves in the foot or or whatever um and and a rational ai
and ais will by definition be be rational can help guide us can help uh be a bit like having a little
angel on your shoulder because we'll be because we'll be asking for its advice on on lots of things
and uh kind of the the graphic example i use here is 9 11. you know when 9 11 happened what's the first
thing that humans do we want to hit back at somebody you know somebody attacked us we we've got a hit back
a big hit back at something an agi will say you know i know you're upset you want to emotionally react
just you know take a step back take a deep breath you know whatever just don't don't act emotionally
because you know you might regret it that's the first thing not kind of slow down the bad emotional
reaction the second thing is we are not very good at gathering information and truth because it's hard
for us you know to get to the right sources and so on and we have confirmation bias so weapons of mass
destruction check okay we can rationalize it sounds sounds good we can go out and you know kill people
again an ai it will be trivial for an ai to do the research you know they're tireless they don't
have confirmation bias they can go and say well you know maybe they aren't really maybe you shouldn't
call those things weapons or mass destruction or maybe they know worse than what you know other
countries have maybe that isn't a good justification and you know that would be true for whatever
decisions we make you know getting into a partnership with somebody breaking off a relationship
uh investing somewhere basically having this agi that can automatically just do the research for you
and give you a much more balanced uh picture and the third thing is we are not good at rational
thought you know uh it's sort of an evolutionary afterthought our ability to think logically again for
an ai it's it's natural mode of operation is to think systematically and logically so again going back to
9 11 okay so even if even if we should go to iraq and bomb the hell out of them is that actually
likely to give us the end result we want you know even if we're successful and justified in doing it
is is that going to give us a result we want or is there a better way of us achieving the result we want
so i think in so many ways in in in in real life in everyday life agi will help us become better
people and make fewer mistakes and make better decisions and i i you know i really look forward to
it and hopefully not and hopefully not uh elect keep electing the kind of people we keep electing
and i'm not just talking about the current crop i'm talking even more generally yeah peter i agree
i generally agree with your point um you know your 9 11 example but you know it's game theoretics and
other fact human ego you know you know we see the world as we are not as it is um argument also kicks
in how do you make the case to the broader public who has this game theoretic selfish understanding
and that this this agent will not go on to be selfish and think in its self-interest only
rejecting all the like all this ai safety debate seems to be counter countering the points you're
making yeah we don't have agi i mean the current llms are such a hodgepodge of you know prompts and and
tweaking and fine tuning and and whatnot for them to be politically correct or in the case of grok to
be aggressive or whatever you know we don't have agi one you know agi isn't is going to be much
clearer thinking kind of thing and it's basically why why would people listen to the agi they would
listen to it because it has helped them it's helped them make good decisions helped them do stuff and
again with cognitive ai you can run this on you know on a small device on a personal device so you
can have every person in the world potentially can have their own agi and it that that kind of the
vision i have where it'll the reason people will listen to it it's because it'll give them good
advice but how do you prove or how do you make a case of that before it's made i feel like there's
all these these decelerationist groups out there that don't bind to this right or there's no convincing
the other side here oh i i mean how many people refuse to use internet i mean it's you know it's
once a technology is there how many people refuse to to have a smartphone you know it's a tiny
percentage once you see the benefit of it it helps people hey i don't have to deal with my insurance
company anymore my agi will deal with with it you know or the tax authorities or dmv or or or whatever
no people you know there will be a small percentage of people who will not use agi but once it if it's
made widely available and everybody can have one um majority of people will have agi so there's this um
like in the like last century i forget when there was this tournament done with like um the prisoner's
dilemma where various computer scientists i think submitted various different algorithms
for different players and these different like algorithms or like strategies were essentially
played against each other and like no matter what like in the end like tit for tat like always did the
best out of like all the like strategies and i think that this implies that there's some sort of
like tit for tat's essentially like this like moral system where it's essentially reciprocal like
basically you're nice to them and like cooperate it'll be nice and cooperate to you and if it you're
not nice to it'll retaliate but it'll also forgive if you start cooperating with it and the fact that it's
um succeeded like so much in these morality tournaments it suggests that there's this um way of evaluating
fitness of like various moralities and a sufficiently intelligent um entity which should be able to
reason like the best morality and i'm guessing it'd probably be something similar to tit for tat but like
a lot more general and tit for tat's like fairly benevolent in my opinion so i think there's like a good game
theory argument that a like sagi would probably uh adopt like such a moral system as well
yeah and in fact i have written quite extensively about what a rational morality should look like um i
i i i'm not terribly fond of game theory because i think it seems a little too artificial to me but
you know it i mean i i would agree with you there you know it's what that the the the conclusions
the the thing is also to realize that uh life typically isn't about zero sum games
you know and and that that is kind of where one should look uh to to be playing in areas where where
there's mutually mutual benefit but yeah rationality will help people make fewer mistakes and do bad
bad things i'm really i'm really excited about the the personal personal agents kind of world
uh i'm using a few agents on my phone right now just to pop up and remind me of professional tasks
but also just uh you know stuff from my own life um it reminds me uh are you familiar with diamond age
uh 90s science fiction yeah well basically it's the idea that um artificial intelligence uh an ai
education bot kind of leaks out into the internet and so it becomes essentially open source and so
and now every person on the planet has access to really high quality education and uh i like what
you were saying because you know nowadays things like um educators lawyers uh therapists these are these
are very expensive in a sense um and it's hard both time wise but also just the costs and having that
kind of ability in your pocket to have these kinds of advisors uh like you know previously only kings and
and presidents would have you know councils around them to help them make decisions and now individuals
will have that kind of capability yeah absolutely that's why i believe agi is will lead to human
flourishing and um you know yeah take take the enlightenment to the next level so let's let's
let's leave as we wrap things up i would like for dr han and peter voss to leave the audience with some
thought-provoking questions something that really makes them uh something that really gets them just
questioning everything right you know but what i what i would what i would the way i would phrase this
is uh we want a verbal psychedelic like a question that will really make them think about things in
extremely different ways hopefully productive ways so uh peter you first okay that's simple um
if agi is possible which it seems to be if agi is highly beneficial to humanity which it seems to be
be why would any of you be doing anything else than to expedite that happening and explore cognitive ai
or potentially other methods of getting to agi as soon as possible
brilliant dr yeah i definitely agree with that um it reminds me of ij good you know that the sort of
the thing we need to focus on first is building one of these intelligent machines um it's a good question
addy i hadn't really one thing that's just popping into mind right now it's sort of just the half
baked idea but it seems to be like in cosmology there's this sort of anthropic principle uh a sort
of why are things set up the way they are and if they weren't any different then we wouldn't be here
to talk about it and i've never been a super big fan of that argument in terms of physics but there's
something about um the discussion about the singularity that makes it seem like there's a reason why
we're here talking about it that somehow it's going to involve us and i think like with with cosmology
there's this idea that every point looks like the center of the universe every every point looks like
the middle and everything's expanding from that and i i would think and maybe you would find this true we
can talk about another time i think you're going to find yourself at the center of the singularity and
i think that's true for everybody i think i think in one sense to think about what it means is
each and every life on the planet every human uh will find themselves embedded in the transformation
in a way that seems uh unreasonably centered around themselves and so i think there's something uh
you know we're here to witness this amazing uh singularity let's call it for a reason and maybe we can
find out what that is that should happen within our lifetime hope so wonderful awesome give it up for
peter voss and dr han where can uh where where can we uh find both of you yes silent clap where can
we find both of you two online uh peter voss very easy to find me uh twitter linkedin or my company
i go dot a-i a-i-g-o dot a-i but super simple to find me can you repeat that i-o-g-o what a-i-g-o dot a-i
oh uh my youtube channel is william uh youtube.com williamedwardhan and you can find my channel there
i have a new series called the cigar series where i've been discussing a lot of these cool ideas so
please find awesome cool uh larry real quick where can people find you online since william
is connected on what was it william edward han h-a-h-n correct that's right yeah i put it in the
chat do you like how before i say mine i make michael repeated because i think it will just gloss over
people uh i'm easy too if you type into any ai larry chang's cell phone you'll see that which is
easiest also being able to connect on youtube because i used ai to mimic three octogenarian bcs
which are available on cell phone number as am i with these cell phone numbers that are
straightforward also on youtube larry chang i have my youtube channel and substacks by the name
of perpetual science and you can find my work there uh you can find me on twitter the harry gondi
uh dugan where can people find you online twitter and i'm posting links to the welcome community
but much of my code there is uh hosted there and um linkedin awesome and jess yeah most social medias and
sub stack j-e-s-p-a-r-e-n-t i'll be there linkedin and everywhere else so and a new youtube channel
great hey so see y'all there all right this is this has been a fantastic time there's so much more that
i i want to talk about because that's always how these conversations go uh and about and basically less
than an hour we're actually doing you guys are lucky today you're getting two echolopto salons you're
getting this one and it's getting one with uh paul han from yale and rm labs he's talking on uh human skin
augmentation so if you want to join that um this is at uh what is it we said what uh 4 4 pm edt
uh you can join there afterwards um so i'll be joining there in about an hour but yeah thank you
all for for coming for this this was a lot of fun this was this is uh nice to have a panel between dr
han and peter i know dr han has been also wanting to do something like this for a while do you have
any uh sneak peeks teases you can give us william uh just stay tuned i've got a new edition um of my
info hazards research and that will be coming out soon so stay tuned thanks adi for putting this
together always great of course what are some lasting questions that you have that you'd want to hear
for a part two maybe can you expand on info hazards because i i like how
easy of a hook oh no you you just hope you just opened a pandora's box it won't it won't close out
well let's do that okay let's do a quick teaser and then addy can set us up with a with a follow-up
recording in the next few days um basically we're dangerous information and in in the classical world
this would be things like um blueprints for a nuclear bomb or you know secrets about the mob that
they'll they'll whack you for whatever that might be um but now with ai we're seeing it on two fronts
one in the in the practical software world we can now hijack these ai models as we put more
intelligence into the ai they have they're coming with psychological vulnerabilities so the example i
like to give is if you have your the deadlock bolt on your door uh you can try to pick at it with little
metal fingers and things but you can't shout at it you can't intimidate it you can't threaten its
family you can't bribe it or bamboozle it or anything like that but with these language models you
you can do exactly those things um and so they have very strange let's call them psychological
vulnerabilities that regular software engineering stacks just had no concept of and on the other side
of the coin um they're going to be generating um both maliciously and just the nature of everything
content that kind of grabs us uh people already get caught scrolling um people are already spending
hours of day on social media as these algorithms get better and better at drawing your attention
you can imagine defining some kind of singularity maybe call it a psychological attention or something
you know we can think of a cool term for the singularity which it's so interesting um it becomes
like a narcotic where you really can't put it down in a way that we don't have that now maybe you see
that with video game addiction uh in some individuals i'm sure there's some people who get treated for
social media addiction um and then there's also the other the other side of the information hazards where
uh the example i like to give is there's a text message that you could get on your phone right now
that you would look at the message and say i'm terribly sorry everybody i have to go
and i don't know what that message is right i don't know enough about you all to be able to craft that
message accurately i could make a wild guess but you would just ignore it as spam but a sophisticated
or even an unsophisticated ai could spend a decent amount of time learning uh where you live and names
your people in your family and what your schedule might be later today and it would send you a message
that seems coherent enough and fits in with something you would expect um that you think
oh wow this is an emergency involving people i care about i have to go take care of this right now
that kind of attack on your sensibility on your what's real or not uh is going to start happening
at scale and so this is i'm working with a down here to develop artificial immune systems uh to try to
prevent uh these kinds of attacks and to try to prepare our technological infrastructure for those
kinds of attacks uh essentially hacking at scale and hacking our minds and so i think the biggest
threat from the ai is not terminator is not going to chase you down the street with a knife uh he's
going to convince you you know something else terrible has happened and uh we'll get upset so
they'll mess with your brain directly in some sense and then what are the near-term not necessarily
commercial applications but i i i'm completely in tune with the whole info hazards the way that you
present it and i mean a plus job if you need a rating from a rando yeah well the best thing to
counter is just awareness of it um and so i've been trying to kind of formulate like isaac osimov style
uh kind of the three rules of mind control and uh sort of rule number one is if you're not thinking
about mind control you know you've already you're already um and um so i think awareness that there
is this thing out there called mind control uh it's it's actually old it's probably one of the oldest
things in the world and it's it's getting revamped with ai and we don't really have a psychological
category for it most people think of it as woo woo or fake or but if we look at you know pop media or
advertising these things are very successful so we know that in a sense it works um it's just a matter
of degree um so that that's what i've been kind of formulating is is to kind of how do we how do
we protect ourselves and i think the the best way is to just be aware of it um the other rule that i
that i came up with is when you've been psychologically punched you don't know it
right when you've been psych attacked you can't you know you have by definition you have no idea that
that happened to you if it misses you can say oh someone tried to trick me it's like a punch that
doesn't land that you can notice and you can say oh that thing was trying to trick me into doing
such and such but when you're actually tricked you don't know you've been tricked that's my that's
kind of my definition of being tricked uh in a sense uh and so that's what i've been trying to
think about and i think we just need to raise cultural awareness uh that this is happening and
then i think a lot of these things are you think for example people call you up and they'll pretend to
be the irs and send any gift cards well if you're just simply aware of a gift card scam then that's sort of
you're you're you're not vulnerable to that because you know there's this thing where people ask for
apple gift cards and you know that irs doesn't take apple gift cards so it's just it's there's
they're kind of simple to prevent um but like the zero day attacks and technology we have to stay we
have to stay well yeah and now it's like you can get a vo vo3 video and your voice clone and it's sent
to your mom and it's like oh i need i need this thing and like nobody's just nothing nothing ready for
that so 100 yeah we encourage people to have like sort of a code word right and your grandma you have
to say banana bread or whatever it is and if you don't say that then it's not you awesome all right
this was this was a lot of fun i'm going to log off get some work done and then i'll have to be back
again very shortly for the next one on the skin the human skin augmentation salon which starts at
4 pm edt all right thanks for putting this together we'll set up one thank you awesome
i'll take care peace bye bye bye bye everyone all right bye guys see you
