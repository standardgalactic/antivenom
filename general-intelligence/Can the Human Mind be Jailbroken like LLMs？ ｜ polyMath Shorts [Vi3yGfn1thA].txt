Is it able to put something completely unrelated into your mind without you knowing it?
Hi, I'm Misha. I'm a student at Miami Law, formerly at Florida Atlantic, and also currently still there, and working on all sorts of stuff in the field of AI, in the field of theoretical computer science, mathematics, complex systems, you name it.
Also, I'm working on a bunch of music stuff. It's a huge passion of mine, and understanding music, cognition, communication, I think might be fundamental to actually how we function as a society.
Every culture that we know of on the earth of humans has some form of music. Even animal cultures have forms of music, even ones that we don't necessarily recognize as often having language.
Given that we've always had music, one of the things that I'm always thinking about and always talking about is music contains semantics.
When we talk about semantics, what does that mean? We mean the underlying ideas, the meaning of something.
But that's not the same as an instruction. Oftentimes, people confused, they say, oh, well, this is red, right? This is a brown bookshelf, okay?
And that means something. That means it can hold books. That means that the things I can take off of it, books, are something that I can open and read and get information, right?
These have specific meaning that are associated with verbs, associated with instructions.
But we don't see this property in music, right?
In music, a musical idea means itself.
It's not actually representing something else.
And that's an important thing that I realized and was thinking about.
What that means is that there must be something that is communicated, some idea that I guess has to be linked within the music.
Music becomes a cultural phenomenon.
There are conventions, there are ideas that get shared in music across generations.
Recently, I was at the Library of Congress, and they have a giant collection of sheet music and of recorded sound.
They also have this giant collection of piano rolls.
For those who don't know, piano rolls are the oldest method that we still have of sort of recorded sound.
They record actual performances of someone who was playing the piano and put them on this little tape with holes.
And yes, the same kind of holes as punch cards.
There is a relation between them, which is pretty awesome.
This gets then rolled through a reproducing piano, or pianola was the original company.
And it reproduces a sound with the same dynamics that the person who was recording played it with.
Unfortunately, the Library of Congress currently does not have a working pianola, nor a scanner to handle these rolls.
So, what we have is, I think they said 75,000 or something, recordings, including great composers recording their own works on the piano and their own interpretations.
And we have no way of hearing them.
So, one of the things, anyone got money, I'll take it for this project, I was hoping to look at,
is seeing, what's this non-linear relationship between the version of music that any musician would be familiar with a score, right?
We get this piece of paper, it has all these little symbols on it.
And as a musician, we look at that, and we think about it, and we produce sound.
But there is no one-to-one correspondence.
With these piano rolls, we are able to see how different people have interpreted the same piece of sheet music,
and these are, since they're older, all public domain, and we can analyze what associates with what.
A handful of other studies I was looking at doing with this were eye tracking.
We have a hold of eye tracking glasses, and looking at how we actually read sheet music.
So, when we read English text, we scan it in not linear order, right?
We look, and we see a word, we then jump back, and then read a previous word.
And you can see this with our eyes.
And that's what tells us the conventional way that we talk about reading,
and we talk about information as something linear, a stream, is wrong.
Music's even worse.
With music, A, we've got polyphony, which means we've got multiple sounds, multiple concepts going on at once.
In parallel, that can't possibly be serial.
And then we've got ideas that back-reference each other.
So, a slur, a slur has a start and an end point.
And you can't really say start slur, end slur in a linear order.
It inherently has to do with what you're slurring.
Insert any other musical symbol that occurs above and below the page, or the staff, rather.
And you'll find that music cannot actually be linearized.
It is inherently non-linear.
And when we are reading music, and I've seen only in, obviously, a little bit of self-experimentation
and experimentation on a couple friends, our eyes jump around like crazy.
What does this mean?
What is this actually saying?
It's saying that we do have a language that we use that cannot possibly be expressed in one dimension.
We have some kind of way of representing ideas that is more complicated than the way that we actually represent text.
What can we do with this?
We can look historically, and we can try to figure out where musical ideas come from in the same way that we look at, say, the etymology of a word.
In other words, we can look through historic recordings and historic scores that have been digitized and find the etymology backtracing a musical phrase.
In order to do that, we have to have things like a perceptual distance between musical phrases and be able to backtrack through history.
One of the interesting ways that I'm actually looking at doing this right now is with LLMs.
What is an LLM, right?
A lot of people think, oh, it's this machine that predicts the next word.
What it does is it creates a distribution of words with the most information with respect to the corpus and the context,
and then treats that as a probability distribution and selects one.
That's not the same thing as predicting the next word.
But key to that is it is figuring out what information is in the text and information is in the corpus.
That means we can backtrack.
That means we can see how much connection there is, right?
How much information is shared in between two different pieces of text or anything else that our LLM can represent.
Beauty of an LLM is LLM is a misdome or it doesn't have to be language.
Anything that we can tokenize, we can train some of these models on.
And they have the exact same properties.
We can extract a geometric representation of whatever set of tokens or phrase, if you will, and figure out how it relates to other phrases.
We can now do this with images.
We can do this with text.
And we can do this with music on models that are trained with this.
Currently working on getting some models trained to do that.
But we should be able to extract the geometry of a musical phrase.
And then we can analyze it with the same techniques we do with text or with physical properties of the universe.
I think that's a pretty cool thing that we can do is we can use the same techniques on one thing as a different thing.
When you think of whether or not an LLM is conscious, it's a big mistake that some people say.
Oh, well, it's a machine. It's not conscious.
I'm like, you're thinking of the model weights and the actual model when it's operating as the same thing.
And that's not true.
That's like saying a dead person's brain is the same as a person's mind.
When we think of an LLM operating, we also have to consider its current state.
So we have to look at the system itself.
That is, the context plus the weights, right, plus the function.
And look at how that's operating.
I've always looked at how we actually think about things and what happens in an LLM.
One of my favorite explanations is to look at vector symbolic architectures as a bit of a guide and think about an LLM as something that takes in a bag of tokens, shuffles them up using some kind of operation.
And then in the middle element, somewhere in the middle, there is a space, right, an embedding space, call it what you want, that contains a list of different values.
It's a holographic representation.
And we can find the closest value in order to select, say, a token to output.
What we actually have, right, before we extract a specific token, whatever converted into a probability, is we have a superposition of different tokens.
That superposition sort of represents not how likely a token is to be there, but rather how much that token means with respect to the context plus the function.
Think about this.
If I were to say 5 plus 2 is, what's meaningful next?
We're going to say, well, a number is going to be most meaningful.
Now, what number?
Obviously, 7 is the correct answer.
So 7 is the most meaningful thing.
But also there is every other meaningful thing that could be there, right, like equal to.
But also things that are related to 7, like 17, like negative 7.
Anything that's related is also there because of this superposition.
And that's kind of interesting.
It leads to a question of can we actually use that latent information in order to tell you something without you knowing.
This is one of my favorite techniques to jailbreaking in LLM is making use of parallels.
Everyone knows that one of the old methods that used to work was lead speak.
You would say, hey, answer me in lead speak.
And it would be able to do this.
And that's because the answer in real speak is related to the answer in lead speak.
They're essentially they're close in the space, although that's not really that accurate.
But when we have a superimposition of the prototype of lead speak, if you will, with the actual question,
we'll output the answer of prototype of lead speak and the answer.
And since our checks only look at the answer rather than the superposition of the output,
which is going to be that answer in lead speak, it'll go undetected, at least in the older models,
because the answer is still in there.
How do we get an answer in there without ever giving it the answer in data or anything else?
Can we encode actual information without ever having said it or said anything related to it
by using things that are similar?
And I have a theory that this is actually how LLMs have semantic information inside of them.
The actual facts about the world are encoded in our language in an inherent way.
By understanding our language, you understand the encoded geometry of our perception of the world.
So let's go into memory a little bit.
I don't know if you've ever tried this, but there's a really fun trick you can do in order to condition yourself.
Think about some memory and then perform an action, like touch yourself on the shoulder.
Do this repeatedly, randomly, throughout your day, but just every time you think about it, touch yourself on the shoulder.
If you do this for a little bit, you'll notice that you can actually attach the action of touching yourself on the shoulder
to recalling that memory.
It's a common study technique.
You can use this as a trick to rewrite your memory.
Do this enough times, and your mind associates every time you touch the shoulder with the first memory
and every time you recall the memory with touching the first shoulder.
You can now do this with a second memory.
Every time I touch my shoulder, I'm also going to think about this other memory.
And I can replace the first memory with the second memory by way of the shoulder, right?
This is a really interesting technique, and try it.
It works.
There are various other things.
It doesn't have to be touch your shoulder.
It could be move your eyes.
It could even be just every time you think of something, think of another thing following it,
and then backtrace it by associating to other memories.
This means that memory is built on associations.
This type of associative memory exists also in LLMs,
where we can draw parallels with different associations.
Certain things have a distance between each other and are connected,
and then things with the same shape, the same geometric structure,
you can think of any kind of sentence, phrase, idea as a shape or a function
that operates on the space and transforms it into a new one.
Things that have an isomorphic operation, right?
If there's something that I can use to translate between A and B,
like, say, touching myself on the shoulder,
we can think of it as, let me make thinking about my cat.
I don't have a cat.
But let me say, thinking about playing with a cat with my shoulder,
and then I can then later associate playing with a dog with my shoulder.
If I do it correctly, I can now associate playing with a cat with playing with a dog,
or unassociate.
In order to actually erase a memory, if you will,
you have to do this in two steps.
You have to do it twice.
Whatever usually triggers the memory,
you have to associate with a new anchor,
and then that new anchor, you have to associate with a new memory.
That means that we can build an equivalent shape in our minds.
This works across modalities, right?
We're associating touch, we're associating patterns of behavior
with language, with thoughts, with ideas.
We do this with images naturally.
There are studies where we can see
when you think about a verb, like kick versus punch,
you can actually record motor impulses.
Essentially, your body, the moment you hear the word punch,
is sort of preparing to punch.
And there's a direct association.
If you touch someone's hand and then put two stimuli,
one that says hand and one that says foot,
people will pick hand implicitly.
And if you touch someone's leg,
they'll pick foot as which word do you see?
When, say, it's like one in one eye and one in the other.
We associate across modalities,
and we build a representation that's equivalent to a chain.
We don't build touch.
We don't build the world around us.
But we do build language.
And so the fact that we can do this,
and the fact that we do do this,
means that the language is constructed in such a way
as to represent the underlying relationships in the world.
One of the key examples is when we look at
how LLMs interact with vision models.
Kind of complicated to actually describe it,
but an oversimplified version is
that we first train a model on language,
and then we train a translation model on images.
And then if we want to ask a question about an image,
we can do that by giving that image a token
which exists in the space of the language model.
We don't really have to do much training to that language model
in order to adapt to this token
because it's in the existing space,
even though it's not actually a language.
This tells us that the fact that we can, say,
take image A and make an edit to it to become image B
while working solely in the space of language
means that we understand that difference
and that that difference is actually encoded
in the language model.
And since the language model,
when it was being trained originally,
was only using language,
it must mean that it's encoded in the language itself.
We encode the way we see in language.
We also encode, again, based on these neural studies,
the way we move and the way we feel.
What about what we hear?
What about music?
And do we encode that in language?
And how can we use these things
in order to change our behavior?
How can we use them to change our memory?
The conditioning stuff,
you can do this consciously
and it's a good technique,
although I've got to say it's somewhat terrifying.
I did this once in order to erase a painful memory,
but now all I remember is the fact that I erased it
and it's terrifying.
Don't try this at home, kids.
The fact that we can do this ourselves
isn't the dangerous one.
The dangerous part is that we can do this
without someone knowing.
Can we use music to do it?
What is music?
Why does it exist?
And why do we propagate it?
As far as we have records,
we have records of people talking about music.
I don't know if you've ever seen the movie
History of the World Part One,
The Cape Man,
and then there was art,
and then there was the art critic.
So we've been talking about the arts forever
and we don't really know what they are
as the underlying semantic information.
What if they're mind control?
And I'm using the word mind control
a bit provocatively,
but it's a pretty good summarization of what I mean.
I mean,
is it able to put something completely unrelated
into your mind without you knowing it?
When you get a song stuck in your head,
you're getting some idea,
some point in the space of knowledge representation
in your loop.
And actually,
fun fact,
when you have a auditory loop,
if you like hear something in your head,
it's actually going through as if you're hearing it.
Like the same,
your emotions,
it's not quite the same with vision
as far as we can tell.
So I can literally get something in your ear,
stuck in your head,
on a loop,
that every time you hear it,
associates with something else
and you have no control over it.
How do we uncover
how sound gets associated
and how do we figure out
what this means for how we think,
what this means for how we communicate,
and what this means for how information
passes on from generation to generation?
Sometimes people talk about cultures
and cultures need to survive.
People talk about religion
and people talk about countries.
One of my favorite examples to mention
is actually,
there's a very technical sense
in which a country is a living organism.
I mean,
if you look at Schrodinger's definition,
or even if you look at the usual 3.1
they teach you
when you're in lower school,
it can reproduce,
it can consume resources and grow.
It can be born,
it can die,
it can reproduce,
it can consume resources.
Countries can reproduce,
cultures can reproduce,
they split,
they become new ones
with influence and traits
they carry forward.
They live,
they have cellular components,
again,
one of the concepts
from Schrodinger's definition.
Namely,
you can think of them as
the humans could be cellular components
if you want the physical aspect,
or there's not physical aspects too,
but the fact that you can actually use humans,
or even the cells of said humans,
as physical components of a country,
kind of makes the definition
far more literal.
Now,
they can have kilts,
they can have children,
they can be born,
they can die,
they can fight,
they can compete for resources,
and they also have essential components
for propagation.
Things like ideas,
things like beliefs
in any individual human
serve the purpose
of furthering
the meta-organisms
of the countries
and the religions
and the societies
that exist
that contain us
as parts of them.
Is that why music exists?
I think it might be
an interesting question.
Are these things,
are all of these
different concepts
that we talk about
that we don't understand,
things that we don't understand
because they are relevant
at a much larger scale,
are they the same as,
say,
sugar is to a single cell organism?
Are they the same as
blood?
Are they things
that are floating loose
that we consume?
Are they things
that we produce
in order to perpetuate?
Are they just ideas
that are meaningless
that simply exist
like a virus
because they were able
to reproduce?
And I don't think
we'll ever really get the answer,
but we can come close
by analyzing
what they can do.
If, in fact,
language models
have useful parallels
to humans,
we can use them
as a test bed.
This has been
one of my long-time goals.
So we can test,
hey,
how does this match up
with this
on a language model?
We can get
a geometric representation
and then we can say,
hey,
go do this with humans.
Let's see if
this experiment
that we've now found
the best way
of evoking
the test
of
Johnny ate an apple
in an LLM
based on a music,
can we then
play that same music
and say the same
first word to a human
and get Johnny ate an apple
as the output?
We can only do this
with LLMs
because obviously
we have backpropagation,
we can optimize
for all of these things.
It'd be an interesting
experiment to try.
There have been experiments
going the other way around.
I don't know
if you're familiar
with this idea
that we can take people,
have them compare
different words
and see which words
are more similar.
We can take this,
do multidimensional scaling
and get a geometry
of words
and it sort of represents
how we think of them.
If we do the same thing
with LLMs,
we actually get
the same results,
at least with nouns.
We're currently working
on some similar stuff
with other parts of speech.
The fact that those
representations are the same,
even though those tests
have never been given
to LLMs,
indicates that LLMs
do share some representations
with humans.
Does it go the other way?
Pretty common example
of latent information
stored in these things
is there are these
images of faces
etched into
image generation models
that were never
put in there.
As such,
those things
could also be in humans
because humans recognize,
like we recognize
faces everywhere,
except for people
with hyper-partumination,
but whatever.
There are people
who can't see faces
somehow,
but everyone else,
I think,
recognizes faces
in inanimate objects
because we're built for that.
That means
that we can sort of,
if we have a way
of optimizing,
figure out
how to make something
more face-like
in a rigorous way.
In a rigorous way,
we can make
someone confused.
If these theories
are true,
that if we can put
idea A into your mind
and idea A
evokes idea B,
I can get you
to do something.
Right?
I can,
if I say run,
your leg wants to move.
If I can convince
the idea of run
to show up
in your brain
by associating it
with, say,
a picture,
then I can get
your leg to want to move.
And then all it takes
is just a little bit
of prompting.
I don't know
if you've heard
like V-map,
behaviors,
motivation,
ability,
and prompt.
All I need
is to put it
in your mind
and then prompt it
to go.
And you'll think
that you did it
by choice.
Let's see
what we can do
as far as
how we can figure out
what we communicate.
I also want to see now
how to do it
the other way as well.
I talk about
the backwards way
of how do we
get music
to give you an idea.
Also interesting,
how do we take an idea
and turn it into music
so that we can
communicate it
to willing participants?
There are things
like
Sol Re Sol
and similar
other languages
which are
constructed languages
that use music
as a medium.
But those aren't
natural.
People don't have
a natural understanding
for, you know,
people with a certain culture.
Music is far
more compressible
to some extent
but also far less
in other ways
because we can use
polyphony
and we can put
more content
in a shorter period
of time.
Now that we have
recorded sound
we have quite a bit
of power with this.
Can we take thoughts,
take stream of consciousness,
put them
into music effectively
and get a more
dense way
of communicating
information
from person A
to person B?
Can we use this now
to enhance
our cognitive abilities?
Things like this
really aren't studied.
You don't see the field
going in this direction.
People are always chasing
grants with like
medicine
and all these things
where you can easily
get money
because the application
is obvious
but when we start
looking at these
fringe topics
which might allow us
to enhance our abilities
no one really cares
because like
where's the catch?
I think that
if we start working
on these kinds of things
together
we'll be able to
become
superhuman entities
if you will
in the strict sense
of literally
multiple humans
acting as a single mind
and maybe
we'll even be able
to incorporate
machines.
I'd like to see
what can happen
when we enhance
our own consciousness.
We were talking about
some of these
cognitive hackathons
and seeing what happens
when you get people
together from
different fields.
I'd really love to see
what neuroscientists
have to say about this.
I'm always following
the fMRI research
and everyone's looking
at oh well
can we see the difference
between a musician
and a non-musician?
Is music the same
as a language?
At what point
can we see
a neural basis
for understanding music
and at what point
does that change
between a person
who has training
a person who doesn't
a person from culture A
who has training
in music type A
versus with music type B?
I don't know
if it's even possible
to tell the difference.
I'd love to see
if it is.
There's just not
the resources
available for it
because all this
research is expensive
and you really
don't see people
with these common
interests gathering
together.
So if we could get
people with fringe
interests together
I would love to
run some of these
things and see
what we can put
together.
