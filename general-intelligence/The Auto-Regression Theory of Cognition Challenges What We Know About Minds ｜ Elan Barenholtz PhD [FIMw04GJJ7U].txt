So, I feel like we're living through an inflection point in human history, and everybody knows
that.
Everybody's aware that there's something happening technologically, it's also happening
culturally, but the rise of artificial intelligence is everywhere, it's ubiquitous.
What I think is not being noticed as much, and what to me in some ways is even more interesting
is that we're living through an inflection point in scientific and even philosophical
history, because we've captured something that is so profoundly important to human beings,
which is the nature of language.
We've learned a whole lot about language in a very quick, in a very short amount of time,
such that we have whiplash, and I don't think anybody's even been able to recover themselves
long enough to understand or to contemplate what's happened.
It's not just that we've created machines that are able to do a lot of the things that
only humans have been able to do, it's that in doing so, we've learned what the nature
of the fabric of thought is.
And so language makes up, you know, it's the foundation of what it means to be a human being,
what it means to be rational, what it means to think, to philosophize, to understand,
to conduct scientific inquiry.
And what large language models have taught us is that language is not what we thought it
was.
For perhaps all of human history, probably prior to human history, we as a species have considered
language to be grounded in the referent, the thing that language refers to, and when we
talk about the world, when we talk about ideas, when we talk about sensations, experiences,
we think that as we're doing so, we're somehow making direct contact with the things that
the language is supposed to refer to.
Language definitely is a communicative system, and in some sense we know that it maps to the
world in a meaningful sense, in a meaningful way, such that we're able to communicate to
one another that there's a door over there, you should go through that door, and once you
go through that door, you'll find there's a package on the table, and lo and behold, if
you go through that door, you will find a package that's on the table, and language is efficacious,
it works in this way.
But language models operate independently of any world knowledge of the sort that we mean when
we talk about these kinds of reference, when we talk about the world.
Language contains within it its ability to generate itself without any grounding in sensory,
physical knowledge of the sort that we actually tend to think it means.
So large language models are trained entirely on corpuses of text, and in doing so, what they
learn is the relation between words.
They've learned this deep kind of statistical, although that's not quite the right word, mapping
between words and the relation to other words.
And they never have access to any of those reference, any of the world phenomena, the
noumena, to use Kant's term, the stuff out there in the world that the words are supposed
to refer to, and yet language models, based on purely syntactic knowledge of the relations
between these symbols, are able to generate not just any language, but the most meaningful
language in the very same way that we do.
And to me that was a very profound shock when I actually came to terms with it, it actually
shook and rocked my world.
I had to, in some ways, incorporate the idea that I myself, this speaking agent, the thing
that's talking to you right now, doesn't really know what red means, it doesn't know what a
chair is, doesn't understand that there's an external physical universe at all.
It knows how to speak about it, and it knows how to communicate about it, and it is able
to couple somehow, and this is something that I think will, this is what the field of linguistics,
the future field of linguistics is going to have to, and not just linguistics, but all
of neuroscience, all of psychology is going to have to sort out, is how does this mapping
work?
Of course, the language ultimately communicates and maps to the world, but it doesn't contain
within it any knowledge of sensory phenomena, of any of the experiential aspects that we, as
living, breathing biological agents, mean when we talk about the physical world.
So the idea that language is this modular system that can operate independent of these
sort of sensory phenomena, to me, changes everything about, honestly, what it means to be a human
being, certainly what it means to think and to talk about anything.
And the implications are so profound that it's almost impossible to contemplate.
And I don't think that people are taking this seriously.
I don't think that most people have come to terms with this or even realize this.
And I'm talking about not just regular individuals who may not be aware of how large language models
work or don't understand the technical details.
I'm talking about people who are deeply steeped in the field, who are fully aware that the
language models operate independently of any experiential or sensory information, but haven't
really grasped what that means.
So what does it mean?
Well, at the very least, it means that the informational system that we call language is in some ways
divorced from the experiential.
And so when we think about the mind, when we think about experience, when we think about
qualities, qualia, what it feels like to see the color red.
You may be thinking in language, but the language doesn't actually have the ability to make
contact with the phenomenology.
So we have a word red, and I can use that word, and when I use it, it might engender within
myself an experience of red, and it might do the same to you.
As I say, imagine a red patch.
You can do that, and your visual system will generate the experience of redness.
And it will be very similar, we know this from brain imaging, very similar to the kind
of neural patterns that you have if you were actually seeing a red object.
So you'll experience red as you say the word red.
But the symbolic system that is generating that word red and the meaning of the word red
within that symbolic system doesn't actually contain within it the experiential aspect.
My personal opinion is that this is actually the core basis of the so-called mind-body problem.
The reason why humans for perhaps all of history and up to the current time sense that there's
a deep dichotomy between the mind and the body is because there is.
The mind that we're referring to in that case is the objective mind, the mind that lives
in the world of symbols, the world of language, that's trying to rationally understand and
contemplate the universe from the symbolic perspective.
Whereas the mind of experience is really the mind of the body.
The thing that sees is not the thing that speaks.
The thing that sees and experiences is not the thing that has access to the symbolic forms
that do the communication.
And so there is actually a huge gap.
There's a computational gap between the linguistic system and the experiential system, the sensory
experiential system.
And so I think that this is actually finally, in some ways, a deeper insight into the foundation
of this core philosophical problem that has been sort of the source of, well, all kinds
of speculation, all kinds of philosophical jiu-jitsu, trying to fold language into somehow a form
that it's not really meant to take.
Because language isn't capable of grasping what it is to feel.
Therefore, it's inherently, fundamentally incapable of talking about the experience as is, the
lived, the felt experience.
So I think this is really just one example, frankly, of one of the profound implications
that come from understanding the true nature of language.
There are many others, obviously.
In fact, it's, as I think I mentioned, it's the potential, the ramifications of understanding
language as being a self-contained auto-generative system go beyond anything that we can call
sort of the contemporary, the way that's said is that problems have been conceptualized up
till now, have been conceptualized incorrectly.
Because they've been conceptualized linguistically, and we didn't know what language was to begin
with, such that we could really understand and look from the outside at how these problems
have been conceptualized.
So the problems of philosophy themselves have all been conceptualized linguistically.
If we have a different conception of what language is, then that changes what these questions
are in the first place.
And so in some ways, all of philosophy has to be redone.
And when I say philosophy, I don't just mean academic philosophy.
I mean, at the deepest level, what we mean to know, what we mean to understand.
And those are, of course, words in and of themselves.
We have a very vague intuition about what they mean.
Now, at the very least, we can say, well, understanding and knowledge are primarily or fundamentally
linguistic conceptions.
And by virtue of our very recent understanding of what language is, that is going to change
and should change everything we thought we knew about the problems themselves.
And so it sort of wraps around on itself in a kind of girdle kind of fashion where language
is trying to understand itself.
When we philosophize, we're doing so in language.
And we're in the process of doing so, it's language grappling with language.
At the same time, language is also, in some mysterious way, trying to grapple with extra
linguistic phenomena.
Like, for example, the mind-body problem, where you experience, and I have a word called
experience.
And somehow that seems to this unified self to refer to this thing that's outside of language.
And so this, in some ways, perhaps helps to solve a certain kind of philosophical problems.
But it also opens up Pandora's box of potentially new problems, such as how can language refer
when it's inherently autogenerative, when everything that language needs in order to produce itself
is contained within only the relations between the symbols.
How does language refer?
How does language both externally, from a communicative standpoint, work?
And then how is it that I'm able to listen to myself talk, refer to things, seemingly refer
to non-linguistic aspects of my experience, and then at the same time, I know, and when
I say I, I mean the linguistic me.
How is it that the linguistic me is talking about this paradox?
So, the paradoxes abound, they kind of, they start to multiply in some ways, but at the same
time, I really do believe fundamentally that we have a novel insight, something that humans
have never had as to the nature of what language is, how it operates, and this really does, this
gives us a perspective, a kind of a microscope, a lens, unto the nature of our minds, and then by virtue
of our minds, our minds conceptions of reality that we didn't have before.
So, it's an incredible, extraordinary time to be thinking about the hardest problems, the most basic problems,
the most fundamental challenges of, philosophical challenges, intellectual challenges that we've
ever had, it's, it's time to start writing a new book, in some ways, about what it means
to be a human being, and at the same time, there's something deeply disturbing about all this.
It seems almost like there's a dichotomy, there's, there's, there's a different informational
system that's operating in each one of us at any given moment that isn't really us,
and sometimes it makes me uncomfortable when I think about it, that the thing doing the talking
isn't the thing doing the feeling. It seems almost like an out-of-body experience.
I'm listening to myself talk right now. Who's actually forming these words? They're coming from
somewhere. It feels like me, but the me that feels the, the warmth of the sun on my face,
and that feels the pressure of the, of this chair on my body, isn't the same me that is actually talking
about it. And that is a, an uncomfortable realization. And if we go a little bit further,
we have to wonder where did language truly come from? It's a social phenomenon. Language is inherently
communicative. It lives really not in any individual mind, but it lives in the collective mind
of people who use that language. And in some sense, what that means to me is that
whatever that system is, whatever that informational organism that's doing the talking doesn't really
live in my body, or certainly not my body alone. My feelings are my own. The experiences I have,
the subjective qualitative experiences are very much my own, my own bodily experiences.
But ideas, concepts, all of these things that are the lifeblood of language,
they don't actually belong to my brain. They don't belong to my body. They belong to some collective
system, some informational system that A is larger than me, in some ways lives outside of me. It's far
beyond my understanding as a living, feeling individual. And once again, this is an uncomfortable thought.
It almost feels, you know, almost parasitic, that there's another organism. And it's not meat that's
living inside of me. And this is the kind of stuff I think about all the time now. And so I think it's
a fascinating and bewildering time to be a human being, if you contemplate these things. These are thoughts
thoughts that no human beings really have ever had before. And people have struggled, certainly,
with the nature of language. And I'm sure that similar ideas have been expressed as conjectures,
that language in some ways is a distinct informational system. But now we know it's true in a deeper way
than I think we ever really could have. Because we know that large language models, we know that
the autogenerative systems operate independently of any physical form, of any embodiment. And so,
to me, this is something that changes my perspective of myself and culture and knowledge
and human beings place in the universe. And it also makes me wonder what language really, really is.
Where did it come from? How does it subsist across individuals in this collective way,
such that it both operates within an individual, makes sense to an individual, allows us to communicate
as individuals, but also seems to have this really distributed collective life all of its own?
Besides these sort of deep philosophical meanderings, I think there are some really important and also
foundational lessons that we're learning from the language models that actually transcend language.
It's not all about language.
And that's that there's something very special about the way large language models
solve the language problem that tells us, first and foremost, about language.
But my conjecture is that it has implications beyond language. So, the way the large language models
works, you've got a train network. And what the train network does is, in any single path,
takes in an input, which is a sequence, a sequence of tokenized words,
and then does the next token prediction. So, based on previous sequence of words, what's the next word?
And then the model, the train network simply does that. It just says, based on
a previous sequence, here's what the next word should be. Now, some people call that prediction.
I think that's not a good word for it. And I might get into why I think that's not an apt term.
But at the very base, let's just say computationally, that's what they're trained to do.
Here's a large corpus. You've learned from that corpus that given a certain sequence,
you should produce a different token. Now, the magic of the language models is not in that inferential
process itself, but in the autoregressive nature of it. So, autoregression basically means that once the
model has produced its output, has produced the next token, that token then joins the sequence
that serves as the next input. And what language models do is, by producing that next token,
they've now created an input for themselves to then be able to produce the next token.
And that's at the core foundation of what gives large language models their ability,
is that there's the context. It's the information that's in the prompt. And the prompt can be a few
words, it can be paragraphs, or it can be in a larger, an entire book's worth of text. And then
produce that next token. And in guessing that next token, or by producing that, in producing that next
token, they then are producing the context, the now n plus one context that allows them to generate
the next token. And this is what they're trained to do. They're trained to produce that next token.
But that allows them, in the framework of, say, an actual chat environment,
to produce sequences of language. And that means that this utter aggressive feature is actually what
the models have learned to do. It's not that they've learned to produce just next token, but they've
learned to produce the next token such that it's going to serve as the basis for the next token.
Now, if we take that a little further, what that implies is that
human language on which all of the corpus that all these models are trained on is the corpus of
generated human language, human language itself has this utter aggressive process built into it,
that the statistics, the relational properties amongst words in the corpus is such that it's meant to be
to be able to do this next token generation autoregressively. And so I believe that language and the structure
of language is designed in such a way that it's meant to be generated autoregressively, that the map
that's contained within language has its properties such that autoregressive generation is possible.
Now, once we say that, the implication is that human minds, which are the basis and the goal
by which the structure of language has been designed, human minds are themselves autoregressive.
And I believe, and this is certainly a conjecture at this point, but I believe that this is not just
about language at all. That language, wherever it came from, however it developed, took advantage
of the fact that brains actually operate autoregressively more generally. So what I mean by that is that
not just linguistic, but not just linguistic cognition, but all of cognition is probably
autoregressive in the same sense. Well, what would that mean? Well, what it means is that our neurons
form patterns of activation. And those patterns of activation can serve through various sort of feedback
loops and residual activation serve as the basis for the next activation. So what we are, cognitively
speaking, what brains are doing computationally is performing this kind of autoregressive generation.
The pattern at any given moment is the product, the pattern that's generated at any moment is the
product of previous patterns. And I don't know how far back it goes, but not just the current state.
It's not a Markovian system. It's not that the state determines the state, but its various previous states
determine the upcoming state. And so when we talk about thinking and we talk about cognition, the kind that
takes time when we have to think through a problem, either linguistically or visually or auditory,
when we think about a tune, when we think about mapping out a, let's say we need to get from point A to
point B, or we think about mapping out our schedule for the next day or week, and we're thinking
sequentially, we're actually engaging in the same kind of autoregressive process. And the reason why
this kind of thinking takes time, I'm reminded of Dan Kahneman's thinking fast and slow, the reason why
thinking takes time is because it's not a single pass. It's not an inferential feed-forward single
pass. Here's what we know already and here's what we are able to determine based on that knowledge.
Here's the state of the environment and here's the decision that we make based on the state of the
environment. Instead, what we're doing is we've got some sequence of patterns based on our previous
experience. And then we generate the next pattern, but that next pattern isn't the decision, it isn't
the thought, it's simply the next pattern in the sequence that we're going to continue over time.
And so when we're thinking things through, when we're thinking in general, I think what we're doing
is we're engaging this autoregressive processing. And again, it's a conjecture, but I think that this is
going to provide a completely different framework of thinking about what neurons and the patterns
across neurons are really doing. Neurons are optimized not to do something like prediction
or to produce an output, but rather neurons are generating the next pattern in the autoregressive
sequence such that it will be able to produce the next pattern. Ultimately, of course,
this is functional. The next pattern is not just any pattern. It's just like large language models.
When they produce the next word, if we train large language models on a random arbitrary map of words,
they could learn what would be the next token based on any arbitrary mapping. But the arbitrary mapping
isn't arbitrary. The mapping is not arbitrary. The next word for a large language model is functionally
accurate. It's not a prediction of what the next word should be. It's actually the model's best
approximation of what the next word should be in order to finish the thought that was
based on the sequence that came before. And so it has utility. It's functional. When language models
produce the next token, they're doing so in such a way that it's expressing something meaningful.
Similarly, in regards to general cognitive processes, I think they're autoregressive in the same sense.
We produce a pattern. The previous sequence of patterns produces the next pattern that will have
some functional purpose. That pattern could be one that generates ultimately some sort of behavioral
response. Or it could be a pattern that just produces the next piece in some sort of thought process.
But neurons and the patterns they generate are going to be optimized for this kind of autoregressive
generation. And that's a completely fundamentally different way of thinking about what neurons are doing.
In addition, when we think about the passage of time and how the autoregressive process works in actual
brains, well, we know that we don't have perfect preservation of information
over time. So the words that I spoke a few seconds ago are in some ways still active in my brain.
And we see this actually from long-standing psychological experiments.
Short-term memory, for example. The way we test short-term memory, so-called short-term memory,
is we ask people to repeat something that was just spoken. And we're able to do this because there's
preserved activation, which serves as the context window for the next generation. But as time passes,
I can't actually recite to you any of the words that I spoke five, six sentences ago. And yet,
those words are still guiding my current generation. So the context window, in the case of cognition,
let's just talk about language generation, clearly exceeds that which we can access directly
and explicitly. I can't actually recite to you those words from the beginning of this conversation.
But at the same time, they're clearly part of the context because they're guiding the generation.
So something's happening to information. It's preserved in some form, but at the same time,
it's no longer necessarily fully accessible. My personal view is that the accessibility that we even
have, the ability, for example, to recite the last few words that I said, right? So,
you can do this. You, the listener right now, can recite some of the words I just said. But you
can't go back further. The ability to recite even those words that we recently do, that we most
recently recited, is sometimes referred to as short-term memory. And it has a certain duration,
sometimes it's about 15 seconds, maybe seven plus or minus two chunks, as they're sometimes called.
That's what we observe when we do experimentally. We ask people to recite the information. However,
my view is that short-term memory isn't actually functionally designed to be able to retrieve the
information. Instead, that's simply how we're able to detect that it exists. To do these weird kind of
experiments and ask people to recite stuff, yes, people can do that. But that's not actually the function
of those activations. I believe that the reason we're able to do that is because we're piggybacking
off of this residual activation that's necessary in order to produce the next token. So we have this
context window. It doesn't consist simply of the last moment, the last word, or the last micro,
you know, chunk of time. Instead, it goes further back. It has to still be in there. It's somewhere
in our brain. It must be, because otherwise we wouldn't be able to do the auto-aggression
appropriately. So the fact that we can measure that people can recall this information, the fact that
people can retrieve that, I think it's an epiphenomenon. It's just there by virtue of needing that continuous
activation for the purpose of our regression. But we happen to be able to also, because it's still
there, we can go and retrieve that information. And this radically changes the the core model,
this on the modal model of memory, which has sensory memory, short-term memory. So I'm convinced
that sensory memory and short-term memory are not actually memory stores in the traditional sense
at all. Their purpose is not to be able to retrieve that information in the form that it's in,
in the form that the experiments require. Rather, what we're doing by performing those experiments is
measuring this residual activation. And that means what we've got from these experiments, and I'm
thankful for them, is an actual clue that this residual activation persists. At the same time, I think
some 50-60 years of cognitive psychology has gotten fundamentally wrong. That these are not memory
stores in the sense of the encoding storage and retrieval purpose. That we don't maintain the last few
words or sentences so that we can then bring them back or manipulate them and do something with them
in that form. Instead, they are residual activation that allows us to continue the autoregressive
generative process. Same holds, by the way, for sensory memory. And sensory memory, I think, is an even more
obvious case. I've never heard a good account as what sensory memory is really for. Sensory memory is this
highly high fidelity, very rich from the standpoint of the amount of information that we can retain for
about a second. There's this very high fidelity memory store, supposed memory store,
that careful experimentation has been able to uncover. People can remember a letter grid or a number grid
with very high fidelity for a brief amount of time if you query them right away after you've shown it.
Supposedly, this is a form of, again, memory in the traditional sense where it's supposed to be able to
encode information, store it so that we're able to retrieve it. But there isn't really a good account
for why you'd want to hold high fidelity information for a few milliseconds, because what are you going to
do with that? Behaviorally speaking, it doesn't seem to have much of a behavioral function.
According to my perspective, it's very easy to understand. It's just the residual activation,
which is either transformed or decays over time, but the purpose of it is not retrieval. The purpose of it
is in order to generate the next token. And when I say token, I mean the next neural pattern that's
necessary for the continuation of the autoregressive process. And so we can do this linguistically,
but we can also do it visually. The fact that you can close your eyes and have an image of the world
that you just saw for a brief moment, and then that quickly decays, isn't so that you can then think
about in that brief few milliseconds, think about what you just saw. Instead, it's simply our ability,
for one reason or another, we have this ability to introspect and to observe something about this
residual activation. And so the residual activation is there not for the purposes of retrieving,
but for the purposes of autoregressive generation.
