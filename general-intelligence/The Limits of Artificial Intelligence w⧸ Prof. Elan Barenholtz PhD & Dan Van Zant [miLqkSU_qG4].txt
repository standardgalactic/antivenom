Hey, how you doing? Hi, I'm Ilan Baranoltz. I am the co-director of the machine perception
and cognitive robotics lab at Florida Atlantic University. Howdy, my name is Daniel Van Zandt.
I'm getting my PhD in computational neuroscience at Florida Atlantic University and this guy's my
advisor. I'm kind of working on a little bit of a sort of grand thesis, which is that cognition
itself is basically autoregressive generation. So what I mean by that is that the large language
models like things like ChatGPT do this thing where they're a trained network that has learned
to just produce the next token. In the case of language, it's just the next word. Typically,
it's just the next word. And then after producing the next word, it uses that to feed that in as
as its next input. And it builds up a larger and larger input, but does this stepwise. So it's
just taking an input, producing the next token, tacking that on to the previous input, and then
producing the next token again, and then just does it successfully. And what we see from the large
language models is this seems to actually solve language. We can get more into that as we dig in.
But to me, this was a kind of epiphany that perhaps this is what cognition more broadly is,
that what our brain is, is a trained network that has learned over the course of our lifetime and
experience, what is the appropriate next token, whether it's, it could be language, but it could
be in other modalities as well, where we're thinking visually. Well, we might produce the next,
the next visual image based on the images that we have thought about in the past, in the recent past.
And I'm sort of going after this aggressively, as kind of like, prove me wrong, Occam's razor,
that this might account for all of cognition, that what we're doing when we're thinking,
and when we're reasoning, is we're really just doing next token generation. And that knowledge and
memories and all of these things are really just the trained model. So when I say, you know,
remember yesterday when we had drinks, what you're doing there is your brain is maybe over the course
of the evening, when you were sleeping, has integrated this information into your brain that
we've had this experience. And what that means, what it means to have integrated is that when I say
those words, remember when we went drinking, it's going to generate that sequence of the visual
sequence, which is going to happen autoregressively. And that's what it means to remember something.
And then as I'm thinking about that, and I'm remembering that, I'll, the linguistic system
will be able to gather information from that and say, yeah, oh, that's right. You got a, you know,
martini. And then the linguistic system and the visual system will sort of act in concert.
And basically that synthesis is what we call cognition. So that's sort of the, the bold,
broad claim here that all of cognition and knowledge, representation, memory, and all that,
it can ultimately be accounted for as a kind of train network that does autoregressive generation.
Absolutely. So Alon said, he's almost putting this out there as a challenge. And in some sense,
I'm challenging it or at least part of it. And that's that I think a really important part of
cognition and Alon alluded to this is knowledge. So it's, it's things we know about the world.
And I have this idea and some experiments I've done that seem to indicate this idea that knowledge
has its own structure and the structure that knowledge has is nothing like the structure that
large language models have. Um, and this is really important, uh, because not only does it mean that
large language models would not be able to have knowledge in the same sense that humans do or even
animals do. Um, but it would also explain things like hallucinations or the fact that large language
models can seem like they have split personalities, where if you get them into one kind of conversation,
they'll say one thing. And if you get them into another kind of conversation, they'll say a
completely different thing, which is a very inhuman thing to do. Um, unless the, the person's
schizophrenic or something like that. And so, uh, we can get into that a little bit deeper, but,
but that's sort of the fundamental idea is that knowledge has its own structure and that
you can translate some of that structure into the way large language models work.
Um, but that you lose something in doing so. Uh, the other part is that I think is important at
a high level is large language models work. There's some sense where you have a computer and you have
software that runs on the computer. And so, uh, I think part of what I'm saying and where I agree
with Elan is that language is the computer. So language has its own processing power. Language
can perform operations. Language can do really important things for cognition, but that large
language models are missing the software that runs on the computer. And the software that runs on the
computer is where knowledge is represented and where information is synthesized and where, uh,
all sorts of interesting stuff happens. Yeah. Um, and these, these are, you know,
obviously good, uh, good points, uh, important points. I want to sort of address the last thing
first, which is, I think there's a very strong intuition most people have and would share, um,
that, that language is, isn't the whole story, of course, uh, when we're thinking and conceptualizing.
So two things to say about that. One is that the, the, the reason I was gobsmacked,
and I think we all should be, um, by LLMs. Well, besides the fact that they're just extraordinarily
impressive and are doing things that, um, you know, I, I didn't see coming, um, you know,
the scaling laws sort of really have been very, the scaling is, is, has sort of solved, uh, a lot
of the problems that, that people, uh, imagined. Um, but they're, they're really running only on language.
Uh, that, that's something that's very important to reinforce is that, uh, leave, leave alive,
leave aside multimodal models for a moment. Although really, I don't think that poses it,
uh, an actual, uh, it's not really a counterpoint. Um, but let's just talk about text-based only,
right, large language models just churn through, uh, linguistic data and what they learn is just the
patterns, uh, and the structure of the language itself without any exposure to, there's, there's
nothing else in its core, in the, in the corpus, there's nothing else in its data set, uh, that
allows it to conceptualize beyond just the, the relations between words. And, you know, to me,
the big surprise was that that's sufficient, uh, to do, uh, what's, what looks like fairly broad-based
linguistic reasoning, in any case, linguistic reasoning about, uh, you know, uh, well, call it
reasoning, call it whatever you want. Um, they, they are able to produce chains of thought, um,
that look very much like, and, and really pass the smell test and, and the sort of the rationality test,
um, of, of soundly reasoning and, and responding in language. And that means language seems to contain
within itself the appropriate structure for generating itself independent of any other conceptual
framework. Now, in the case of human beings, of course, language is ensconced within a broader,
uh, kind of environment. And that environment includes the ability to think non-linguistically.
And that, I, I'm certainly not denying that, and I don't, and I'm not claiming that language operates
completely modularly. If it, of course it doesn't. I can describe, you know, there's the black shirt
you're wearing. Language didn't have linguistic knowledge about that. It had to get there somehow.
Um, but that information, uh, it ultimately ends up for the linguistic system as some sort of
linguistic token. And then I believe then it just runs within, based on, uh, sort of its internal
processes. But at the same time, when we're, um, engaging in, in thought and we're, we're talking
about, you know, abstract ideas or, or anything, any, any, any time we're, we're frankly, when we're
thinking, the language model is also acting in concert with, with other, um, other faculties.
And I think this, now I would call it being, you can be, one can be mistaken, and I'm saying
on the basis of my, of my thesis, one could be mistaken for thinking that, therefore,
there's a completely different kind of conceptual framework, uh, that isn't captured by autoregression,
because we only know autoregression works. Right now we only really know it works for language,
and we don't know that a similar kind, um, of, of, uh, sort of computational framework is going to
work for other, uh, other kinds of thought. But the thesis is that if it works for language, it
likely works for the other, other frameworks as well. And therefore, when we're talking about
having ideas outside of, of just language or just out of autoregressive, uh, linguistic, like the kind
of things that alums do, I'm saying, who says? Uh, I'm kind of making the argument, like, are you sure?
Perhaps we can account for all of those things. Is it just an intuition, a very natural intuition,
to say, like, oh, come on, it's, that's not all we're doing. We know about abstract concepts. And I'm
like, no, we don't. What we've done is we've learned over the course of, uh, you know, of our
developmental history to produce this kind of autoregressive process that appears, uh, to be
doing something much more abstract. But in fact, perhaps it's, it's, it's closer and more concrete
to the kind of rule learning, structure learning that we've done in the, that we know the LMS can
do in the linguistic domain. Um, now I forgot there was, there was the earlier point you wanted to,
that I wanted to return to, but I'll need you to remind me what it was. Cause, uh, the first point
that you made, anybody remember? Addy, what was it? Well, before we do that, you're using the word
autoregressive a lot. Do you want to explain what that means briefly? Okay. Well, so I kind of
mentioned this before, um, but I should unpackage it a little bit more. It's, it's
autoregressive is the, uh, the process where your, your, your output serves as your next input. Um,
and, and so as part of your next input. So let's, let's, let's, you know, think about in the context
of, of ChatGPT, which a lot of people are very familiar with. You, you type in a question. Um, you
know, here's, here's the question. Um, you know, what do you think of, of, uh, the idea that, um, that the
the brain is just an LLM, right? And there, there's, there's the input is just that sequence of text.
So, uh, what, what the, uh, LLM is going to do with ChatGPT or Gemini, it doesn't matter.
Um, they all operate, uh, based on, on this same, uh, same basic principle. It's just going to say,
given that set, that string, uh, you know, what do you think of the idea that it's going to produce
just a single token? I, like, let's say the next thing is I think, or as an LLM, whatever it's
going to say, it's just going to produce that very next token. Let's say it's as, right? As an LLM,
I mean, or I don't, you know, it often says things like that. Um, and so that as is now going to be,
uh, tacked on to the previous sequence, which is the entire question. Um, there could be some
invisible to the user kind of, uh, uh, uh, other tokens that are in there, uh, that shows sort of,
this is where the, the, uh, you know, the, the user stopped and this is where the LLM started,
but it's taking in that next token that it generated, it's feeding in as the input. And
it's going to say as, okay, given the, given the previous sequence and the word as, what's the next
word? And it's going to say as an, and then it'll continue to do that, uh, successively. And that's
thinking, uh, for the LLM, that's the only kind of thinking it can do. Um, but turns out that that is
sufficient, uh, to be able to not only do next ticket predictive prediction based on the previous
five, 10 tokens, but, uh, with a large enough context, as it's called, meaning the, the entire,
the size of the, of the input, uh, the basis that's getting churned through, uh, on each successive
generation, uh, it can think about, think about, uh, things, you know, concepts or whatever, uh, tokens,
anyway, and let's just be concrete that has occurred very far in the past. You could give a, a
document, uh, that's many paragraphs long. You could get an entire, some of them, if the context
is large enough, you can get an entire volume, a book size volume. And what it's doing is it's
guessing this very next token based on taking into account, uh, something that was, uh, you know,
many chapters ago. Um, and that's what we as humans do all the time in this conversation. I'm making
reference, uh, now to, you know, the, the, uh, comments that Daniel made. I, I just failed at that,
by the way, uh, I was not able to retrieve. There's some sort of decay that happened there
or something like that, but, uh, I'm taking into account things that have been said throughout
this whole conversation, but I'm doing it at this discreet or this, and again, this aggressive,
it's just this discreet next token prediction. And so the power of that, uh, that, that ability to do
next token and then just do that successively, uh, is, is what I find astonishing. Um, but seems to
be able to, to, to reason and to conceptualize, uh, far beyond what I would have, uh, assumed just
based on the, that simple kind of process. And so that's, that's really auto-aggressive. Now auto
regression, again, it could apply to something else. Let's say I'm thinking about, um, you know, uh,
what I did this morning or something like, I'm thinking about, you know, I, at first I had to go
on my computer and then, uh, you know, and I, I, then I got on the phone and I was coordinating with
these guys. And as I'm doing that, uh, I, I have the experience of kind of going through a temporal
sequence. And the idea is that you could probably model that same, just like the, in the, in the
language context, what I'm doing is I'm taking the previous input. So here's, you know, the, the,
the, in my mind, I'm thinking of all the events that took place up to this point. And then I'm
remembering by what was remembering means in this case is just generating that next token. And then I got
my phone, uh, and I'm doing the same thing. So that would be sort of writ large. What auto regression
would mean is that what you're doing is constantly just generating, uh, sort of the next slice of, of
time, um, or, or linguistic token. Um, you know, this would be kind of visual tokens in the, in the
visual case. Um, and then you're, you're just tacking that on again to the, to the input and the input is
just growing larger. Now, in the case of people, I don't think what we do is, is similar to LLMs.
It's clear that there's some sort of decay function or something like that.
Large language models take literally the entire sequence and ram it through again and again,
very, very inefficient in some ways. And, and by, and we weren't without getting too much into
details. Every time you do that, you literally have to, you have to process every single word
relative to every single word, which is a very computationally expensive thing to do. Um, the,
the, the GPUs, the, the, the compute power is there. Um, although it's not probably, you know,
it's not efficient to the point where ask open AI, they're not making money. Uh, nobody is, I don't
think. Correct. And it's too computationally expensive. Um, but in principle, it's sort of,
I think it captures the essence of what we do. Um, and, and in humans, it's clear that we don't retain
the entire, you know, I don't, I don't remember, uh, what I said, you know, really the verbatim,
uh, you know, even if two or 30 seconds ago, but something, some version of that, uh, where,
where we just, we're somehow, uh, capturing, you know, the, it's probably transformed in some way,
but it's still probably this, this basic auto regression. So that's, that's the core idea
of what auto regression is. And when I'm, when I'm proposing, uh, accounts for, for it, all of it,
for, for cognition. And so knowledge in this case, um, isn't organized in any, um, way that's,
that's not captured by the, the, the representation, the pattern of, um, of, of how brief tokens have
appeared together, uh, I guess you could say. Um, so I, I, and I, I don't, I don't know what your
conception of, of knowledge is and I'll hand over the mic to you. Um, but yeah, the Occam's Razor
challenge here is given that knowledge in the case of LLMs is just the ability to produce the next
token in such a way that they sure sound as if they know a lot of stuff, right? And there's
nothing else in there, but just the representation of, uh, you know, the relations between words.
And it gets very semantic as you get deeper into the network. It's not, you know, we can't just
talk about like literally relations between words. They're there. It's much richer than that. The
representation of these words is, is a rich, very rich thing. Um, but fundamentally that's sort of
the claim. It's not more organized than what the LLMs are doing. So not, or not, not organized in a
very different way. Yeah. Yeah. I see what you're saying. Um, so I think that's actually a great segue
into the first point that you asked about and what I wanted to talk about. So I mentioned, uh,
I think knowledge is software and it was software, the software that runs on a computer that we would
call language. Um, and, and so it's language, isn't the whole story. Uh, and in particular,
what I, and something I want to mention really quickly that I was thinking about when you talked
about is, uh, I think a strength to your point and maybe something you should start mentioning is,
uh, language isn't the only auto regressive process that generative AI and humans share.
I would say that producing images is also an auto regressive process for both.
So, no, I did say that. Oh, you did say that. I did make, I make that, I made that claim. I just
said, uh, so I think it's worth mentioning that humans generate images auto regressively,
where they draw out like a rough sketch and then they fill out the details.
Interesting. Okay. Not just speculation. I see what you're saying that when people do,
uh, actual image generation, they sort of do it auto regressively.
When people draw a picture, they do it auto aggressively. And, and when the,
the image generation models that exist draw a picture, they do it auto aggressively, where they
kind of, it's different, but they kind of make a rough sketch and they fill in the details.
So diffusion. Okay. So I was gonna, uh, so I, when I think of imagery, I'm thinking of like,
like, I think all of our imagery is, is, is, uh, is video and video generation, right? We don't really do
image generation as people, as humans in our mind. Um, no, we, we have to do it when we, when we're drawing.
Um, but I, you know, when I say alluded to earlier and, but it's more of a conjecture is that our,
I think our, our visual thinking, like a memory, um, you know, episodic memory, I think could be
accounted for as a kind of, uh, auto aggressive video generation. Similarly, visual reasoning,
like thinking about how do I get from this point to that point? I think if we're doing, if, if it takes
time for us to do it, like cognitively, like I, my suspicion is that what we're doing is just this,
that's why you can't do it instantaneously. There's certain things you can just solve,
like in a moment of insight, uh, recognition, boom, I've done it. Um, where some things we have
to think when we're thinking, what are we doing? And it's my suspicion is that what we're doing is
we are actually doing, uh, this, this is consequential regression. Now, I think that the video
generation models, um, I'm not well versed enough, uh, to know if this, to what extent industry,
uh, is, is, is, uh, homogenous on this point, but I know that many of them do use
auto regression. Um, and that would be essentially the same idea. Produce next, produce a frame,
take into account the previous frames, like start from this point, uh, then produce the next frame,
then take the previous two frames, produce the third frame, and so on. And that's classic auto
regression. And, and I think that that is very likely the same exact sequence that's happening when
we do visual reasoning. Now, your, your point about when people do it, like sort of actually generate
in the world, our actions are also in this way kind of utter aggressive. When we're reaching for
an object, uh, and it might be, I don't want it to, you know, it can border on the tautological and
you have to be careful. I'm making a very specific claim that, that the computational process really
takes into account previous states, um, and that those are used to generate the next state. You know,
when you're reaching for something, it might not be autoregressive in this way, but it's, uh,
but it very well might be. Um, I, I haven't been able to, to deal, to delve deeply enough into it.
Um, but certainly when we're acting and when we're generating, like generating, we're, we're doing
it sequentially. Um, it's certainly true, but the world operates this way. Like it's sequentialism is
sort of built in, like, like it or not, one moment follows the next. Yeah. It's just how it's
physics. It's like with the world, whatever, you know, I'm not going to get into the, we're not
going to go too deep here, but you know, the world sort of unfolds and, and, and over time, um, for
lack of a better word. Um, and, and of course, sequentiality is there, but there's a, there's a,
there's a sharper point here about the computational process as to whether it's autoregressive.
Really it's building up a longer stack, um, of, uh, uh, uh, increasingly, um, uh, larger input
that then forms to, to do the next, uh, to, to, to form the next output. Um, so, so I, you know, um,
I don't know if I could point to, to, to your example as necessarily supporting it, because it
might be that's, that's really just happening. I don't, you know, to use the Markovian, uh, it's like,
is it taking the past into consideration or not? Um, auto-aggression absolutely has to take the past
into consideration. Markov, Markovian processes do not. Um, it could just be the current state
is sufficient to generate the next state. And I don't know if drawing necessarily requires
that you've taken the previous states. The world takes care of that. The world, uh, is in capture,
you know, by the, by the virtue of the fact that you've made marks on the paper. Now you've got,
instead of just a little point, you've got a curve, right? But that could just be the next
area for that. That's not part of your cognitive process that's taken into consideration. It's the
world is changed. It's similar to, uh, to, uh, uh, uh, cellular automata, um, which, which, um,
in some ways sort of seems auto-aggressive, but it's not, um, because this, the, the rules just say,
given the state now, they're totally Markovian, uh, the, the, the state of the environment, uh, in the
cellular automata case is keeping track of the past such that the world sort of takes care of, of,
of the maintenance of, uh, uh, and, and so it's sort of, uh, in, in, in totality, you, you get sort
of auto-regression, strangely enough, but it's not. Computationally, in terms of the, the rules,
uh, for generation, it's not auto-aggressive. The auto, would you say that in cellular automata,
the auto-regression is an emergent property that comes from the local properties of the cellular
automata? Plus, plus the environment. Plus the environment. Plus the environment, which has its memory,
right? The environment, the environment has this memory of, of sort of what, uh, you know, the
changes take place, are, are, are, are preserved. Yeah. And that defines the state for the cellular
automata. So it's just any sort of, you know, state is, it's just, uh, uh, you know, I guess it's a
finite, it's not even a finite state machine. I don't know how to use it. Forget jargon, um, right?
Especially if you're not, you don't know if you're using it correctly. Um, but, but it's, it's just,
uh, you know, the rules are only taken into consideration. It doesn't, you don't ask what
happened in the past. In, in a, in a cellular automata, you don't, uh, the, the rules do not
ask what happened in the past. In a auto-aggressive, uh, generative model, it very much inherently does.
But just by virtue of, uh, the, the input, um, is, I mean, in some ways it's, it's not that different.
And I've, I've been thinking about this, but I shouldn't do this on camera, but you know, there,
there is one is a sort of an internal process where you're building up this, the, the, the state is just
this increasingly accumulative situation. And one is, it's the world, uh, that's doing that.
Um, so they're, they're very, they, they have certain very strong similarities.
So you're, you're saying that an auto-aggressive process has to account for multiple states.
It can't just account for the previous state.
It, it, it has, it needs, it needs an increasingly, uh, well, it's just that the, the state itself,
um, is dependent on its own outputs in such a way that, and, and, and it has to, and it's sequential.
This is very essential. Um, so this is the big difference, right? So in a, in a, in a cellular
automaton, there's no record in the world as to how it got there. So how, what should, right now you've
just got, and I don't know if we have, and we haven't explained what cellular automaton is, but
let's just, let's let, let's let the listeners, you know, just look it up. Cause it's just look
up game of life. Really cool. Yeah. So a cellular automaton is basically, uh, it's, it's a process
for taking, um, uh, some simple rules that say, let's say you have a grid and a grid, uh, can be made
up of little squares and the squares can either be black or white. Um, and then what you do is you have a
rule given, uh, so for, for a given square, uh, you say, okay, what are my, what is, what's going
on in the surrounding squares? That's going to determine an update. There's an update rule that
says given, okay, let's say I'm surrounded by black squares. I'm going to turn black, or I'm
surrounded by black squares. I'm going to turn white. Um, and, uh, basically what happens in, in cellular
automaton is, is you have those rules and then you do this across for a complex grid with, uh, you know,
whatever pattern and then interesting things happen. Um, and, and it evolves. And so you can get,
for example, you can get certain kinds of patterns that recur, um, and a certain set of
stabilities, uh, that can emerge, um, as you run this thing. Um, and the point I'm making is,
so there's a certain sense in which it's like the, the, the, the past leads to the present. Um,
and, and, and it's certainly the case that the, the environment is doing this, this kind of evolution.
But first of all, in the computation itself, uh, the, the rules do not specify what happened,
you know, three moments ago. It's just whatever happened, whatever is the situation right
now. Here's what we're going to do next. Um, in the case of large language models,
the sequentiality, the sequentiality, the, the, is, is built in the, the input, uh, that we keep
feeding. So, so let me just back up. So, right. So the input in the case of cellular automaton is the,
the current state status of the grid. The input in the case of LLMs is the previous sequence,
including whatever I've generated that's tacked on, um, most recently. Now, so these could seem very
similar, like we've got the past led to the, the, the current existence of this grid, and the past
led to the, the current, uh, sequence. But the difference is, and this is fun, absolutely fundamental,
is that in the case of language models, the order, uh, in which those words was produced is preserved
in the current environment, the current input. Whereas in the case of the, uh, cellular automata,
the order is not preserved. Uh, it's just the current state. How did we get here? And that's what makes
it really Markovian. It's just, does, does, there's nothing, there's no information about the past
that's, that's built in to, um, to the current state. Whereas inherently in the case of, of language models,
uh, the, the, the input contains all of the history about the past and the, on the previous states.
So, uh, that's, that's really a certain sense of which, uh, the, the, there's, um, that auto,
these auto-aggressive things, they're, they're different. Then, then the environment, when the
environment keeps track, that's not the same thing. The environment doesn't keep track of
the entire sequence that happened. It's just really just the present. And the claim is that what we're
doing, what our brains are doing is we, we actually are, we have to retain a history,
a sequential history in order to do the next thing. I see. I see. And so I think this is a,
so this is good in specifying where I'm disagreeing with you. Yeah. Yeah. That's what I'm hoping to get
to that. I'm sorry. Yeah. Yeah. No, no, no. This is all been good discussion, but the, where I disagree
with you is that, uh, auto regression is all that there is. And there's just sort of the
entire history with maybe some decay function to drop things out fed in, and then we generate
things. And that's how that's in a nutshell, that's that's there you go. Um, and so I think,
and my view is actually the, the conservative one, um, which is funny with you being the advisor and
older scientists and me being the younger scientists, but my view is the more conservative one. And it's
that there's two systems here. Um, and these two systems are very similar to what a lot of
cognitive scientists have thought for a long time, which is that you have, that's how I know you're
wrong. No, I'm kidding. Which is that you have long-term memory and short-term memory or working
memory. And these are two separate processes. And in some sense, uh, short-term memory operates on
long-term memory. I think where Ilan's absolutely right is that auto regression is the core mechanism for
working memory or, or, uh, sort of cognitive functioning or whatever you want to call it.
The things that go on in your, your prefrontal cortex. Yeah. I, I think they go on in working
memory seems to be kind of broadly distributed, like, well, when you, yeah, prefrontal, maybe,
well, let's say it's, it's, it's, it's controversial, uh, as to what, you know, how, how, if there's like a
coordination, but let's leave that aside. It's a nice abstraction to say this process happens here,
even though it's not true. So we should be clear. Uh, good. Neuroanatomy is, uh, there's this,
one of my favorite papers is called, uh, functional neuroanatomy is modern phrenology.
Let's, let's be clear that it's very, don't get me started, but I think when you're trying to
simplify things, it's still really useful to say this thing happens here and this thing happens
here. And then this thing moves this information from here. It raises my blood pressure a little
bit, but okay. We'll talk about things that way, knowing that that's not how the actual brain works.
And there's lots and lots of issues with that, but it's a useful abstraction, I think. And maybe
that's why it's stuck around for so long. Yes. It's a good point. Right. If it's sticky,
then maybe it's useful. Okay. Yeah. Yeah. I mean, if it's, if it's not, what is it,
I had a mathematician who always used to say, uh, he used to say, well, this is how the universe
works. And then he would say, well, and if it isn't true, at least it's useful. So if it isn't
true, at least it's useful. Or it's leading us down the wrong path, but anyway.
At any rate. I think there is in the comments.
I do have a comment though on this experientially, right? I mean, it's interesting that
an idea pops out to you, a thought pops out of you out of somewhere. Yes. You're not actively
thinking about the thing and it pops up. And I wonder if you wanted to... And that's, that would
seem to be like sort of a challenge to my perspective, right? It seems completely unpredicted.
Uh, it doesn't seem like the previous tokens would, would generate that. But I, you know,
my, my argument would be like says who, um, you know, if you'd spend some time with LLMs,
you can find that they, they can surprise you. Uh, I wouldn't have gone off on a tangent necessarily.
It goes off on a sort of a tangent, uh, that I wouldn't have said that would be the next token
I would have generated. Um, and I, I think the richness, you know, what we've seen and what we
know now is, is that the, the, the, the richness of the autoregressive, autoregressive, um, process is,
is, is extraordinary. Um, and maybe, you know, if you're taking in totality of the, and we're also
adding, by the way, not just linguistic, when you have that, that thought that comes from nowhere,
um, seems to come from nowhere, there are other processes that might be going on at the same time,
as just, you're, you're sort of like thinking out loud or thinking in loud, whatever you call it,
like thinking to your, to your, within your brain, uh, linguistically, but you're also thinking
visually, like it, you know, the predictiveness, it's very hard for us from a metacognition
standpoint to be able to say that, that insight came out of nowhere. Uh, there's some other process
that the argument would be like says who, uh, maybe it is predicted in some way, in ways that we're
just not able at a metacognition, my cognitive level, able to, to, to be able to, to grasp.
But that doesn't mean it's not the same process. I don't think that, that that's a fair conclusion.
Um, what do you think?
So, so what I think is, is that there's, there's two things and, and there's, uh, there's working
memory, which I think you're, you're absolutely right. I think it's autoregression, but I think
there's this other thing. Yeah. Yeah. I think there's this, there's this other thing that's
long-term memory. And, uh, there's a few reasons that it's important if long-term memory isn't
anything like large language models, but let me give an example of what I'm talking about and how
I'm thinking about things. So, uh, there were two people who have been going to my church and in my
friend group for a year that were siblings and I didn't realize they were siblings. Uh, and so I
had talked to one of them and he briefly mentioned that his sister, uh, also goes to this church,
whatever. I didn't think much of it. I don't know everyone. I was like, okay, sister also goes to
this church. Um, they, these two people look similar to each other, but people look similar to each other
all the time. That doesn't mean much. And then I was talking to this girl and she said, uh, oh yeah,
my brother goes to this church. And in that moment, I had three facts in my long-term memory.
Yeah. I had the fact that his sister goes to this church. I had the fact that they look similar to
each other. And all of a sudden I have the fact that her brother goes to this church.
And so how I want to imagine things is those are three facts. And I imagine they're stored in
and something like language, maybe language with, with the graph over the top of it and connections
between them. And when I had those three facts together, I asked her, oh, is your brother this
other person? And the reason I asked that is because like you said, it clicked and, and how
I'm imagining things is through some mechanism, these three things got loaded into the auto regressive
process through some mechanism that I think is somewhat separate from the auto regressive process.
But these three things got loaded into my auto regressive process. My auto regressive process was able
to make a relatively small jump in reasoning about it at that point. And then what's really interesting
thing is the auto regressive process changed my long-term memory because I loaded in three things.
It generated a new thing. And then all four things got loaded back into my long-term memory.
She looks similar to him. She has a brother that goes to this church. He has a sister that goes to this
church. They are siblings. And so you, so you sort of updated your, your, your memory in a,
in obviously sort of a structural way that you feel like wouldn't be well captured
by whatever it is that, that these neural networks are learning by virtue of, you know, churning through
this data and learning to do ultimately next token. Absolutely. And the reason I feel it's,
I think it's interesting to talk in specifics about the reason I feel like it wouldn't be
well captured. So I feel like the whole system is something like, uh, what we call a graph and
a graph is essentially, you have, you have nodes and you have what are called edges and, uh, you have
this thing. And then there's an arrow pointing from this thing to this thing. And that means that this
thing affects this thing and in one direction, one direction. Yeah. Um, and, and so what I talked
about is a great example of this, where I have three pieces of knowledge and kind of I, how I think
of how that's stored in, in long-term memory is almost as a graph where all of a sudden I have arrows
from these three separate pieces of knowledge to this new piece of knowledge that I created with my
auto-regressive linguistic process. And then I go and store that back in working memory. And we can imagine
these pieces of knowledge are connected to a host of other things. And, uh, so I, I mentioned,
you know, they both go to my church. So that's connected to a whole other body of knowledge
about who are the people that go to this church and, and who is the pastor of this church and
where is the church and, and space and all sorts of other things. Right. Um, and, and there's all these
arrows pointing from all of these sort of small beliefs to other beliefs. And this is really important
if this is a graph, because, uh, one way to think of large language models is that
they're a hyperdimensional space. I'll get into what I mean by that. Essentially, there's something
you can do with a lot of things that's called, uh, embedding where you take something that's
structured in one mathematical way and you put it into a different mathematical structure. So,
uh, everybody's seen a map of the world. Uh, everybody's seen a globe, a map of the world does
not look like a globe. It's flat. It distorts certain things because you have to, if you're
going to flatten out a sphere. Uh, but one way to imagine that is that you are embedding a sphere
into a flat plane. And so you're, you're sort of translating each location on that sphere to a
location on the flat plane and you can sort of, you can still perform a lot of operations that you
could perform on a sphere. So you could say, these things are this far apart and these things are this
far apart and you don't need the sphere anymore. You've embedded it. So there's two types of ways
to embed things. Uh, there's what's called lossless. So that means I can do everything perfectly just as
well as I was able to. And then there's what's called, uh, lossy embedding. So I mentioned that the
distances change. If you turn a sphere into a flat map, at least the normal maps, there's special
maps that are weird shaped and you can perfectly preserve distance with, but most the map you've
probably seen, the distances are changed. So unless you, unless you know what the original transformation
was, the information is actually lost. The distance, the correct, the original information is lost.
And if you want to get a ruler, you can sort of measure here's this location, this location,
here's this location, this location. And usually the way flat maps work is it'll work on a
small scale, but if you try to do like, here's California to Iceland and here's California to
Germany, it'll be way off. Um, here's how big Greenland is. Yeah. Yeah. And, and so, uh, so exactly.
So, so this is lossy and you can also embed the kind of structure I'm talking about, which are graphs
into hyperdimensional space, which are the type of structure that neural networks are based on.
So neural networks are a series of transformations in hyperdimensional space. I won't get
too into sort of what hyperdimensional space is because that's another conversation, but just
imagine graphs are a sphere, hyperdimensional space is a flat map. And to translate one into the other,
you have to lose information. And actually, if the graph is very complex, you have to lose quite a bit
of information. And so the case I'm making is that the knowledge capabilities. So like if you go into LLM
and say the cat is the opposite of a, the LLM will say a dog and that appears to be knowledge.
I would say that the knowledge capabilities that we see are the large language model having
a lossy version of the kind of knowledge graph I'm talking about in its hyperdimensional space.
But because it's lossy, we also see these kinds of issues at the edges, the same way I mentioned where if
you're using a map normally and saying, how do I get from Florida to North Carolina,
it's going to be fine to just look at the distance with the ruler. But if you're trying to do other
things with it, it'll fail. If you try to do some of these other things with large language models,
they'll hallucinate, or you can get them to stay completely different things depending on the kind of
conversation you have. And my argument is that these failures are because of the lossy embedding of a
graph into this hyperdimensional space. And it's because the large language models are trying to
do something that's not natural to the structure that they have. So they're doing it. And they're,
if you get, if they're large enough, I mean, there's many, I think we've reached as many neurons
of the human brain are close to it. So if it's large enough, you can really get really far along
with sort of saying, well, this isn't quite how I work, but I'm going to truck through it anyway.
Large enough and also have enough data. I mean, they're exposed to ridiculous relative to humans.
So there's, of course, you know, very, very significant differences right now. So this is
actually a point. And I remember the point I wanted to come back to earlier about things like
hallucinations. And in general, whether the current state of the art and the current performance of
these models points to a difference in kind or is it, or is it just, are we some fixes away
and we're on the right sort of theoretical conceptual path? Or you're making the case that no,
there's something radically different going on in the case of people. And I think this is
sort of the core discussion here. You kind of, you sort of detect in the kinds of mistakes they make
something that's extremely not human-like. And I think anybody who spends a decent amount of time with
these elements will have those experiences where you're like, that's not just like you're missing
a fact, which people do all the time, or maybe, um, even contradicting some belief that you say you
had, but now you're saying something, maybe that's not consistent with that belief. But it seems like
there's like, like, who was I just talking to? You just said this. Um, I said, you know, don't,
don't repeat this. And then it repeats the same thing. Like, don't, please don't repeat that. And it just
repeats the same thing. Like, what's, what on earth is going on? This is an extremely unhuman
type of thing to do. Um, I mean, it's, it's not the best example of, of the kind of things you're
talking about. But I think this is sort of where we're at, um, in sort of the, the, the empirical
landscape is we certainly do see these kinds of this dissonance, uh, these, these, these, uh, cases,
some things like hallucination. Now people hallucinate too. Um, but is it a different, it is a different
kind of hallucination. I think most of the time they realize they're hallucinating, which is really
key. So, I mean, I know people who make stuff up and they kind of seem to believe it, uh, in some
sense, uh, that, that almost looks like it doesn't really matter what their actual past was. Like, this
is the, the network is generating this now. Um, and I don't know to what extent, I guess that's, that's
where I'm, you know, I, I think, I think there's a lot of room here for making more precise, specific
claims. And maybe you, maybe you've done that, or maybe you can do that, um, about what kinds of,
excuse me, of hallucinations, uh, exist in, in the networks, uh, in, in the, in the models that, that we
would never see in humans. And they're just of, of a different point, type where, or what kind of
contradictions, um, you know, things like, now, you know, these three people are, they're siblings
or whatever, uh, you know, that realization, and that sort of percolates to the rest of, you know,
how you're going to talk about these people and how you're going to think about them. Maybe you could
argue that, that, you know, large language models don't do that. Uh, they can learn, you know, you
train them out on some new facts. Does that successfully percolate in the way that you would expect
that in your, in your system, there's, there's sort of hierarchical knowledge graphs. Um, and then
that percolation sort of maybe comes for free is like, there's a central node that's connected to
a lot of other nodes. You update that node and it, and it, and it changes the, the behavior more broadly.
Maybe the claim would be that we don't see that kind of thing in, in the, in these models. We haven't
really talked much about what it is that they're learning. You know, if auto-aggression aside, what
happens in these models though, is that they are churning through enormous amounts of data. And what they're
doing is not memorization. They're not, and that's, they're not regurgitating. People have
this misimpression that what they're doing is predicting the next token based on that, the fact
that they've seen this sequence before, that's not how it works at all. Language can't work that way
because every single sentence, the combinatorics are too big. Our, you know, previous 10 words were
probably never uttered in the history of humanity, unless you're reciting this, the pledge of allegiance,
you're not going to have the exact same. So they're not simply regurgitating what they've seen.
They are doing something much more similar to this. So there's this embedding that's happening
across many layers, and they really are abstracting away, for lack of a better word,
what these tokens, sort of how you represent them. The word doesn't, it's not just the word. And I kind
of alluded to this a little bit earlier, right? And the word, any word that you use within the model,
as it goes into the model, it doesn't just remain that token in this sort of very, very base sense.
And it's just trying to guess based on, it's a very important point. It's not just trying to guess
based on the pattern of these tokens per se, it's turning these tokens into this much richer and
embedding. And with the transformer models, those tokens, the word, the way it gets represented is
combined with all the other tokens. And that's really, that's what attention is. The ultimate
representation of any token, any, any piece of the sequence is combined with all the other members
of the, all the other, the other tokens, and it's sort of structured that way. And so it's not
this dumb system that we can just easily conceptualize and say, it's, it's just taking
this pattern and it's producing the pattern. These are hyperdimensional representations of the
original pattern in such a way that, and when it's learning, it's doing that same thing. It's not
learning the sequence of tokens, just like that, just the way they appear. It's learning them in
this much, rich, much, much higher dimensional, much richer and, and combinate, I don't know what
the word is, combinatoric structure. Again, the idea that the, that it's structuring the, the
individual tokens as combinations of the other tokens, this is all built in to the way it's representing
that information, which is why it's really doing conceptualization. It's not doing
what we think it's, you know, at the simple level, it sounds too simple. Does that end up looking like
a graph? In some sense, there, it is sort of a, it's, it's a directed graph as you go from layer
to layer. But I don't think that it captures it. I think, I think we are, I think we have a disagreement.
Maybe we're closer than we would think, you know, in terms of what it is it's doing when it does that,
there's hyperdimensional embeddings that it ends up and, and, and it is, again, sequential
across layers. It may end up being captured, you know, sort of with, with a kind of graph
theoretical framework. But I think we're disagreeing on whether it's sufficient. The way the LLMs are
doing it will ever be sufficient. This is a good word. I think in technical philosophical terms,
you are saying, we both agree that auto regression and this kind of process that large language models
do is necessary for cognition. Certainly auto regression. But what about the, what about the,
you know, the training of the model, uh, where you're set, you're determining the weights in the,
in the, in the, in the transformer model. Um, that's this embedding, right? There's embedding
plus, plus, uh, self-attention. Um, but the embedding happens with self-attention. It's churning
through the whole thing. So it's doing the embedding such that it's going to be good for
the combinatorial self-attention mechanism. This is very, very, very abstract, complex stuff that's
going on. It's not just, you know, simple, uh, again, such pattern match. Could that be sufficient
to capture the kind of knowledge representation
that you're rightfully pointing out? Certainly we do. Um, now, does the fact that there are deficits
now in the current system say that we're, no, it's doing something radically different? Um,
you would say maybe make that case. You have other ideas of how you can do it computationally
that you're, you say are inherently absent. First of all, are we sure that whatever it is you're
envisioning isn't actually sort of epiphenomenally happening in the, the entire embedding,
you know, layered embeddings and all of that? Well, I, I think I'm actually sure
that it is happening. I just think it's happening in a lossy way. Okay. Um, and so,
well, so maybe our, so maybe our, our point of contention is, uh, it may not be that large. So,
so what do you mean? So lossy as opposed to, I think there, and there's sort of, there's all sorts
of interesting things about how you, you can represent each neuron as a hot field network and
other things you can transform neural networks into. But I think, uh, I think what I'm saying is that,
I think you have to have some kind of knowledge graph structure of, for memory and knowledge
and things like that. I'm saying, I think the large language models have some kind of
knowledge graph structure, um, sort of embedded in the neural network. But what I'm saying is,
and, and by the way, the reason we keep talking about deficits, we're talking about really clear
ones like hallucinations, but some of these deficits are like fundamental reasoning capability.
So this isn't just the academic pie in the sky conversation. This is like,
you know, there's reasons our, our robots aren't smart and can make mistakes and shouldn't be
performing surgery or not. This is an important conversation. Very important. Um, it's like,
maybe the most important as far as these models go is like, is, are we getting to like utility where
they can do the stuff that we want them to do reliably? I agree. That's, it's a very, very fundamental.
Yeah. Yeah, absolutely. So, so I think there are, so I guess what I'm saying is there's,
there's two systems and, and, and they're sort of the more graph based system. And then there's
the auto regressive system and we've built auto regressive system. That's large enough. And that's
been fed in the entire internet that it's sort of bodged together. The second system that it needs
inside of its hyper hyper dimensional embeddings. Um, and, and so it's, I think somewhere in there you
would, you would find something like a knowledge graph that's embedded in. So, so I want to stop
you real quick. So, you know, when we say it's an auto regressive system, auto, so there, and it goes
back to the software kind of a point, maybe not exactly software, but short term memory and long term
memory, the, the training regimen that it is trained to do auto regressive, to do auto regressive next
token generation. But in the process of doing that training, it is forming this very, very, very,
very rich, hyper dimensional, all of that, uh, you know, sequential, uh, you know, we could even
say graph, you know, it's, it's, it's a graph, um, representation of all the information in order
to do the next. So the auto regression is sort of the task and it's the, it's the, it's the,
it's the environment in which it produces, it's able to perform its behavior. Um, I think what it
learns, it learns to do the auto regression, which is, which is, in other words, it is fundamental,
but at the same time, that doesn't, that doesn't limit. That doesn't limit. Yeah.
Every, everything you just said about these are doing something that's not just next token
for it. It's something we agree on by the way. Well, no, but, but I'm saying it is just learning
to do next token generation, but to do that, it's got to learn about the whole, it's got to learn
about the world. I see what you're saying. And so it needs, and, and so, and we, we see that it's,
that's why it's so, it's so surprising on some level. Um, I, when I first, I kind of remember
when I first like, that's, I learned that that's what they're doing. I was like, that can't be right.
Like, I thought somebody made a mistake. Uh, next token, what do you mean? Like, how could that
encapsulate, uh, and, and, and contain within it all the information, you know, how could that
be the, the, the key? Um, but the answer is that in order to generate next token, you've got to
organized, you've got to organize your knowledge of the world sufficiently that you end up learning
concepts. You end up, I mean, it's still just next token generation is what it's learned to do,
but that, that the, the train network, which is why, so the brain, you know, sort of the,
I think it was like, uh, you know, long-term memory is just the, the sort of the, is, is, is the train
network. This is how I think of it, right? The train network is, it's a set of weights, right? And the
set of weights at this moment is just some system that if you give it a certain input,
it'll produce an output, a single output. That's the brain. It's a strained network.
The process of cognition is that trained network in an autoregressive environment
so that when it produces the next token, that's fed back. And then it, and then it proceeds from
there. I see. But I'm not really making a strong, well, I am making a strong statement about how the
brain gets to the state it does, but the state itself, the, the weights can, can contain within
them all of this richness that you're talking about. So it's not a, not a limiting factor to
say that it's autoregressive. That's the environment in which, which it produces and that's what's
trained to do. But what you get in the end is a very rich, uh, you know, uh, representation of all
of the information that has been fed in order to, to, to learn how to do that next token.
So I think we might actually be agreeing here. Are we going to hug at the end?
I think we might be agreeing here, uh, which is that in the process of doing the next token
prediction, it learns to simulate the whole system. And what I'm talking about, and what a lot of
cognitive scientists besides me, by the way, would say is a sort of a, almost a two systems
that interact with each other. It learns to, to have both of those systems, sort of the,
the generative part of the system. And then also here's a stored knowledge graph and here's stored
knowledge graph. So I'm saying there's one system, right? I think I'm claiming that, that it's one
thing that just operates in, just like Dell operates in this autoregressive environment.
Um, but you're saying, you're saying there are two systems. Are we disagreeing? Let's, let's get to
the point. So, so what do you mean by two systems? Transformers are universal function approximators.
Sure. I'm saying there's two. Neural networks in general. Yeah. Yeah. So, so universal function
approximator just means if you have a big enough one and enough training data and enough training time,
you can approximate any function. So it's a Turing machine or whatever. Yeah. To an arbitrary degree of,
of error. Um, nobody thinks the brain is a tape with the lead head moving back and forth. You know,
the fact that that's, I, I find that a curious, an interesting point, but I'm not sure that,
yeah, but, but go on. Sorry. So, so what I'm saying is at its heart, cognition as we think of
it is a two system process where you have one system that's sort of autoregressive generative
in the same way that transformers and, and transformers are able to do this very naturally
because fundamentally that's what they are. There's a second system, which is a more static system.
It's something like a stored knowledge graph. And then a lot of what we think of as cognition,
it's taking things out of the second system, putting them into the fort first system,
doing some transformation and putting them back into the second system. And so you've almost got
like a warehouse and then you've got a guy that gets boxes out of the warehouse and he works on
something and puts the box back in the warehouse. Got it. Got it. Okay. And so what I'm saying is
I think because transformers are universal function approximators, they've managed to capture both
systems to some degree because they can capture graphs and you can loss lossably, but you,
uh, you can embed a graph into a hyperdimensional space, which is what transformers are. But what I'm
saying is that they're not capturing the second system, this graph based system very well.
Hmm. And that the fact that that's why they make the mistakes and that's why they make the mistakes
they do. And that, um, the second system is really, really, really important for a lot of advanced
reasoning capabilities, especially reasoning over very, very large amounts of information. So I think
a perfect question that it's really hard to get large language models to do, even if you hook them
up with rag, which in some sense, if you do that real quick, let's rag rag is where you,
by the way, I think this may, might be a point towards me. Rag is where you give the large
language model a second system that stores information. And there's a lot of technical
detail there, but essentially you give it a second system that stores information and you can take
information out. It does. I'm working on that. I certainly can't, I'm not going to hold you to
account because rag and its current incarnation, like, you know, it's like six months old or a year old.
I don't know. Uh, like I'm not, I'm kidding. Uh, that's not really a point for me, but it works
better for a lot of things than a large language model by itself, I think. Um, and, and I think
that's the important statement. So, so rag is where the real case, but let's, let's, I think that's a
great point, um, to, to jump off of a little bit. So why do you need rag? And the reason is, um, there's,
there's two ways to get information sort of into, well, now there's, I guess there's three,
uh, right. There's, there's three ways to get information into a large language model.
You can train it, which is me updating the weights. That's, that's sort of the brain that I was
talking about. That's sort of the static system, um, by virtue of, of, uh, here's, here's, here's some
text. Here's what you should generate based on that text. Uh, you know, and it, it, it just,
like any neural network, um, it will try to guess based on, you know, here's, here's the Pledge of
Allegiance. It's never seen it before. Um, what should the next word be? Uh, you know, I, I pledge
allegiance to the, it's never seen before. It doesn't know. I pledge allegiance to the God.
I pledge allegiance to the right. But then you say, no, it's flag. Uh, and now it's learned it.
And now next time it comes through, uh, it will, it will get the right output. Um, so that's training
and you've changed, changed the weights. The second way is, uh, you can do it in context, meaning, uh, when
it's having the conversation, it's going to learn, it's quote unquote learn, but it's going to go into
the, the, the, the, this context is going to go into its inputs as it's going along. That's how you
can tell a large language model. Um, hi, my name is Elon. And I'll say, how's it going Elon? Um,
it's not that it's put it, that name, it's not taking, uh, in, in, it's not taking that fact and put
it in some other stored knowledge base. Um, it's just in the context. Uh, and it's, in other words,
it's just part of the, the, the, the input that it's going to keep, um, auto aggressively going
through. Uh, that's number two. And then number three is kind of the stuff that, uh, Daniel's
talking about where you can have a, a, a, a knowledge database that the large model can try
to access. Now it's the auto aggressive system isn't really doing it. You have to have some other
process and retrieval augmented generation is one of them. Um, augmented here is, so the generation is
just auto aggressive generation. The augmented is that, um, is that you're, you're taking the,
the, the, the current, uh, you know, whatever the, the, uh, input is and you're saying, based on that,
I'm going to go look up and see what might be relevant in some other database. And it's just,
it's a distinct separate process. And I think this is the key. Uh, and so it's why I elaborate on
this so much as sort of the key of the, of the debate. Um, is there knowledge stored externally to
the sort of train model and then the train model goes and retrieves it, or I don't even say the
train model, it's not the train model retrieving it. There's some process that goes based on
whatever's going on in the current, uh, you know, conversational context or thought context says,
let's go retrieve information from that other thing, pull it in and use it, right? That's what
retrieval augmented generation does. You're saying, yes, there is this other database that has to be
accessed. I'm making the, you know, sort of, it's maybe a devil's advocate argument, but I don't know,
uh, that, no, that doesn't need, you don't, it's all a train model and that what knowledge
representation is, it's when we, when we, you know, consolidate and, and, you know, when we're
dreaming and sleeping and whatever, uh, all of this is, is just a way of training the model. It's much,
it's, it's really just a training process rather than an external separate. Now, do I think that
training process leads to exactly what LLMs do? That's not the claim, right? In other words,
could it, I don't know what the structure, no, and it's not a claim that the particular transformer
structure is the right, is the right drivers, but in the end, it is a substantive claim about
what you have as a trained model rather than something that is a train model plus an external
representation. And that I think is the point of, I think that's the crux. That's the crux.
Is it a two system or one system process? Because I think we both agree that that system,
the autoregressive system is important. I'm just, and very rich and complex and, and has sort of
conceptual knowledge, right? Yeah. Yeah. All of that stuff. Okay. Yes. Yes. I think we completely
agree on that point. My point is that you still need this other thing. So the autoregressive system
is necessary, but not sufficient. After an hour of talking, I think we figured out what we were
arguing about. That's fantastic. So let's scrap that whole thing and let's start from there.
It's record time for us, by the way, but. Okay. Now, now we, now we have to,
you have to fight about it, but, um, but honestly, I don't know that I have that much more
even to say substantively, um, about who's right here, except is maybe a call an article of faith.
To me, it seems like the brain is just one thing and it's just a big fat neural network. And I think
that given that we, we have a glimmer at least of a system that seems to do the whole knowledge thing,
maybe poorly right now. Um, but well enough that I would say, I'm going to go out on a limb,
on a sort of theoretical limb and say, let's pursue the theory that it's just single system.
And then I put the ball in your court and say, no, you know, you have to give me the argument as to
know everything I see, you know, tells me that, or there's, there's substantive reasons to think
that it's the wrong direction. By the way, Alana is saying that the brain is just one thing
because neuroanatomy as it stands sucks. And the divisions we make in the brain don't work very
well, but it's worth mentioning that in my opinion, this is also something we disagree on. I think the
brain is, is a few different things that have sort of, and I don't really disagree. Of course there are,
there are, you know, nuclei, there's these clusters that do stuff. There's peduncles that are sort of
highways between the separate portions and correct like these. And so, uh, it's worth mentioning that
Alana's making a controversial claim there, even there that the brain is just one big ball of fat.
Well, I mean, it is, except there are, you know, it's just that it's topology is not simple. Uh,
meaning you can, you know, you have dense, closely, highly connected regions, and then like you said,
sort of pathways to other regions. And so the topology, you know, it is a network.
Right. It's sort of, it all is connected one way or another.
Correct. And there's, there's certainly more, you know, directionality, like things go
one way, not necessarily the other. And so, you know, there, there, it's, it's a graph and all of
that. Um, so again, I don't know, you know, that, that claim isn't really a controversial one.
It's really more conceptually. Does labor get divided up in such a way
that this model of just next token generation, even though it's, it's recruiting all these different
systems isn't really an account. Like computationally, it's not the right account.
So biology sort of aside, like, yes, it's, it's almost hand waving to say it's one big network
or it's not, um, you know, even, even large language models. Also, there's, there's all kinds of,
I'm sure fascinating topology that happens sort of semantically, even though it's all just,
you know, the connect, but the connections get zeroed out. I mean, a lot of it's just not,
you know, meaningfully there, there in, in some ways that things are not always uniformly connected.
And, uh, there probably is interesting topology there too. Um, of course it's not as simple as
just saying, it's, it's all just one big brain. Um, I'm not making, you know, the sort of the, the,
the, uh, very simplistic version of that argument, but computationally are these things being recruited
in this, in a way that's captured by what, uh, you know, the train network doing the next generation
or not. And you're saying, no, uh, there's a different computational process. It has to be
described differently. Uh, it's not a multifaceted train network. And you have to have this other
process. Right. Right. And I think that's sort of our core disagreement. Um, it's worth mentioning
your idea is more parsimonious. It has less moving parts. That's why I think that until you're proven
wrong, science is really on your side. My intuition is that this is wrong, but as, as a scientist,
I have to go out and prove it. And I'm working on that. Well, I mean, but on the flip side,
you know, you can point to deficits in these models as you know, and, and it's just, it's just,
it's, let's face it. It's sort of subjective. It's kind of a gut shot, gut call. We don't know how
to do science on these things yet. Not even close. And when we're in an absence of, you know,
really any science of it, what we can say is like, I, I could, you know, I could, I can make
the claim that it's just a matter of time and, and, and improvement. Um, and you could say,
no, uh, it's never going to get there on this current trajectory. And we can't really resolve
this except empirically. And that's why science is cool because like that, but I think, I think
it opens the door to like, this is an interesting debate. And I'm not just self-servingly saying,
yes, this is an interesting debate. Like, I think this is something that, you know, research,
uh, hopefully can, you know, certainly on the, on the company, on the, on the engineering side,
people want to solve problems like hallucinations. And maybe you've, you have insights that can solve
those problems. And you're saying, this is an engineering solution. I'm also saying, and we're
both saying, this is also, I think a question that has potentially very large importance to
understanding humans and our brains. Um, and I think those are both true, uh, in the sense that
this, this both presents a operational practical question and also a theoretical, you know, basic
science question. Um, I think we both need to, uh, we need to go back and get into the lab.
Yes, we do. And we're in the same lab. So we're going to pursue, uh, you know, opposite, uh,
opposite. And that's perfect. I think that's great. That's science. Yeah. We, we both, uh,
go poke at reality and see who's right. That's what science is. I think if we, you know, the next
conversation or more of this conversation is what would be sort of, what can we do now besides just
say, let's keep trying to improve on, you know, in the current models or you see radical departure,
what can we do now in the, in the current space? Like what would be good tests of, of sort of, uh,
your theory or my theory that we could say, uh, you know, that really lends evidence. Like,
you know, I, I'm not, I'm not claiming like, uh, that if we just keep training them on the, in the
current, uh, you know, with the current architectures, although it may be the case,
you know, maybe, you know, we haven't seen GPT-5. Uh, we haven't seen what these things look like
with the scaling laws may end up solving the hallucination problem. Apparently, certainly
got a lot better. Uh, hallucinations have gone down a lot with, with newer models. Um, maybe
I'll just sit back and be like, let's see. Um, but are there things we could do now experimentally
where I would say, look, if we fine tune it, you know, here's, here's some new information.
Uh, we're going to give it to the thing and we're going to train it on that information. I would expect
according to my theory for things to work that, but according to your theory to work out differently.
Um, you know, like your case of the, the, the siblings and stuff, is there something that
your theoretical framework would predict, uh, in the case where you give a certain information,
uh, and you fine tune it where it would, it would, it would, you would predict a hallucination
or you would predict a kind of a mistake because it doesn't have access to the database. Whereas I
would predict, no, it should not make these kinds of mistakes. Yeah. We should have that conversation.
Yeah. Yeah. I wish we could do that for the next couple of years that we, oh my gosh,
that would be great. Okay. And in a dream world, if anybody wants to, uh, anybody wants to write
a check to support. Yes. Uh, Daniel's, uh, next couple of years of graduate school, uh,
then we can focus exclusively on this. That would be helpful. Yes. Yeah. Please do.
You know, it's, it's, it's gonna be, I think it can be astonishing to realize, um, this is it,
that this is, this is what's doing the, all of that stuff. Yeah. And then even more astonishing
if you, if you buy that, that's what's even knowledge aside, right? That's sort of at least
conversational language use and stuff is like, is, is enough. But yeah, I wanted to get into working
memory because, cause, uh, I don't think it's a memory at all. I think it's just auto-aggressive
loop. I agree. Actually, I do. Yes. You want to write a thought paper on this? Cause I, I, I,
I've been meaning to, I want to say working memory is not memory. I want to, I want to put out a paper
basically that now we know that, that like, I think we, that this is a, um, this is an insight
we now have that auto-aggression works so well in the linguistic case that there's no reason
whatsoever we should posit that anything besides that is happening. Um, and that the reason why you
can recall stuff is because from the, is because you, because it's not Markovian because you have to,
so the brain has to encode the previous few sets. That's the state has to, it has the sequence.
And, and that all we're doing when we see, when you do this working memory experiments
is we're just tapping into the fact that it's there, but it's not for the purposes of retrieval.
That's where the cognitive science got it wrong. They thought this is for retrieval. It's not for
retrieval. It's just the fact that there is activation. There's, there's, uh, that in, in the brain has
to retain that information in order to do next token generation, right? But it's not so you can then
repeat it. That's like, uh, that's like an epiphenomenon that happens to work. And then
stupidly, I'm sorry. I mean, I, I think it's dumb. I think they're like, because nobody ever gave a
good explanation of why you, why would you do it? So you can remember a phone number. That's what
that's, that's why we evolved this. That's how this, this, this core function, there's a core
function of our brain. That's maybe the core function, maybe the core function, right?
The cognitive core function. I mean, breathing's good too, right? All that stuff. But yes,
so the kind of core function is, is you can remember a phone number that seemed that always
seems to suspect to me. Uh, like it never was clear why you'd want to go and retrieve
like explicitly retrieved. So I don't think the explicit retrieval is the point at all.
I think it's a, it's a indication. It's good evidence that we do retain the previous sequence
up to some extent. I think the, the drop off in the drop off in accuracy in retrieval,
I don't know what it means necessarily because the explicit retrieval is not the point.
So I don't know if you really are forgetting the sequence, uh, in terms of like for, for next
token generation, or it's just that the retrieval process, this explicit retrieval process,
which is some sort of weird clue like that we're able to do, but isn't really the core function
may just drop off. I don't know. And it's an important question for me because
can we use all of the, is all of that cognitive science useful? I don't know.
Like the drop off is an important fact, but is the, the nature of the drop off and the,
I don't know if that really tells us anything about like how, how, uh, how, how the, the
sequence, the context is being updated, like, you know, and being degraded. I don't know.
Um, I'd like to think that to 50, 60 years of work, I'd like to think that we can go back to
and be like, Oh, now we have insights about how the brain is representing the sequence
for the next token generation. Um, but I'm not convinced.
You'd be really interested. My kind of mentor is an undergraduate, uh, Dr. Anders Ericsson. He
worked on a bunch of the really foundational work on working memory.
Really?
He's an older guy. His, his, uh, his mentor was, his advisor was Herb Simon.
Cool. And so, uh, MIT. Yeah. Yeah. Yeah. The kind of cognitive science. Yeah. Um, and,
and so he did a bunch of the foundational work and I remember I talked to him when he was older,
obviously, and he was really disappointed that people had adopted the ideas that him and some
of his colleagues came up with so well. He said, we were scientists. We were coming up with, uh,
the best ideas that we had of the time because that's what you have to do with scientists.
And he said, and, and I'm proud of that. I don't have, you know, you know, I don't have,
but he's not so proud of, of the people who came after like,
they should have done better. Yeah. Who he told, Hey, here's a theory. And they said, Oh,
okay. That sounds good. And, and he was really, he said, he's, he was really disappointed that in
like 30 or 40 years, since he had come up with these ideas, nobody had come up with something
better and really poked holes in his idea. I think the big problem for cognitive science,
a real scientist, by the way, that he was disappointed that people weren't just,
no, I love that story. I think that's awesome. And, but, but I think the problem with cognitive
science, um, certainly at least the cognitive psychology and of cognitive science, uh, is that
you're, you're sipping through a straw, like the, the kind of data you can collect, it's just so
constrained and limited that what ends up happening, what I think ended up happening is people built
theories and it's sort of like looking for your keys where the light is. People built theories out of
what they could, what they could observe and what can we do. We can read people's sequences and they
can recall those sequences and you can measure it and it's highly measurable. And we can like,
you know, there's, there's empirical data, you know, this is the, the behaviorists, uh, kind of,
uh, impact, you know, don't tell me about your highfalutin abstract theories. What can we measure?
And, and, and they were constrained by what they can measure. And you end up with something like,
there's a core function of the brain to retrieve, because we can see that people can retrieve,
but maybe it's just because that's how you're able to, what's something you're able to measure
is retrieval. And maybe that's not even a core function at all. That's, that's sort of my claim.
And I think maybe this is true for a lot of cognitive psychology that you end up, unfortunately,
because of the limitations in, in the kind of data you can collect, you end up building theories out of,
uh, that very, very narrow, limited kind of data. And I, you know, I don't know that there was a better
solution. I think now we've got computational models that do, that are able to do this stuff.
It's a different way of doing, you know, cognitive science always was supposed to also be doing,
but the computational stuff didn't work. You know, the architectures, the, the AI architectures,
good old AI, um, all of it, uh, uh, to some extent didn't quite produce the connectionism,
parallel to PDP, whatever ended up becoming neural networks,
turns out to have worked, um, to, to do some, some very significant portion of, of,
of what we call cognition. Um, and you know, I guess that argument could be like, that's okay.
So, you know, the, the, that, that was necessary. Everybody had to pursue these different,
but I, I don't think, uh, I, but I, I guess what I would say is like, I think there's textbooks
filled with, would have probably including my own textbook filled with wrong information,
uh, making claims about boxes that maybe don't exist, um, in these, you know, so long-term memory,
short-term memory, working memory, or it was working memory as sort of a, a memory system.
I think that needs updating. I think maybe the, the conclusions that were drawn need,
need to be updated, um, based on new, new facts as, and have come to light. Uh, new information has come
to light, um, that, that call into question whether these, these were, these were inaccurate. There's,
there were actually mistakes made. Um, I don't know. Yeah. Yeah. I think, um,
yeah, I think there are boxes, but because large language models are a giant universal function
approximator and because they have all of this data, they managed to simulate those boxes within
the large language model. And, and so, right. Our debate is whether it's one box or multiple boxes,
but you may agree that the particular boxes that cognitive science has arrived at may not,
may not be accurate, but, but I think, uh, you know, we, large language models are really smart
and they're very impressive, but if you gave me the brain a size of a warehouse and gave me a
thousand years to read the entire internet, I could beat the pants off a large language model.
Um, yeah, but they're going to get more, if they are getting more efficient all the time,
there's some spectacular much, much, I mean, compared to the number of parameters you needed,
uh, you know, a year ago. Um, and, and some of these lighter, um, and by the way, uh, open source
models that are like a few billion parameters, uh, they're, they're much smaller and they're,
they're beating benchmarks. So maybe we'll get to much more efficient. And the training regimens also,
it hasn't been much of a, an area of research. I mean, at least I don't know what's going on on the
engineering, on, on the industry side. I think there's potentially a lot of work that could be
done, um, in training these things in a more sequential way. Like right now, you don't, we
don't do it the way we do with humans. Maybe they should start out with simple stuff and then build
up to more complex stuff. Maybe that, that would lead to, uh, you know, a faster and you don't need
as much data, but, but the computer is cheap enough and people don't really care. Um, like if we get to
that model is the only care about inference, uh, you know, if everybody just wants to get to the
model that, you know, the best model and they don't really care about, you know, they're throwing
ridiculous amounts of money and, and compute. And then once it's done, it's done. Uh, and they're like,
okay, uh, you know, uh, they're not worrying about the, the, the, the right now they're just scale,
scale, scale, scale. Um, and, and it may, but in terms of explaining and trying to replicate what humans
do, which doesn't require, you know, what, what are the equivalencies like thousands,
hundreds of thousands of years you would need to spend as a human being going through all this data.
And, and of course that's unrealistic, but from a scientific standpoint,
maybe the engineers don't care. Um, but thinking about how people are able to learn so much
and get so good with so much, so much less data, of course, is a critical question. Um, both for just
language use, but knowledge, all of this stuff. Um, and again, maybe you would say that it's because we do it
completely differently. I would say maybe not a difference in kind of, you know, quality,
qualitative, just a quantitative difference or, or, or, you know, a better model with a better
training measurement. Maybe we could end up doing something like we see with humans. And there's also,
we don't know what genetics is doing. We don't know what the structure is, like how much knowledge,
knowledge may be in there to some extent. I mean, you know, I don't want to quote Chomsky too hard on
this, but, and cause I, I think he's a guy, I think he's wrong. And I think, I think grammar,
syntax, it's, it's wrong. It's like, well, now we know you can do, you can do ground,
you could do language production without syntax. You don't need, it's nothing to be explicit.
It's not in there. Um, it's just not in there. It's implicit, but that's, that's epiphenomenal,
whatever we can get into that, but he is right in the sense that there's not enough data.
Uh, the children don't have enough data to learn the rules. So there's some sense in which perhaps
these rules, remember they're just weights, right? Whatever is the, when the model's trained,
you're just training the weights, maybe in the process of, you know, with genetics,
plus developmental environment, plus, you know, stuff that's happening in the womb and all this
stuff may give you lots of scaffolding that makes you ripe for, for language. That's, that's not,
that's before you ever truly encounter language. Um, so it's not, um, it's, it's, it's, it, there's,
there's room for, for lots of other stuff besides what we, we think of classically as, as training a model.
There's a meta point to be made here about AI research, which is that all of this work may
already be going on, but AI research currently, because it's the biggest economic thing in the
world is in the same place that finance research or defense research is, which is the really good
stuff doesn't get published because you don't want other people to know about the really good stuff.
Right. And that's my deep seated hope and fear at the same time. Like, uh, maybe we'll never know
about it. Maybe it's just like, boom, my model can run on a thumb drive and I'm not telling you how,
it's like, and it's able to do all this stuff and it, you know, you can put an edge and it works and
we'll never find out. And it turns out you solved like the problem of cognition, you know, in a deep
way, like that, that tells us about humanity. And we're like, we're never going to find out.
Because it makes enough money. And, and, and absolutely. And, and as much as open source and,
and, and all of AI and then the progress has been a product of the fact that there's a tremendous
amount of, of sharing and collaboration that the open source, uh, kind of ethos has been huge and
critical. Um, but let's face it, open AI is not open. Uh, and then we don't know what their models are.
We don't know how they work. Um, we, we can only try to draw certain very coarse inferences
about their scale. Uh, you know, you could try to follow as some people I know do like
GPU orders, how many H 100 is anyone are shipping and things like that. But you don't actually find
out what tricks they're using. Um, and I agree with you that they're very well, maybe, um,
these kinds of things are happening and insights are taking place, but, but science for science's
sake and basic research, you know, I don't, I, there's not really an incentive or motive for
the, the industry to care how humans do it on our wetware, which has all these other constraints
and, and it doesn't have to operate in the same, under the same conditions. Like maybe they just
don't care. Yeah. Um, if there is economic incentive, that would be other concerns that I would have,
but. Oh, well, let's not get into it. I'm sure. But you know, let's take the, there's tricks,
there's secrets that the brain has and they want to steal them. But frankly, their job is not
to figure out how the brain works. Um, and, and to, and therefore, if there's a direction they
can go in that, that, that just seems to yield fruit that doesn't reflect to other brains doing
it, they're not, they shouldn't care. You know, they, they, they, you know, to use the modern parlance,
they're, they're beholden to their shareholders, right? Uh, although open AI is not publicly traded,
but they're, they're, they're the organism. They're beholden to Microsoft. They're, they are,
they're very much beholden to Microsoft and other, and other investors. That's true. Um,
and they're beholden to their organization, right? They, they have their, they are, they
are, they got to stay up front and at the top of the pack and they have to do whatever they
take and they, they have to put all their resources into that. They can't be worrying
about, well, is this how the brain does it? But what about humans? They don't have time
for that shit. So, uh, I hope that, you know, that there's the, that, uh, a, um, people whose,
whose concerns are maybe different than just engineering the best, uh, most efficient and
economically viable model, uh, have an opportunity to, to, to plumb these depths. Um, and, and I hope
that they're, they're able to do that. Um, and, and I hope also perhaps that whatever happens in
an industry, you know, can percolate out at some level. There's a big concern. Uh, uh, I think we
all should have, you know, that science and, and academic, you know, academia itself is just, is
falling off the map in the sense that like the progress is all happening in industry. Um, and that
means that the insights may be closed off from us, uh, you know, because they're privatized and
they're not asking these kinds of questions that I think, uh, basic research people are interested in
neuroscience, for example, uh, would want to ask. And so what's the solution to that? Well,
you know, I guess more public funding, that's not exactly the, seems to be the climate right now.
And public funding isn't really funding these types of things either. You, you and I both know public
funding in neuroscience is extremely, extremely clinically focused, uh, extremely clinically
focused, which is understandable. So I should say I'm not against the people that are doing this
because the people that are giving out this funding is the National Institute of Health. And
it's completely reasonable for the National Institute of Health to be clinically focused.
Yeah. Just a quick aside. So, so Daniel and I are on, on, uh, we're on a grant. Uh, we won't get too much
into it, but it's, it looks, it's, it's trying to predict, uh, or to detect, um, psychoactive
substances that are emerging and stuff. And there's somebody from the NIH who sits in every meeting
and often her, her role is to say, okay, what are the health implications? What are the public
health implications of this? And that's NIH's mandate. Good. And I applaud that. I applaud that.
If my tax dollars are going to the National Institute of Health, I want them to be health focused.
Correct. However, basic research, as we all know, has yielded, you know, extraordinary
results that have had health implications and, and, you know, human, human focused implications.
And it's, it seems like this is a moment in time where there's really, really some very powerful,
important insights into, you know, the nature of language and probably the nature, you know,
the nature of cognition and all of that. Um, and I would, I would hope, and I, you know, personally,
you know, uh, I guess, advocate if for lack of a better word, um, that basic research and, and so
research for the sake of understanding and human, human cognition, um, in this particular time period
should, should really be, it has to be boosted. Like we can't let industry just walk away with this thing,
um, and, and not learn what, what's possible, you know, what we could possibly learn because
we don't know what, what are the possible implications, right? Um, for, for health, for,
for, you know, for, for, uh, understanding ourselves, understanding how the brain works.
There, does this have implications for, here's a good one, right? Alzheimer's and, and memory loss.
I have some things to say about that. Um, and I don't know, you know, I don't know if they're true
or not, but that are, are, uh, strongly influenced by these conversations, uh, the things we're talking
about, like, like our aggression, like in the, and the memory and the, the, the, the fact that you need
to maintain that sequence, right? I think there, there may be some very core insights, uh, if we
understand the brain this way, and if we could substantiate that the brain operates this way,
that could have real implications as to understanding the nature of, of memory loss.
What is, what is sort of the computational basis, uh, of dementia? What's going on,
um, at sort of this level? Um, can we, do we have some sort of things to say now, uh, on the basis of
that, if, if we have a theory now of what, how our cognition operates, and it involves, in this case,
uh, the ability to maintain a certain, um, uh, uh, fidelity, uh, you know, a certain degree of fidelity
of, uh, the, the previous, uh, you know, sequence that happened recently in memory, um, if we, if we
kind of buy that the, the, the architectures, um, that now work computationally in, say, LLMs,
may actually real, yield some real insight into how it works in human brains, and maybe we will have
something substantive to say about something like dementia. Um, and it's not neuropharmacological,
it's not saying that, uh, you know, it's not, it's not giving the other cellular molecular basis
for it. It's, in some, it's, it's computational, but that doesn't mean it wouldn't have broad
implications for how, once you understand it this way, if it gives you insight into understanding
what's going on, that could, of course, have implications. You know, let's say it's about
brain regions and how those brain regions are just talking to each other. Where is the breakdown
taking place? That could, of course, dictate and, and inform, uh, how you might design a drug to
target, uh, in these deficits. So, um, I, I think, uh, the, the health implications, the sort of human
focused implications, it's not just knowledge for knowledge's sake. Uh, it's not just sort of
computational philosophy, which personally I, I think is important. Uh, you know, as a species,
we do want to understand ourselves too, but I think, uh, it behooves us as, uh, you know, as a society,
even if, if concrete results and, and sort of health benefits and, um, societal benefits are,
are what we're, we're throwing our money into. And I think that is, you know, very reasonable to say
that that's what we're going to put tax dollars behind. I think that, um, this, this certainly could
yield a lot of these kinds of, um, investigations are potentially very important. Yeah. And I think it's
worth mentioning, and this is sort of a central point in my dissertation, because in my dissertation,
I'm using large language models to help people build better neuroscience. Yes. Right. And it's,
and it's worth mentioning that, uh, neuroscience research and all scientific research,
but neuroscience research works like a funnel where you have what are called computational
neuroscience to come up with an idea and they test it on data that's already available. So they
just say, here's the state of how these neurons connect to these neurons. And I'm going to run my theory
and see if my theory plays out how I expect it to there. And then some percentage of theories get
knocked out at that stage. Um, but the good theories make it to the experimentalist and the
experimentalists have rats and flies and all sorts of other things. And they say, all right, well,
we're going to test this theory in a more experimental way. And here's how you would falsify
this theory. And we're going to do an experiment. And then some of the experiments don't work out how the
theory predicts. And so you throw out some amount of, uh, things at that point. And then the
experimentalists pass things off to the clinical neuroscientists and the clinical neuroscientists
do further experiments, but experiments that are more focused on, all right, well, if the brain works
this way, then it means Alzheimer's works this way. And it means if we do this, we should be able to
help Alzheimer's. And so they'll do some sort of experimentation, really focus more on an
application like that. And some of those things don't work out how the theory predicts. So more
things get thrown out then. And then finally you have sort of clinical trials, which the FDA is
involved with. And that's where things are really, really tested rigorously before they're used on
humans. And some percentage of things are thrown out then. And so you have something like a funnel where
theoretical ideas about the brain get thrown out at each stage. And by some napkin math estimation,
I've done looking at sort of the stats of how many theories make it to experiments and how many
experiments make it to clinical neuroscientists and how many of those make it to clinical trials and
how many of those actually make it into real products that help people, I estimate that if
we're able to build theories, so do this sort of fundamental work, if we're able to build theories
that are 5% more robust, then we're able to... Computational, like on the computational...
On the computational side, then we're able to double the amount of effective clinical trials.
Cool. That's awesome.
So that's huge. That's having the time till we find a cure for Alzheimer's and Parkinson's and
all of these other really important things. But the work that the computational neuroscientist is
doing at the beginning of this process, he's not necessarily talking about anything clinical.
He's not talking about Alzheimer's. He's not talking about Parkinson's. It sounds a lot
more like the conversation we've been having. And so that receives less funding. But actually,
if you give a small amount of funding here, the other nice thing about computational science...
It's cheap.
It's cheap. You don't have to buy equipment. So a very small amount of funding here can
have... Danny's cheap, guys. Just send checks to...
But a very small amount of funding here can have an outsized impact down the road.
I think it's a great point. But I want to... So there's something about this moment in time
where the computational theories... So historically, and I don't want to throw shade on the computational
approach because it's absolutely necessary. And as we've discussed, you don't know where the
insights are going to come from. However, historically, most of computational neuroscience
that fit you to go on and these models of spiking neurons and all this stuff, they didn't do much.
They tried to replicate certain kinds of sort of low-level dynamics that we observed in the brain.
The models now, some of them anyway, right? These, at least like the LLR models, but also,
you know, in general, neural networks, they actually do the thing. They do the thing,
the behavioral thing. They go all the way, in some ways, to behavior. And so it's much easier,
in some ways, to make the case now that these computational models have... First of all,
that they have these implications, potentially. But going back to your sort of point of like the
robustness and the likelihood of success, it's... I think there's an environment now in which your
likelihood of success should be higher if you're working within the domain where you can actually
see, does it do the thing? Like, can we get... I think we're going to get to the point, probably
fairly soon, with robotics plus AI, where you're going to be able to test these fruit fly models
in a synthetic fruit fly. You know, it's going to do the thing that fruit flies are supposed to do.
Or you can, you know, or you can lesion it in ways that stimulate a lesion in a fruit fly. And so,
as the models get more and more powerful, as they surely are becoming, I think the argument becomes
much stronger for investing in, on the basic research side, you know, computational models,
not with the clinical end in mind, but at least behavioral end in mind. Something that's close,
not just simulating some process deep within the bowels of the brain, but that actually
inform some outcome that in some ways looks similar to the outcomes we care about. Things
like, you know, whatever, navigation, whatever it is, your fruit fly, your memory, learning,
whatever the fruit flies do, or that people do. So I think this should be a golden age.
Again, we need to invest for it to be a golden age, but a golden age of computational neuroscience,
computational research with possible, with these kinds of possible human focused, human positive
outcomes. And there's things that have happened just in the past five years that I think have
gone computational neuroscience to this point where it's a really exciting time to be in it,
where we've gotten the complete connectome of the adult fruit fly, every single neuron,
how it connects to every single other neuron. That's huge. I like,
You should do a dissertation. Yeah, I'm doing a dissertation on that. We've got a complete map
of every single cell in the mouse brain. Right. In the past five years. Right.
We've got the first clinic, clinical tool that uses these connectomes, human connectomes to,
to sort of assess, all right, where is the damage to humans? We've gotten, we've got a huge,
huge project working on getting the complete mouse connectome that I expect to wrap up within the next
five or 10 years. We've got more. You need to finish your dissertation before that though,
just so you know. Yeah, yeah, yeah, yeah. I'm working on, we've got more genetic information
about the brain than we've ever had before. I mean, the amount of data that we've got that you can test
your theory on and sort of have a synthetic fruit fly is, is enormous. So yeah, this is an exciting
moment to be in computational neuroscience. Yes. And, and, and just the, just the additional factor that
at least some forms of computation have, have really proven to be, um, like models. Yeah.
It seems of, of the real thing, I think should put the sail, put the wind in the sails, uh, of,
of computationalism in general, not necessarily the, the classic computationalism of, you know,
of, of what cognitive science historically did, but it's going to be closer to, uh, you know, neural
networks where you're, you're, you're building systems that, that can, um, I think one thing
we've, you know, get the human, the human conceptualization can sometimes be, uh, a little
bit of a, uh, a red, uh, a red herring or a, a garden path. Um, I think that's, we don't need
to be able to consider, you know, it's not about conceptualizing it externally. Um, it, we may just
be able to do it building smarter machines in the way that we now know to do data plus
architecture. And, and that's not sort of the classical cognitive science perspective
of computationalism. It wasn't about, it was about theorizing about what the architecture
was. Now we were like, it's, I think in some ways it's healthy shift is to say, um, let's just try to
finesse the architectures. Yeah. Build something that works, build something that works. Um, and then
figure out after the fact, okay, what does that tell us? Like in the case of LLMs, in some ways
it wasn't because I don't think people said, hmm, uh, autoaggression seems to be the way humans do it.
And I'm proposing that. And therefore let's build it this way. Instead it was, what can we get a neural
network to do? Predict the next output. And then what if we do it this way? Boom, it worked. Lo and
behold, that ends up being pretty, very, very similar to what you expect people to do. We kind of think
sequentially and we talk sequentially. Um, so yeah, maybe, you know, get, get the, uh,
the smart people have to take a step back. Like, don't be too smart. Don't be too clever
about thinking that our brains are going to sort it out, uh, uh, theoretically and instead
really kind of lean in on, on, on sort of computationalism, uh, you know, as, as the approach
as almost like engineer our way to these solutions, but with a basic research kind of mentality,
um, yeah, yeah. Thanks. That was a really cool and interesting conversation. And, and, and, uh,
I think there's something really kind of meta about this, which is that in, in the process of,
of talking about stuff, I'm sort of running these autoaggressive models and reasoning through, um,
something we haven't talked about, you know, sort of like the, the, the chain of thought and,
and this kind of reasoning processes, how that, and it lands you in a different spot. And, you know,
coming back to your point, I have a different, I have a different model now by virtue of, of these
conversations. And it's, and it, and, uh, so the, there is clearly this dynamic that's happening,
you know, even though I think it's one system, but it's one system in, in embedded in, uh, in the
world and, and, and in a conversational context, that's going to get fed back. Like next time we talk,
you know, even if it's, you know, a week from now, we're not going to be mid conversation,
but I'm going to talk about this and not just with you. I'm going to, I'm going to think about
this differently going forward. Um, so there's this very cool kind of process of certainly
informing and updating the model through this conversation, because the conversation as
thought is sort of one of the key foundations of like this whole perspective that that's what we're
doing. Um, you know, it's not all in there. The information is not in there. It's not in the model.
Um, it has to do this stuff, get it out of this model, get it out of itself, work through things.
And then that ends up getting fed back into the model to retrain itself. So it's all very odd,
but it's sort of a wonderful, this, this was a sort of a wonderful kind of a demonstration for me
of that process. Yeah, absolutely. And this is the really cool thing about being a neuroscientist is
you get to use the brain to study the brain and that's a unique place to be. Well, you can find
me on Twitter and sub stack. And then I have my own website. Uh, I'm also on, on a blue sky,
because I know people are leaving Twitter and drills right now. So, uh, now I'll show my age,
my, you know, my socials are, uh, much less developed. Uh, but I do have a website, uh,
that's kind of rudimentary on baronhouse.ai. Um, I have a sub stack, which I, uh, have to populate with
more stuff. Um, but, uh, if you check it out and I know you're visiting, that'll encourage me to,
uh, to do a better job on the social side. So, uh, please encourage me to do that.
Oh, like, and subscribe. Yes. This is an awesome channel. Uh,
he's having lots of conversations like these on here. Oh yeah. I love this channel.
I have to skip over the, the, the interviews with me. Uh, I don't look at those, but, uh,
I love to watch the other stuff. It's, it's a great channel. So like, and subscribe. Absolutely.
