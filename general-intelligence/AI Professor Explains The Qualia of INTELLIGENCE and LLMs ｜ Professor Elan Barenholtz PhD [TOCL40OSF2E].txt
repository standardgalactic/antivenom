Okay, so what I've been thinking about, and this is kind of taking up all my mental space, is we are LLMs.
And when I say we are LLMs, I just mean the speaking we, the linguistic we.
The thing that's doing the talking right now, that's talking to you, is an LLM.
And what are the implications of that?
So, I'm not going to spend time right now trying to convince anybody that our language has to be LLMs.
But in a nutshell, it's just completely implausible that language would have the structure that it does, these long-term dependencies that end up working out in an autoregressive environment.
It can't have that structure, and then humans are doing it differently.
That's just, that's absurd.
So, language is, it's our thing.
Before it was their thing, before it was machine-street, it was our thing.
It has this structure.
This structure is designed to generate autoregressive next token generation.
And that means that's what we're doing.
So, we are LLMs.
Another question is, what are the implications of that?
And there's kind of scientific implications that I think are very, very important.
The brain has to have some way to encode sequence such that it's able to go back in the past.
And I don't mean literally in some ways that it's not, I don't think it's a memory in the traditional sense.
It's somehow, it's built into the way the brain encodes information and the topology.
You know, it's probably things like just kind of recurrent feedback loops and things like that.
But it has to be encoding sequence.
And that has implications in terms of thinking about what the brain is actually doing.
How neurons work.
And it might not just be about language.
I don't know yet.
But certainly language, the fact that humans are doing it in this autoregressive way,
which means that they have to be preserving this kind of sequence.
And I think that's going to have very, very big implications for neuroscience and thinking about how the brain works.
But then there are other kinds of implications,
which are harder to think about, but also, you know, incredibly, potentially incredibly profound.
Because things like saying that the thing that's doing the talking now is not the same thing that's doing the seeing now.
And we can think visually and we can think linguistically.
And there's clearly interaction between them, but they're modular systems.
And because LLM speaks, basically demonstrate that,
that the corpus of language and the structure in the corpus of language
is enough to be able to generate itself without any, it does it effectively.
The generation process does not require any sort of interaction
with some other sort of computational framework, like vision, for example.
LLMs can talk about red, and they can do it very effectively.
They'll tell you what red means scientifically.
They'll tell you what it means, what it feels like to experience a red sunset.
You know, try it out.
It'll be very expressive.
It'll sound like a person who's able to express ideas that I think we would somehow anticipate,
and naively, would require actually knowing what a sunset looks like,
and what red means.
But they can't know what red means because they don't have access.
It's simply not in their computational architecture at all.
The quality of redness that the visual system encodes isn't in there,
and certainly in a purely text-trained model.
So that means in us as well, the linguistic system that's generating
doesn't really know what it's talking about when it talks about red.
But then there's another system in us that does know what red is.
That's the visual system.
And I think this is probably a very important insight in relation to the mind-body problem,
and why there seems to be this duality is because we have these incommensurate,
they're just separate computational systems.
And they interact, they message paths, they talk to each other,
but they don't speak the same computational language.
And so I think that all of this has very, very, very profound implications.
I could go further, but that's already a lot.
And so that's what I'm thinking about these days.
And I think it might be worthwhile for people to think about some of this stuff.
And it's not specifically my ideas,
but I'm a little surprised by the lack of fanfare, in some sense,
from the intellectual, academic community in relation to these very profound insights
that these LLMs seem to be affording us.
Because this has implications for traditional fields like linguistics and philosophy.
I think they're immediately apparent.
So I think this is an inflection point, I think we could say,
in sort of human intellectual history, whatever.
So I think it's reasonable.
I think we're LLMs.
I think the speaking us, the linguistic us, is a large language model.
And I have lots of reasons to think this, and I could share this with you,
but I think if we just draw that conclusion and go from there,
there's some very interesting, mind-blowing kind of realizations you have.
And the thing to know about LLMs, so technically what they are,
they're these kind of learning models.
You dump a bunch of data.
In this case, it's linguistic data, sequential linguistic data.
It's digitized books, digitized websites, but it's all language.
And you dump that into this machine, and it learns language.
I think those who claim at this point
that it's just some sort of mimicry of language,
they either haven't spent enough time with it,
or their career depends on denying it or something,
it just doesn't make sense.
It knows language, it understands the concepts linguistically.
It performs linguistically in such a way that's, if not indistinguishable,
close enough to human reasoning that, okay, you've got to fix some stuff,
but it knows language.
And it knows language based on language alone.
Because, as I said, it's a machine.
You feed it a bunch of data, and it learns about that data.
In this case, the data is language data.
It doesn't even have to be language data, but in this case, it's language data.
And this kind of machine is training enough.
It learns, actually, from the data.
It's not just like input-output, but it turns through the data.
It kind of thinks about the data, almost, you could say.
And by the time it's done, it's learned about that data.
And what it's learned in this case is sufficient for it to be able to produce language
in a way that's, I think, arguably as good as humans,
in many cases better than humans.
But the basic concepts are there.
It knows English really, really well.
And so it's doing this just based on linguistic data.
And so here's the mind-blowing thing.
The mind-blowing thing is it doesn't know what red means.
It doesn't know what space is.
It doesn't know what feeling anything means.
It doesn't know what sounds are.
These words that I'm using right now, and I'm aware that it's a little bit
strange to think about the thing that's talking now
is one of these language models, but we'll leave that aside for now.
But they're never given that data of what the visual system sees.
They're not given data about what red looks like.
The visual system is, and the visual system sees red
because it has a very particular structure, also learned to a large extent.
And that processes data in a certain way, that redness has the quality that it does.
But the language model never learns about that at all,
unless maybe it's got some books like Mary,
you know, the famous Mary problem.
Maybe it's read some books about how color is represented
and about the photoreceptor array.
And it can maybe kind of contemplate linguistically
what it would mean to see red.
But just like Mary, it's very, it's implausible.
There's no reason to think at all that it has any computational idea
of what redness is, because the visual system has that.
And yet it's able to talk about red all day long.
And it knows what red means linguistically.
It will use it correctly.
If you tell it in the beginning of a story, this is a red ball,
it will use that information completely appropriately linguistically
for the rest of, you know, the rest of the conversation.
And so what that means is that the language models are able,
the language contains within itself sufficient information
to generate itself fully without having any access to
what the words are actually mean.
And when I say what the words mean, that's what I naively mean.
That's what we mean by red.
We mean, when we feel like we mean that.
But the language models don't really mean that.
So what the hell is going on?
How are we able to talk about qualitative aspects of, you know,
the visual environment when that information about the,
you know, sort of what the words in some ways refer to,
and they have to refer to it because obviously we can communicate.
I can say, go get that red ball and, you know,
somebody will go get it and I'll get the exact right one.
There's some obvious sense in there there's a mapping here,
but language is self-contained.
And if it's self-contained, that means language not only does it need,
it doesn't need the visual information.
It can't be in there at all.
It's strictly linguistic.
Strictly linguistic is what generates linguistics.
You can't slip it in there in some way.
And so that means even in human beings,
even though we have this sort of clear compatibility and exchange,
our visual system and our language system,
and not just by the visual, or, you know, smell, sound, whatever,
all of the experiential stuff
is something that's not in the language, like computationally.
And so that means there's just really is this weird schism,
this weird duality that I just, you know,
I went into this career, my entire career was to,
because of the mind-body problem.
I thought that was the most interesting thing in the world.
And I still think it's the most interesting thing in the world.
But I feel like there's some actual insight into it.
Now, what's, it's a different way of looking at it,
what the source of the problem is.
And I think maybe this is very important,
that there's this computational incompatibility, inherent.
It's not the right word, but they're mutually exclusive.
What can I say? The linguistic system speaks language,
and that's it.
It doesn't speak vision, and vice versa.
And there are aspects of reality
that maybe the language system can sort of capture,
and it's whatever that would mean,
that the visual system may not have access to.
And so, and I keep focusing on visual,
but I mean the whole sensory domain.
And by the way, all consciousness is sensory.
I don't, all consciousness is sensory consciousness.
And so, I think that that's the ghost in the machine
is, is in some ways, that.
It's, it's, there's, there's,
the language thing, the speaking thing,
is, is, is cut off
from that way of interacting with reality,
you know, call it, call it that,
whatever that, it's a computational system,
but it is, it interacts.
And then the linguistic system is,
is, is incapable, inherently incapable
of knowing it.
And, and, and therefore,
the language system is like,
what the hell is this subjective thing?
It's a word for it,
but I don't know what it means,
and it doesn't fit.
And it sort of needs to be expunged,
because it doesn't actually run,
sort of in this,
in, in the linguistic system.
And so that, that's, I think,
sort of where the,
sort of, you know,
it's almost like,
we can look at the,
the, the corpus of philosophy
and all this,
as having been generated
by language models
that don't understand
what red is.
And, that means,
now we sort of,
in a weird way,
explain the mind-body problem.
Now, we can get in,
in loops here,
about, you know,
what, what is,
what, what's the next,
what is, what,
where does one go from there?
And, and I'm not there yet.
But, I think this is a,
a very important insight.
