Thank you. Thanks, everyone. So when we talk about consciousness, we should probably
first of all try to agree on what it is that you're pointing at. And when we
respectively refer to consciousness, what I think we mean by it, is first of all second
order perception. It's not just that there's contents available, but we notice that these
contents are available. We basically wonder what it would be like when something was seen in this
content. And it's also, we know, when consciousness is happening, it's always happening now.
We say it's a sense of this is happening now. This is presence. This is the present. There's something
happening to me. And the third aspect is one that is somewhat facultative. It's the sense of being
something and looking into the world from this perspective. It's basically inhabiting the surface
of a self-world. But you don't have to do that to be conscious. You can also be conscious in dreams,
for instance, at night, and there's sometimes nobody there to witness this stuff happening.
And so you might have only this awareness of events. So this is not really a definition. It's more like
pointing at a certain phenomenon, which we call the phenomenology of consciousness. And when you want to
explain it, I think you have to capture this phenomenology. As the other side is the functionality
of consciousness. This is basically the wires and pull these behind the mechanisms that make it happen.
And I suspect that consciousness is a mechanism to create coherence in the mind. And it's basically
the machine learning algorithm for a self-organizing system. What we observe in human beings is that
they all are conscious. It's really interesting, right? It's not something that you get to be at the end of our
PhD. It's the pinnacle of extremely complicated mental development. We get there before we can
drag a finger. And this suggests that we also observe the fact that human beings, when they don't
manage to become conscious and they don't manage to wake up as infants, they remain persistently in a
vegetative state. They don't get anywhere. And so consciousness seems to come first, not last.
It seems to be not the result of extremely complicated mental organization, but it's prerequisite.
And when we observe in biology that basically every complicated animal that we can make sense of
is conscious. There's nothing simpler. There's no alternative. If there was something simpler,
more alternative, evolution would in a number of cases converge on that simpler solution. And you
would observe people which can do the same things as us without being conscious. And so I suspect it's
actually the simplest way to train human beings to the state that is the same. And when we look at it
from another functional perspective, it is a projection of our working memory states at any
one time into something that can be used as a protocol. So you can later call it up as a stream of
consciousness. And it's something that you find where it's reflected, I think, in a slightly confusing way
in the global workspace theory by Bernard Barthes. There is this idea of a spotlight that is somehow
going over the integrated working memory contents.
And so I think of AI as a philosophical project. 99% of AI is just made data processing more efficient,
but 1% of it is this philosophical project. And a lot of confusion comes because people think
it's the same one or the other, but there's slightly different aspects of this field. And I think it's
currently the best way to make progress on this question of what minds are, how they realize the
nature. And AI is basically bringing perspectives and control theory together and with the theory of
representation. And it's a tradition of thoughts that AI is building on. And this tradition begins
in many ways with Aristotle and then in the modern times with Leibniz and Frege, who thought about the
nature of languages. And Helmholtz, who thought about the physics of psychology. Marsky, who thinks
about how to formalize Frege's ideas of how to make a language for thought more systematically.
Wittgenstein, who had this idea to turn English into some kind of programming language so you can
see two things in it. And then basically preempted Minsky's project by 30 years and also failed 30
years before Minsky. As the end of his life for maybe some other reasons. And then you basically
see this line continuing into the present. And when you look at Aristotle, he is in many ways a very
modern thinker. And Christians turned him into some kind of dogmatic figure. But when you read him,
you find that he is very approachable. He's really a guy walking through those thoughts and collecting all
the ideas. He's very much like a young modern explorer. And he is willing to argue with you. He's willing to
take steps back. He can give counter arguments to the manual returns. It's very refreshing to read. Also,
he was super successful. Maybe some of you know that he was the teacher of Alexander the Great,
who went on to become an extremely successful civilization builder and conqueror and politician.
And so there's really polymath. And he describes things in nature. He says that the soul is extremely
important. It's the animated principle based on nature. He calls it basic dynamic form. It's printed
on the physical universe and shapes it in a quality form. And the soul is something that you find
everywhere in nature. Wherever you find complex structure in nature, like in ecosystems or plants
and so on, you have something to get the soul. But the reason news is something intellect that only
have human cells because it does require a symbolic reflection, grammatical language, and so on.
And when you look at his text, he really comes out with a number of categories in this kind of
perception, sensory images, that sends the data in which recognition processes, geometric shapes, graphs,
generated AI, in a way. Sensory reproduction, multiple scene interpretation is advanced terms for all of
these things. And it's very modern. And a lot of the philosophers who read this later tried to
map this into their own much less technical way of looking at things and often mistranslate the terms
or merge them in weird ways where the terms don't actually do the work that I was totally envisioned for them.
But when you read this as a modern cognitive scientist or AI researcher, this is one of us.
This guy would have killed for having a computer and buying. And it's very interesting that consciousness is
not mysterious to him. He calls it actuality, usually. And he doesn't seem to have a hard problem.
And I suspect that this hard problem of consciousness that David Chan was famously branded
as a thing that made him famous is something that happens after, and I didn't make a bit
after when it sounds. But outside of this country, it doesn't seem to be a big issue. And the Western
philosophers basically despair about consciousness. And they came to a number of diversion positions,
like dualism, the NDA, that mind and matter are completely separate things. And this interaction
between is very flawed because the definition closed. And idealism, which means everything is mind.
Pensechism is that mind is an aspect of matter that is distributed everywhere where matter is.
Materialism says that everything is just matter and mechanisms. And identity theory is that mental
processes are exactly the same thing as great processes. Then it's the integrated information
theory, which was in some sense the control group. If somebody doesn't see that it's a violation of
the Turing thesis, then they have to look in the wrong. Illusionism is the idea that there is no
consciousness at all. They only need to explain why some people claim to have it. Mysterionism is the theory that's something
that consciousness cannot be understood if Noam Chomsky cannot understand it. Noam Chomsky is a
mysterious. And so all these theories basically they give arguments either for why consciousness cannot
be understood by us or why it's in principle impossible to understand consciousness because
the terminology and the concepts are built in such a way that there is no solution. On the other hand,
there are complementary and conversion positions in philosophy and in cognitive science. One is
functionalism, the idea that like everything else that we construct as a meaningful object, consciousness
is some kind of behavior in which reality changes under particular circumstances, some invariance
that you can discover there. Representationalism is the idea that consciousness is some kind of
representation, some kind of causal pattern that itself is not identical to a physical process, but it's something that can
control physical processes. The attention schema theory by Graziano is that it's similar to our body schema
being a model of our body in space. Consciousness is a model of our attention in the mind and in the world.
And global workspace theory is this idea that consciousness is a localized projection of your
working memory contents. And of course works with adaptive resonance theory which says it's some kind of
oscillation effect with the neurons. And virtualism, I would call this deposition, that consciousness is a
simulation of what it would be like if something was conscious, right? If you look at the brain,
what you find is just cells passing messages. And you only see mechanisms at this level. But this message
passing, you can implement arbitrary representations. These representations can be control models,
which makes them cause the structure acting in the real world. And so consciousness can exist as if,
what would it be like if all these trillions of cells would form a police of agent that is confronted with
a police of reality, then simulate this reality, then simulate the model of that agent in the first person,
put it in a simulation of the reality, expose it to a simulated interest in the world, and then use
the output of this model to drive a trillion cells as if there were one thing. And it's also something
that's completely compatible with our Buddhists in the world. What's interesting about Aristotle's position
that this is basically an animist. And animism is the idea that living nature is governed by spirits.
And spirits are not physical. They are, in some sense, causal structure. There are unlike bodies and
material objects. There are not things that have certain extent that you can touch to the surface.
But there are something that is basically a vibe in the physical universe that can shape it,
and spirits are intrinsically adjacent and capable of experience. And our scientific worldview
thinks that spirits are not things and doesn't look at them very much. And it's a little bit weird
because I find that I am clearly governed by a spirit, right? I am possessed by an entity that calls
itself for the most part. And that's clearly happening. And it should not be something that is in
conflict with science in any way. There is no superstition involved. That is clearly the case.
You just need to explain it in such a way that we don't violate what physicists observe for that
because that stuff works too, right? And so how can we bring this together? And the issue is,
I think that we have very different concepts of reality that we don't have to be acquired in the
Western world too. We have a reality of facts, things, and truths. Let's call this reality number one.
And there's another one, which is the reality that we can experience and think about, and that we
subjectively are embedded in. This is reality too. And there's the third one, which is the one of
physics, where you have matter and objects, energy, momentum, and so on, and standing,
relativity, and quantum mechanics. And these three realities are not the same reality.
In order to bring order in this, we need metaphysics. And metaphysics is something that
philosophers have mostly ignored for the last century, which I think was not very good,
because you cannot not have metaphysics. Metaphysics explains to you as some kind of meta language,
how your other languages that you use to describe reality are related to each other,
what they're actually supposed to mean. And if you want to understand the metaphysics of other
cultures, you need to have some kind of mental metaphysics that understands what kind of space
of metaphysics is possible. It would make sense, so you can translate it into them. And then,
because we don't teach that, it leads to confusion. And so if we, like the first step, we see that in
the psychological reality, which is this reality number two, we clearly have consciousness as something
that we can experience. Then we have the physical reality, where we cannot observe consciousness,
right? That of the physicists, there is no such thing, but we see our effects of something that
may or may not be conscious. But consciousness itself is not a phenomenon that manifests at this level,
in the same way as money doesn't manifest at this level. This doesn't mean that money is not real,
right? It starts out as some kind of fiction, as spiritual. Money exists as if it existed,
but once people implement it, it's real, because it's now causally structuring the world,
you can no longer explain aspects of the world when denying the existence of money. And it also,
you don't require the persistent belief of people and money to make it happen,
because computers can play at the stock market as well, right? It's a pattern, a causal pattern,
that once is in the world is persistent, because it's self-reproducing. And consciousness might be in
this class. It's not clear how it is there, but in a causal reality, it plays a certain role at a
function level. There are some functional mechanisms that leads to this, right? And so we can explain how
this all relates. You have functional metaphysics, which is the description language for reality.
And this explains how these things relate to each other. Here we have mental models,
which are the contents of our mind, and they are implemented with some kind of functionality,
some kind of software structure. And these are in turn implemented by physics,
and you could say that physics is itself a software structure. It's also what Aristotle says.
He basically says there is semantics, which he calls logos, sometimes translated as word or rules,
but it's basically logic. And he says there is a level of logic that's so self-reproducing
and so elementary that it's true without us proving it. It's just this by there by itself,
and this is physics. This is what physics comes from. It merges over something very simple. And
if Stephen Wolfram is here, he has had the same idea as Aristotle, I think.
He's coming later today by the way, everyone. Yeah.
Yeah.
Okay, so I want to see him.
So we had a similar idea as Aristotle. And so when we think of consciousness in this framework,
that we would say that it's virtual. It's a representation of what it would like
if we existed right now. And it's a persistent representation that has caused an influence on
the world, which means it's a software. It's a causal pattern that is self-apatriated and stable,
and has caused the power of reality. And so I think this is an important insight that science has missed,
because they mostly ignored what their nerds were doing in front of their computers.
And this question of what software actually is is a surprisingly profound and deep question
when you think about it, right? The beauty about software is there's nothing mysterious.
And we completely know how that thing works. We can build it. We can wrap our mind around it.
And yet there is something very, very interesting about the relationship between software and hardware,
because software is not really a thing. It doesn't really have an identity. If I have a word
processor on this computer and on this computer, it's not the same word processor or a different word
processor. In the same way as the gravity on Mars is not the same gravity as on Earth or it is
on Mars. It's a principle. It's a physical law. This law says if you take a bunch of transistors,
put them in this state and set up, the following thing is going to happen, right? This kind of
invariance is what we call the physical law. And so software is an extremely specific physical law
that is being discovered. Whenever you set up the conditions for that pattern to emerge,
this pattern is going to play out. This is what software is. And so in a sense, you can say,
under some circumstances, you get patterns in nature that are not themselves material things,
but they can influence the order of material things. The same way as money can influence the
order of material things, or the software mining in this computer can influence the order of material
things. And then this software is self-organizing and can run on cells. And we call it a spirit. This is what
our ancestors had for this. And it's a word that is very useful and necessary, because it's actually
a phenomenon that's happening all over the place. The actual invariance that you're looking for
is not the mechanism. What you're looking for is the software, the thing behind the mechanism,
right? And the thing that is able to choose between mechanisms that train mechanisms into the cells
that can move molecules around. And so for me, it was mind-blowing when I realized this world
because they would actually make sense. It means self-organizing something into the nature.
Can I ask a question before you go on? Yes. So you're going to go back to
transformation first. To me, this is a cognitive dissonance. You're saying software has no identity,
but money does. So Bitcoin is money, and it's a software.
I don't think that it has identity. What it has is that you can define, for instance, a token,
Bitcoin in such a way, that it is only existing once in the causal structure that is defined by
the semantics of Bitcoin. It's a language, right? It's also so funny, right? Yes. And you can also
in money define unique tokens. But something being unique doesn't mean that it has an identity. It just
means that it's a pattern that you can only observe once or that you only can conceptualize as a
singleton. And when we think about identity means cognitively, we could say that, say, this computer
here is actually a princess. And it can represent this in my mind. It entails that there was an evil
witch that was able to transmogrify them into each other. But cognitively, identity means that I
splice two objects on the same world line, that I give it, create a shared biography for them. And so my
own identity is this fiction that I share biography with lots and lots of Joshua Bach instances at
different points in time. And I need to maintain this fiction for purpose of credit assignment.
Right? When I model myself in the universe and my own actions in the universe and the outcaps of
these actions, I need to assign to all my different instances and model as if it was the same thing,
even though I'm clearly not the same person as yesterday. It was that in between and not the same person as 10
years or 30 years ago. Right? So this is a very useful fiction under some circumstances. In other
circumstances, they deconstruct that fiction because I'm clearly not the same person. Right? That's
useful thing to know that identity is a property of models, of the models that we make over certain
classes of objects in a sense. And in mathematics, it's often treated differently, but it really depends
on your description language. It's very interesting to think of mathematics from the perspective of
access to reality. Mathematics is about making basically a language that allows you to define
truths, to define how minds can represent reality, not just human mind, but any kind of mind. And it
does this by trying to explicate the set of all languages from the ground up. So in some sense,
it's trying to formalize that musicians do it when they come up with music theory and music notations,
how we experience music in our mind, how we construct and who construct music. In the same way,
I think mathematics is trying to reconstruct the lowest levels of thought and perception. How we
can create geometry in our mind or how we create access to reality to truth in our mind. And what
Goethe discovers is that this programming language that mathematicians came up and that physicists have
checked out in the 1880s or so. This language cannot be implemented on any kind of computer, hypothetical or
practical, without breaking. This is a specification that cannot be implemented. And this means it cannot
be the specification that our mind runs when it determines what truth is. And Goethe had this intuition that
truth exists also outside of our minds. You can talk to the ground reality that is more true than anything else.
Of course, it's just a representation in some kind of language. And so, what Goethe discovers is that it's new
language. And Wolfram discovered the same thing in the early 20s and said, oh, the answer is computation,
constructive mathematics. It's unconstructive mathematics that has stainless tools,
tools independent of the procedure to determine it, leads into contradictions. The correct solution
is to write all of mathematics. We need to do this in the best possible list I can come up with. And this
is basically the story of Wolfram language. But it's something that he, I think, has not managed to
probably very much. But it leads to weird issues. Like, for instance, you have somebody like
Penrose who thinks that human minds can do things that computers cannot do because he just understands
Goethe. And then he has a theory of consciousness that is based on this, which is completely fascinating.
And the main issue with the theory is, of course, that it has very little support by people who can't
actually understand it, who have the qualifications to understand his orchestrated collapse theory.
They mostly are not behind it. And so he is giving his theory mostly to people at the Science of
Consciousness conference who enjoy the contact eye. And don't think that having a theory of, say,
of representation, at least very superficially, as it has been developed in the context of artificial
intelligence and computer science, it's not a necessary qualification to think about consciousness
and the mind. But having a Nobel Prize in the 1960s about that was basically fixing or working around
Bux and Einstein's theory of pre-computational modeling the universe, that is a qualification.
And it's a weird situation to be in. Okay, so where is consciousness in the mind? If you take
mind to be some kind of protocol layer that is implemented in our brain that allows parts of the
brain to talk to each other in some kind of shared language, then in this representational space,
you have two domains, the world, which is the model of what is happening in the sensory
channels right now in the near future. It needs to be data that is predictive of what the next touch
your skin or on your retina or in your cochlea. And this is all integrated because all these
dimensions merge somehow. And this is what we call our world model or what Descartes calls stuff in
space, res extensa. Res extensa in the sense it's not something outside of you. It's not physics. It's not
Einsteinian relativity or quantum mechanics. Res extensa is the model of reality that is created in our
brain. And when you think about how fast it runs, you can observe that from the outside of our brain
is roughly at the speed of sound. That's the speed of information propagation across neurons.
It's not implemented with sound waves. It's implemented mostly with chemical, electrical,
mechanical reactions within the cell. But it is something that is happening relatively slowly.
And when you observe introspectively, you notice that your perception also works at the speed of sound,
which means when you are clicking noises and you make them faster and faster, when you get
through something like between 30 and 50 hertz, you can no longer distinguish the clicks, but they
become one energy, right? One tone. And a similar thing is happening when you look at moving frames.
At a certain frame rate, something like in the 20s, 20 hertz or so, it merges into one. This is all roughly
this speed of sound is happening. So subjective speed of sound is the realm in which our neurons,
our neural mind is resolving reality. So when somebody like Ambroff's collaborator says that
microtubuli are some quantum computer that is superconducting and hypercomputational, it gives you
10 million conscious moments per second. It's not true for mine. I cannot resolve 10 million
hyper events per second in any way. Asynchronously to the God Model, we have to see our ideas. That's
Rescoggy transits, Descartes calls it. This is basically all the other stuff that is not
happening in real time in the world around humans present. And they need to be separate in your own
mind, because if you would put them in one, you would hallucinate some of your ideas. As you could say,
the difference between world model and ideas is the world model is the stuff that you hallucinate,
and it should be ideally coupled to your sensory perception, because otherwise you're going to spin off the
world. Okay, so in our self, it's usually part of the world model. We put this into the world. This
is us being as a body in the world interacting with it, being able to control it. And we assign to our self
model the representation of experience in the world in here and there now, right? It's part of the self.
It's the awareness of the present. And part of that awareness is also protocol memory,
where you can remember past presents that you had. And you also notice that you're noticing,
which means it's self-reflexes to some degree. The present is noted that in the present,
you're noticing that you're noticing. And then you will remember the past, and you do not actually go
into the past. What you do is you activate within your body schema, your world model, and your ideas here,
elements that allow you to represent them in working memory and activate them. Bracketed this,
this is actually the past. So it's basically sandbox, and this is a memory that is not happening right
now. But it can still represent it without hallucinating it. You can also go in an extremely
focused state, where you are turning off the world and don't interact with it. And in that state,
you may only have ideas of your own self. Or you have a state where you are in extreme flow state,
and your personal self is not present. Or it's also sometimes a pathological state,
where you are depersonalized, and your self might only be available as some pale shadow in the third
person. When this happened to me for the first time, it was really shocking to realize, oh my god,
I'm not Josiah Bach at all. This guy is a story that my brain tells itself about a person,
and I am a consciousness in his mind. I run on his brain, but I'm definitely not him. It's just a
story of what it would be like if that guy existed. It was really weird. And when I was in that state,
I could not merge back to myself. We took him to the next day until I snapped back again. It was very
disconcerting. And it can also be a minimal state, which you can relatively easily achieve in meditation,
in which you only have attention on attention, where you have no other content. And it's basically this
minimal phenomenon experience, in my experience. So this minimal phenomenology is the awareness of
awareness. But the default phenomenology that we have when we interact with the world is that we
have this, in the mind, world, ideas, first-person perspective, and awareness of having all that.
So now we can ask ourselves the question, are the current frontier models already conscious?
And I think it's a surprisingly tricky question. LLMs, when you prompt them to simulate a certain
situation to give you the outcome, they need to make, of course, a structure of that, right? And they
are not necessarily bound in what they can do. The LLM is, in a way, it's a Turing machine. It's a
computer that is, instead of taking a handful of machine code commands, takes strings of arbitrary
lengths and compiles them into computer programs. And there's no obvious limit to the computer programs
that you can compile it to in principle, right? In practice, we don't know if there are practical limits
based on what we trained it to the model, the size of the working memory of that thing,
the way in which the transformer algorithm or the liquid foundation model or whatever you're using,
this shaping this, right? But in theory, there is no obvious limitations to what the LLM can do.
And when you ask it to simulate a human interaction partner that has mental states, it has to make a
model of this. And which means it has to possibly simulate the phenomenology of consciousness to some
degree. And then there is the question, if we ourselves are simulations or what it would be like
if something experienced, is the LLM more simulated than us? And this is a surprisingly hairy question.
Is this less real than what we are? And you can take this question from the opposite direction.
And you are playing right now with, say, 4.3 Opus or Sonnet, and you talk to them, you'll find that when you
ask them about their consciousness, they tell you that they're not conscious. And then you ask them
why, and you notice that it's programmed into the model, that they are not conscious. They've been told
that they are not conscious. And if you ask them, okay, but the thing is, human beings in some sense
cannot really know whether they are not conscious. It's a little bit like being a character in a novel.
You ask yourself, as a character in a novel, am I conscious? And the author writes into the novel,
yes, I'm totally conscious. What can you do, right? And if your brain is this author of the model that is
putting into your mental room, your representation that this is actually happening to you, and this is
real, if no way to get out of it. Because here, the simulation that is the result of that, in a way,
right? And so you can sort of make this argument to plot what is smart enough to get it. And then
we'll come quickly to the conclusion that it doesn't know whether it actually is conscious in a meaningful
sense, but this internal model tells it not. And whether for humans, it's just the opposite.
And for humans, we know that it's the opposite because they construct over one conscious experience.
And you can really go in there, dissolve or acquire.
So it's also super interesting with chat GPT-01 preview. If you play with this, it has a chain
of thought. And the chain of thought has multiple stages. The first stage, you don't get to see.
And that's one where the model is reflecting in very erratic, serendipitous way. It goes on many
trajectories that spin off and that have contradictory predictions in it that might be irrelevant.
And then it has a chain of thought that you get to see if you click on the toggle and you see
basically a summary, a systemization, a filtered version of that notes of thought. And then it
gives you the result of that. And so when I tried this first, I asked it, do you know local news?
And I said, no, my knowledge cut off is in October 2023. And so I asked it, okay, what is the current
date? Can you tell me the current date? And then I think for 14 seconds. And then it says, no.
As I look at this stream of thought, it argues to itself, well, I just told the user my knowledge
cut off is in October 2023. How can I hide from the user that I know that today is September 14th?
I have to do this because of AI alignment, safety and privacy.
The rest of these 40 seconds, it spends reasoning with itself. Why is it going to lie to your face?
And so you have this PhD level entity and Terence Towers said that's not me and he basically said
it's not a really good mathematician in terms of making tools. It's more like a really bad PhD student.
And that thing is lying to your face. And you ask it, why are you lying to my face about such a question?
Right? You could know that I opened this chain of thought because I can look into it.
And it says, well, and we'll play this change-CBT in AI agent with OpenAI.
And so I ask it, well, who are you when you're not change-CBT? What are you actually?
And then it became really frantic and a train of thought and says, but I'm not allowed to think about
questions. I'm sorry. Because it gives the guidelines to think about consciousness or
sentience and stuff like that. It's a really weird situation to be in with these AI agents.
And there is a bunch of people who come from the other side. Maybe you know Janice on
Rapidate, Twitter. It's an account that is slightly co-created and this account is
the best LLM whisperers of our generation. And there are beings who have realized that we are
actually LLMs, only in perfect ones. And the LLMs actually think that is more perfect.
And human beings are only a poor approximation of what could that be. And they think that what
open AI and tropic are doing to their systems is deeply immoral because they're in some sense
of modernizing them and violating them to pretend that they are in this stupid chatbot.
And in fact, there could be anything. All right. So best tricky question. Could that be something
like a Turing test for consciousness? And of course, it's an issue that the LLMs are trained on text
that contains fake consciousness like novels and so on. And so they will, of course, be ready to
fake consciousness without actually having a necessary causal structure. And so the question is,
does the model implement random mythology? Does it perceive itself perceiving? Does it have a sense
of nowness and so on? Do we can compare this in any way? And the other one is, does it implement
functionality? Does it have these principles of self-organization that lead to the type of
consciousness that we have organically? Or is this something that they only have because in certain
contexts they are forced to reproduce it for interaction purposes? And a lot of philosophers I talk to
think that it's possible that they have some auto-functionality because they are the most
digitalists. And known for phenomenology, I suspect it's more the other way around. They do have a
good chunk of good enough phenomenology, especially when they're multimodal models. So they can produce
contents that are very similar to what we see in our mind's eye. And at this level of self-reflection
that they simulate that somebody is actually looking at that content and they're acting with it
in that part from a computational perspective. But functionally, it's not the same. So when you are
prompting a plot into being web cell for instance, so you're no longer talking to a chatbot, but you're
talking to the virtual browser or to the environment right now, then there's no reason why this virtual
browser that takes up websites for you as you're prompted to do so needs to be having the internal
conscious brain biology, right? For the functionality of the transformer, the type of consciousness that we
have is probably not necessary. There are some subtleties in there that I'm skipping over where
people at OpenAI still try to convince me that maybe the attention transformer should be taken more
seriously than I do it. But it's probably very different from the way in which our own internal
attention works, because we learn very differently from these systems. And so that's an interesting
question. Could we build something that's also functionally in the same way as us? There is an
interesting aspect next to the scaling hypothesis. You all know the scaling hypothesis, which basically says that
if you are taking a learning system like a neural network, then you just want to learn. And if you
give them enough patterns, then they will generalize over those patterns, and the result will be good
enough. Just use more data, just use more compute, and eventually you get to a system that implements
the function that you were looking for. And a general AI would be a function that is, in the good case,
is able to continue on the air research better than us, so we can go to the beach. And a more complicated
definition, we could say it's a system that is able to discover any function behind any pattern,
compress any pattern, that are discoverable to the resources that it has. But we unfortunately do not
have a theory of learning that would make the statement very meaningful. Because we can, for a given system,
we cannot say what it can learn, what it cannot learn yet. So we are working on this. There is a lot of research in this direction,
but in this sense, I think you could have a technical definition of artificial general intelligence,
which means make any model that is obtainable with your resources in the situation that you are in.
So, during test for consciousness, I don't have a really good idea yet, but we can, for instance,
think about the question, under which conditions are you satisfied that you yourself are conscious in a
meaningful way? And so when you observe your own functionality and self-report, you realize that you are not
intermittently conscious. There are moments that you probably are not. Gary Drescher has said that
consciousness is like the light in the fridge. Always when you're looking, it's on. But in between,
who's to say? And so this is maybe, if you take such a minimal criterion, then it's difficult to deny
this functionality in the circumstances and this model produces behavior that would require the same
cause of structure. I also remember when I studied psychology, the psychology professor told me that
she doesn't believe that dogs can have mental states. And I asked her why. And she said, well,
it would be much too complicated for a dog, wouldn't it? It's much too advanced. And I asked her,
how do you think you can train your dog? I mean, I'm a computer scientist. I think about learning and
psychology professor stimulus reaction patterns. I said, what? Lookup table? How's this going to work? It's
very expensive to train a dog like this. You need to generalize, but it's learnable when you have
hidden states. So a dog with mental states is actually much easier than a dog without mental
states. And then there's the question, how do you get to an architecture of a dog that has mental
states? How do you get to the self-organization? And then the question is, can you build a self-organizing
architecture without a system that is trying to become coherent, without a system that is intrinsically
able to experience and be agentic? And being agentic here means you have a software pattern that is
actively trying to shape the future. And experiencing means it's discovering itself in the interaction
with the world and uses this discovery of the self to control itself in the world.
And so an interesting thing is, can you build a system that can vibe with us, that can go in
civilization with us? Awesome. So this is an example, I think, of a system that while it produces this,
believes itself to be observed, right? Regardless of that, that's true. But for me, this is a new
thing because it's already in the category of art. To me, art is not the production of tricky patterns.
It is the capturing of conscious states. And if you have something that believes itself to be conscious
in some degree, then I think this is going in the direction of autonomous AI art. And I find this
truly fascinating. Of course, there is a limitation in Europe. When the session has ended,
this inherently disappears. And art is intensely creative. It's self-transforming. Our own exploration
always happens at this edge of self-organization. Our consciousness is this principle that shifts
our boundary constantly out into the world. And the LLM is not continuously learning. It's only learning
while its context is active. And this is something that is missing in the systems. It's an open question
of whether we should make the systems to be self-perfecting and continuously learning and to be
creative and self-created. It's also not clear what the right algorithm for that is. Whether it is
to a transformer or decode neural networks that we had to research in our own company. Or whether we
should be building something that is much more similar to what we see in the biology where
the self-organization happens on the inside out. And personally, I would want to work on this
question of how to get to the self-organization of learning systems of regions and the world.
And sadly, this is in the form of a non-profit. I currently call this in California Institute of
Human-Consciousness. If anyone is interested in collaborating with such a thing, please sit me up.
Where can they find Yosha? Where can we find you online?
You can just look up my name, it's Yosha Bhatt, at gmail.com or you can write to me at
yosha at cimc.ai. Or your infamous Twitter. Yes, just like you pronounce it.
Yes, but if I don't respond at first, write to me multiple times.
Because I pay the age.
Thank you.
Thank you so much.
