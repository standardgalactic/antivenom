Hi everyone, my name is Augustia Chenoy.
I am an AI Research Fellow with the Teaching Lab Studio,
which is an education-focused R&D organization
that's in my day job.
My academic background, aside from AI and machine learning,
is in evolutionary cognitive psychology and consciousness.
So this was really my cup of tea.
One thing I'll say is I elected to go first
because this is probably the most poorly thought out
presentation you'll see today.
I didn't actually get to participate in the full hackathon,
but I was here Friday night,
and some of these speakers really sort of inspired me.
There were some ideas that really just like
worked their way into my brain.
Couldn't stop thinking about them as my wife and son
were carving jack-o-lanterns yesterday.
So, yeah.
Who were the speakers?
Yosha Bach, specifically.
This will be, this is about consciousness.
Yosha Bach really had some great ideas,
along with Gil and-
Manolis Callis.
We're talking about latent space stuff.
This is all sort of like burst out of that conversation.
So I say that as a disclaimer, tenuous right here.
I think this is a really interesting idea.
This is an idea that I want to sort of present to you all.
You are the first humans to hear this idea.
Claude and Chad GVT have assured me that I'm a genius,
but I'm not so sure.
Okay, so I want to start by first motivating the problem.
And I'll start with a set of observations.
The first observation, and I'll say again,
I'm making blanket statements here.
I'm saying, presenting things as fact.
Please sit on your hands until the end.
Okay, one observation is over time, systems tend to move towards maximizing ways of sensing
and perceiving the universe, maximizing accuracy of predictive models about the universe, and maximizing
ability to transform the universe.
So when I say systems, I mean biological or artificial systems.
So that could be an octopus, it could be an LLM, and these systems can be discrete or continuous.
So we're talking about scales.
These are sort of familiar concepts to people who have talked about consciousness at different scales,
but we're talking about cells as well as like entire societies.
So when I talk about systems, these systems, whether they are single cells or massive elements,
or societies working together, these things tend to move in this direction, to sense more,
to model more, and to transform more.
Okay, that's observation number one.
Observation number two, this comes a little bit from Yosha's talk, a little bit of an extrapolation
on that, but subjective experience does not require coherence.
What exactly do I mean by that?
First, with respect to time.
So the I of yesterday is not the same as the I of right now, right?
That's what Yosha talked about.
When we really think about it, like physically we're not the same, right?
The conviculation of the atoms are slightly different.
We also are just not the same thing.
The consistency is sort of like a useful fiction that we'd invent for ourselves.
There's also not required coherence with respect to activity.
So when I'm in a flow state, that I is quite different than the I when I'm outside of the flow state.
Anyone who has experienced the flow state might recognize that feeling that like when you reflect on your activities then,
that felt like maybe a different type of consciousness that is not necessarily coherent with your current.
And then finally, with respect to complexity, so the unit I is subsumed by the system I.
What I really mean by that is, let's take the society example, there is the I myself,
but when we are looking at the scale of society, that gets subsumed by sort of the society itself.
Or if we think about cells within a network or any kind of self-organizing system,
we see that the unit I, I, is subsumed by the system I.
So it depends on the level of complexity.
Okay, and then finally, existing theories have gaps, sometimes intentionally, not every theory is trying to explain everything.
But the theories that sort of come up that are really interesting right now in this field of consciousness
don't, at least to me, feel satisfying in explaining everything.
So for example, free imagery principle is about minimizing surprise and that perhaps explains the emergence of better predictive models,
but not necessarily consciousness as a whole.
The global workspace theory explains in the moment subjective experiences, but not necessarily the self.
I'm not going to go, this is not about attacking these things, I'm just saying where the gaps are.
Integrated information theory measures the degree of consciousness, but know why or for what.
So I'm really interested in answering or trying to fill these gaps within the span of whatever, this morning.
So there are some hints before I go into sort of what I think about.
So one is this idea of software agents, Yosha talked about this on Friday.
It's this sort of maybe like a more mature, a safer way to say spirit as he was saying.
So we don't have to worry about animism, but we do see some sort of ideas like that floating around right now.
Behavioral self-organizing systems is similar across systems.
When we think about like the things, the problems that they're trying to solve and why they're trying to solve those problems,
we see that at different scales.
Just the problem itself is different.
And then sensing, modeling, and transforming seem to be consistent functions at all levels of life.
Okay, so where does that bring us?
Can someone give me a time check?
Okay, so this brings me to this framework of information drives.
So what are information drives?
I want to start with a useful analogy, which is much more concrete.
And then from that analogy, I'll get a little bit more abstract to what I'm talking about.
So I use my devices like my laptop or phone to enact my will in the virtual world.
In fact, it's the only way in which I can interact with and affect the virtual world.
It's my only conduit into the internet.
It's the only way that I can actually do the things that I want to do in this world that is real but is virtual.
I also rely on and am constrained by the local compute and architecture of each device.
So the things that I can do on my laptop or anything like the things I can do on my phone
or anything like the things I can do on watch.
But I still use each one of those things to interact with some virtual world.
And as my intended actions in the virtual world get more ambitious, my hardware needs an increase in cutting.
Next, while I use a device for the duration of my interaction with it,
it assumes my identity.
It is me for that purpose.
So what I mean by that is my identity can be distributed across multiple devices
as I use them simultaneously or for different current tasks.
So again, what I'm saying here is that like me, my identity can be distributed across these devices
because those devices are sort of taking on my will.
And then using Yosha's words again, this allows me to assign credits to actions in the virtual world.
That was me that did that.
So in total, I am a driver using vehicles to enact my will in a real space that is otherwise inaccessible to me.
It's the only way I can enact things in that virtual space where those vehicles exist.
Okay, so what does that mean outside of this analogy?
So I'm proposing three information drives.
These are self-replicating patterns of integration that use vehicles to interact with and affect the physical world.
In the same way that I use my laptop to affect the virtual world,
these use sort of physical substrates like us to affect the physical world.
So what are those drives? There's three specific drives.
An interfacing drive, a modeling drive, and a transformation drive.
I'll talk about those more, but what are they driven to do?
What are these drives?
To maximize information about the universe,
to minimize prediction error about the universe,
and to maximize novel configurations of the universe.
Universe with respect to the organism, to the system, right?
Universe to a cell might be a lot different than the universe to, you know, a galaxy.
And then finally, a vehicle in this framework refers to any system with enough computational power and complexity to enable the drive.
So this is sort of like level zero of this framework, again, that hopefully doesn't sound insane.
Okay, so there's one more level about the interactions between these drives.
So first I want to say that I'm going to define within this framework,
I'm going to define life as a system that has the minimum complexity required for all three drives to utilize it.
In any given living system, all the three drives are in competition with each other for computational resources.
And at the same time, the trio act as a flywheel.
The interfacing drive provides data for the modeling agent,
or drive, I've called them agents at one point.
Transformation drive uses the modeling drives predictive models to discover novel configurations,
which provides new data for the interfacing drive to collect.
Crucially, vehicles may develop abstractions unrelated to drives themselves in order to resolve computations.
That's my fancy way of saying that's where our concept of the self comes.
So coming back to the analogy, my phone at times is a calculator or is a newspaper.
At others, it is something even more abstract that I can put my finger on.
And I actually don't care about what its abstract representation of what I'm trying to do is,
as long as it can do the thing that it needs to do.
So in the same way, well, look at that in a second.
So these are sort of like the, I don't know, laws of this framework.
So someone in this room is thinking all these things.
What about the self?
So I think in this framework, the self is a useful abstraction to aid in computation.
For example, like Yosha was saying, we need to have some sort of continuity of self to understand our past actions,
that kind of thing.
So perhaps the self is a abstraction that is made in order to do the computations that these drives are pushing us to do.
That doesn't make it any less real.
I'm not saying subjective experience isn't real.
I think subjective experience is real.
But it was also sort of created out of this framework.
What about individual agency?
I guess in this framework, that's a fiction.
What about AI?
I think this is really interesting.
AI is just another vehicle for information drives to follow their drives and propagate.
Which would mean that we can assume AI is like us once it can achieve the same effects that information drives elicit in us.
So that is like maybe an example of like how we might begin to define conscious AI, right?
So if it can do the types of things that these drives use us to do, then we can assume that there's some sort of abstraction being made within those systems as well.
That is similar to like a self and phenomenological experience and all that kind of stuff.
Is this just a different flavor of psychism?
I don't know.
And ha, give me a break.
I just came up with this.
Yeah, perfect, great.
So finally, I just want to give some testable predictions that could arise from this framework that someone with more time and who doesn't think I'm crazy can go and look at.
So first is vehicle independence.
So the same consciousness patterns should be identifiable across radically different substrates, whether they are biological or artificial.
This is implied again.
These are all applied by the framework that I just presented with you.
A single drive to be able to manifest through multiple vehicles simultaneously.
That's similar to like the analogy when I was saying that I can enact my will on my phone and my laptop and my watch at the same time if I choose.
And the coherence of the information drives should matter more than the coherence of the vehicle itself.
I think that's really compelling when we think about a system at the level of a society where we have individual researchers, for example, who might have sort of like within their own systems.
And but, you know, the vehicle was on.
Okay.
And then this one is sort of like cutesy active inference, active transformation.
I'm saying that these systems actually seek out novelty.
And I think that is something that we do see in nature.
And there's something that I want to try to figure out about like how do we square that with things like minimizing surprise with the free energy principle.
I won't read all these aloud.
But that is it.
That's information drives.
I'm Auguste Hichnard.
You can say something that's there.
One really good question.
What?
Or judge question.
Actually judge question.
Judge question.
Any judge question.
Kurt, Michael, Nick.
Kurt.
So you said that the self is a useful abstraction.
Then you said that that doesn't make the self any less real.
Yeah.
real yeah however most of the time when people say so-and-so is a useful whatever they they use
that as a way of saying that something's not real but politely sure they'll say something's
a useful fiction yeah what's a useful illusion or useful construct right so you just use it's
a useful abstraction uh-huh how does that make not i mean how does that not make the self not real
um i i think what i'm trying to say here is that the way we like conceive of ourselves and like the
self arise i think i'm trying to explain how the self arises i think that's like an open question
as to like how consciousness arrives in general but specifically like like me not like you right
like the the self-contained self and i think that is a necessary computational um artifact in order
to do the computations uh but i think like the sort of like side effect of that is to sort of
have phenomenological experience and the phenomenological experience is happening
it like it almost doesn't matter if it's real or not i think is what i'm saying
yes yeah can you uh cite the the best evidence data wise in terms of being simulated environments or
things to have a tendency or emergence towards spontaneously uh greater prediction from your like slide three ish
spontaneous greater prediction i i've definitely discussed the tendency for it sure sure i i i i'm
definitely using sort of like evolutionary history as existence proof kind of like where where we see
action in artificial systems for that also so sure so so in that case we can look at sort of like as we
scale the lms parameters and train data we also see increased problem solving abilities like there's
lots of things that we can go down whether that's true or not um in terms of like do
emergent behaviors exist um but that's sort of like the the um the gut that i'm going with
and my follow-up question using that example you just used you see that as something that is a
byproduct of programmers deliberately trying to design systems that are capable of predicting
the future or are you describing this as an emerging property independent of that yeah i think i think
what i am trying to connect um is the idea that like these these drives are using whatever physical
structure use and create whatever physical substrate will allow them to enact their drives
and therefore like humans right now are sort of like the most efficient way to do that
um and as we create more and more efficient and capable systems um that allows those drives to
also like sort of inhabit and use them judge questions we have good one i know the judge
questions all right so we have to move on because of time sorry but you'll be available thank you for
listening
