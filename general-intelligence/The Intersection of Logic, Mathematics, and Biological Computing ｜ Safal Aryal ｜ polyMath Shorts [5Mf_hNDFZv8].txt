Hi, my name is Safal, and I'm studying math and philosophy here at Tufts.
My focus is the common structures underlying human reasoning.
What do I have in mind when I'm saying this?
At the turn of the 20th century,
mathematicians and philosophers realized a profound connection
that existed between logic and mathematics.
Mainly, that you could take mathematical statements
and you could formalize them within a logical system
like the one that had been considered by Aristotle,
and that these formalizations had properties that you could study
by proving theorems about them.
The remarkable thing about this is before this turn of the 20th century transition,
logic was largely seen as something under the purview of metaphysics.
Not that I consider that to be a bad word,
but in the mind of many philosophers,
it evokes associations with a non-rigorous, non-thorough kind of reasoning.
The kind of philosophy that I'm pursuing,
which is in line with the tradition that GÃ¶del and Hilbert established
in the early 20th century,
seeks to look at structures that scaffold human reasoning
and understand them mathematically.
Now, what do I mean by this?
Say you're solving a problem on your math homework.
The kind of reasoning you use there
is not going to be the same kind of reasoning that you use
when you're teaching a child something, for example.
Nor is it going to be the same kind of reasoning that you use
when you're trying to persuade another person of your point.
More technically, it's not going to be the same reasoning that you use
when you're trying to allocate resources in a certain scenario,
for instance, a political scenario or a computer program.
All of these different types of scenarios share one thing in common.
That is, there can be things that are true of these scenarios
or false of these scenarios.
What non-classical logicians study
is essentially given a situation about anything at all,
how can you parametrize a system of reasoning
associated with that situation?
So the earliest example of a non-classical logic
comes out of intuitionistic logic,
and I'm going to talk a little bit about that history.
Classical logic has two major rules
that ensure its bivalence,
essentially that every proposition in classical logic
is either true or false.
One, you have the law of excluded minimum.
So it is always true that either A is true
or the negation of A is true.
And you have the law of non-contradiction.
That is, it is always the proposition
stating that A and its negation are always true
is always false.
For instance, if you assume the negation of a statement
and you show that leads to a contradiction,
you can show that the statement has to be true
without actually providing any constructive reason,
any constructive proof,
you know, a proof that gives you an insight
into the statement itself
about why this statement ought to be true.
And so, in the early 20th century,
Brewer, John Brewer,
developed intuitionistic logic,
a system that rejects the law of excluded middle
and focuses only on constructive proofs.
Now, as it was later found in the 1950s, 60s, and 70s,
it turns out that this intuitionistic logic
had deep connections with another area of mathematics
that was rising then.
And this is category theory.
Now, this connection is quite remarkable,
so I'll explain it.
But category theory is a branch of mathematics
that doesn't understand objects
by properties that you look at by examining them,
but instead by seeing the ways
they can transform to other objects.
The analogy the mathematician Ravi Vakil used,
which I like,
is that you understand a particle in a particle accelerator
by throwing it at a bunch of other particles.
And once you've thrown it at every other possible particle,
you basically understand via its interactions
everything you want to understand about the particle itself.
And category theory sort of does this for mathematical objects.
It lets you throw mathematical objects at other ones,
and these maps between objects
determine the object up to isomorphism,
or up to objects that share the same maps with others,
structural equivalence in a sense.
The connection between intuitionistic logic and category theory
turned out to be is,
if you put many of the standard objects of mathematics
into categories,
and you look at how these categories behave internally
with respect to their internal logic,
this logic turns out to be intuitionistic.
And indeed, it's not just normal intuitionistic logic,
it's higher-order intuitionistic logic.
All a higher-order logic is,
is a logic that lets you reason not only about propositions,
but collections of propositions.
So you can say,
for all objects, this proposition has to apply, and so on.
And this kind of led to an explosion in the research community
for mathematical logic,
because mathematicians could take many of the techniques
they developed from category theory,
which were originally developed to solve problems
in algebraic geometry, in algebraic topology,
and apply them to logic.
And so you get this categorical semantics for many logics,
a category-theoretic way of understanding these logics.
And the reason these semantics are good
is because category theory is supernatural
to most mathematicians working in these areas.
Mathematical logic otherwise had the reputation
of being a bit unnatural.
So last summer,
I started the research project
of looking into a categorical semantics
for paraconsistent logics.
All a paraconsistent logic is,
is it's a logic where your propositions
can be both true and false at the same time.
That is, in intuitionistic logic,
if you reject the law of excluded middle,
in paraconsistent logic,
you reject the law of non-contradiction.
And this rejection of the law of non-contradiction
in classical or intuitionistic logic
would allow you to prove any arbitrary proposition,
but ideally,
in a well-behaved paraconsistent logic,
you shouldn't be able to prove any proposition
by rejecting non-contradiction.
Philosophers have been trying to give semantics
for paraconsistent logics
since the 1960s and 70s.
And the most popular of these options,
and even before that,
like, you know,
as early as the 30s and 40s,
the most popular among these options
are many-valued logics,
where, you know,
you have one truth value true,
another one false,
and another one you,
for either both true and false,
or neither true nor false.
Because the problem with these logics
is that their proof systems aren't very nice.
For instance,
a commonly used rule
that comes from the proof systems
for classical and intuitionistic logic
is modus ponens.
Let's say you have a proof that P implies Q,
and you have a proof that P is true.
Then Q is true,
because P implies Q is proven,
and P is proven,
thus Q must be proven.
You can't actually do that
in most of the many-valued logics.
And so,
in giving this categorical semantics
for pair-consistent logic,
I wanted to try to construct a proof system
where you could do this.
And it turns out
that some work towards this
had already been done,
although a characterization was missing.
Essentially,
if you take any topological space,
a topological space
is just a mathematical structure
which consists of points
that are either near or far
to one another in a certain way.
The open sets of these topological space
or sets were, you know,
so sets you can just think of
as containing points
and containing sequences of points.
An open set
is a set that contains sequences
but doesn't contain the limits
of these sequences.
So logically,
you can imagine open sets
as consisting of worlds,
worlds where propositions are true,
but these open sets
don't contain the boundary cases
where propositions are both true and false.
And it turns out
that if you take the collection
of open sets
from any topological space,
these form a hating algebra.
And hating algebras
sort of give you the semantics
for intuitionistic logic
and let you do all those nice things
with category theory.
It turns out
that if you look at the logic
of closed sets,
sets which contain
the limit points
of their sequences,
you get a paraconsistent logic.
Because if you look at a set
and you look at all the,
if you look at a closed set
and you look at all the points
that are not inside the set,
but then you take the closure,
you take all the limits,
you can get a boundary
that's contained,
that's shared with both sets
or essentially in a sense
A and not A can be true.
And while this connection
has been understood
for this time,
sometime the algebraic properties
and the categorical properties
of such logics
are not well understood.
So to better understand
how our human minds
scaffold contradiction
and how we might formally
represent contradiction,
I have for some time
been looking into
the categorical,
algebraic,
and topological aspects
of paraconsistent logics
and how they should be modeled.
And in doing so,
I found some intriguing connections
to other subject areas.
For instance,
the topological semantics
for intuitionistic logic
are deeply connected
to S4 modal logic
or the logic of reasoning
about necessary
and possible propositions.
My intuition
is a dual intuitionistic logic,
which is a logic
of close sets,
the paraconsistent one
that I mentioned
will let us reason
about impossible worlds
or worlds that can't
be actualized.
And Stephen Awodi
and Kohei Kishida
at Carnegie Mellon University
and the University of Illinois
at Urbana-Champagne
have already done
some of this work
for intuitionistic logic.
So I'm trying to see
if I can extend this connection
to modal logic
for the dual intuitionistic case.
Moreover,
there are also intriguing connections
to computational complexity theory.
The unprovability
of lower bounds
for the Boolean
satisfiability problem,
certain lower bounds,
they're quantitative,
can be proven
in intuitionistic bounded arithmetic.
where intuitionistic bounded arithmetic
is basically
a restricted theory
of natural number addition
and multiplication
formulated
in intuitionistic logic.
And it's still open
whether this holds
for classical logic.
But in dual intuitionistic logic,
we get an intriguing setup.
The law of non-contradiction
does not hold in general,
but the law of excluded middle does.
So if you take the intersection
of intuitionistic
and dual intuitionistic logic,
you get classical logic.
So in trying to prove
these bounds
for classical logic
or classical theories
of bounded arithmetic,
it might be productive
to first construct
a co-intuitionistic theory
of bounded arithmetic
where we can prove these.
And this is what my research
is trying to do at the moment.
I'm also more interested
in modeling
how non-classical logic
might manifest
in cognitive
or biological contexts.
For instance,
contradictions arise
in our reasoning all the time.
We deal with incomplete reasoning
all the time.
And so applying
these modal models
of non-classical
pair consistent logic
to situations
like belief revision theory
or even biological contexts
where you can assign cells
as having beliefs
much in the vein
of what Professor Manolis Kellis
at MIT is doing
is also something
that interests me.
Overall,
I'm interested
in understanding
the scaffolding
and structure
of human reasoning itself,
developing a broad canvas
through which we can understand
the convergence
of these topics
through logic.
And given how foundational logic
is to the rest of mathematics
and to intellectual inquiry
as a whole,
I think this quest
will be fruitful.
I think Echolopto
has given me
an extremely like-minded
group of people
to discuss these topics with
at the recent
Cognitive Hackathon.
There are so many people
who are interested
in the easy ideas
related to logic
and it's natural
if it is a human mind
that we frame
the rest of our
intellectual inquiry through.
Understanding how
the human mind
takes its intuitive soup
and gives it rigorous shape
is a first step
to understanding
anything else
about how it
construes phenomena
and it's that mission
that keeps guiding
my intellectual inquiry.
Sometimes, though,
studying the scaffold
of human reasoning
isn't enough.
You need to see
how different types
of reasoning
actually manifest out there
in the natural world.
And this is a key part
of why I do my research
at the Levin Lab
under Dr. Hanan El-Hazan
to try to understand
how networks of cells
communicate with one another.
In biology at large,
there is an increase
in recognition
that intelligence
can perhaps be understood
as a collaborative effort.
You know,
if one agent
knows something
and another agent
knows something else,
taken together,
those two agents
know more
than each individual
agent knows.
In a sense,
the whole is exactly
what you get
when you conjoin the parts
and these agents
can coordinate
with one another
in ways that individual
agents can't even imagine.
When I look at
cell communication
with Dr. Hanan El,
this is exactly
what I'm trying to examine.
I'm trying to examine,
we're trying to examine
how cells interact
in a network
to solve challenges
within the body.
And of course,
the point of understanding
how they collaborate
to solve challenges
within the body
allows us to abstract
and consider
how networks of agents
working together
might collaborate
to solve problems
in general.
You can think of this
as, you know,
inputs to an algorithm
all forming
a sort of network
with one another
and trying to use
that network
and leverage
the computational properties
of that network
to come to a conclusion.
This is exactly
what the area
of biologically inspired
computing tries to do
and it's another
big interest of mine,
biologically
and more in general
scientifically inspired computing,
which is looking
at actual properties,
looking at actual
processes in nature
where agents
seem to be interacting
with one another
to a common goal.
What common features
can we extract
from these
to build computational models
and what is the power
of these computational models?
I like this work
because it involves
a good mix
of statistical work
in separating the signal
from the noise
when looking at
actual natural data
but also a lot
of theoretical work
because the characterizations
of the expressive power
of these computations,
right,
what they can do
is something
that is more principled
and more theoretical
in nature.
This field was started
in the late 90s
when computer scientists
first started to look at,
you know,
DNA networks
and saw that DNA networks
could actually approximate
the traveling salesman problem
but the possibilities
are limitless.
Like I recently looked
at a paper
where people look
at certain classes
of automata,
register machines
and they associate
these register machines
with networks
of chemical reactions.
So it is this view
of nature,
it is this view
of nature as a symphony
where each part of nature
is contributing
towards a greater whole
and, you know,
in a way that our human minds
can apprehend
that motivates me
to study
what I study
at the Levin Lab.
Superficially,
these might seem
like very different
strands of work.
You know,
my logical work
is highly abstract
but this work
is highly statistical
but in fact,
I see myself
as doing roughly
the same thing.
I'm just looking
at a different class
of agents.
Instead of looking
at how my own reasoning works,
I'm looking at how
the process of reasoning
at large
manifests in nature itself
and in this sense,
I kind of converge
towards a Spinozist
understanding
or not Spinozist
understanding,
you know,
a more Kantian
or Hegelian understanding
of reality
as mind.
Not one huge mind
in the way that,
you know,
more,
not in any nebulous way
but in really
a precise way.
A network
of interacting components
all of which
know things
but that might know
different things
when put together.
And I think
this is really
what ties together
the threads
of my research
as a whole.
What can individual
agents know
or derive
and what can they
derive
when considered
as a collective?
And at Eccolopto,
we are also mirroring
this because
we are a collective
of thinkers
who are imaginative
and creative
that all put out
things together
that we could not
put out individually
but yet bring our own
individual strengths
to the table.
to the table.
