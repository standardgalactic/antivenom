Gemini
Gemini
2.0 Flash
Try Gemini Advanced
New chat
Recent
Computational Sovereign's Grand Proclamation
Nerozumiem, Potrebujem Kontext
Biomimicry in Algorithm Development
Installing Cool-Retro-Term on WSL
Sky's Blue Color Explained
Less
Unclear request, contact call offered.
Function Alignment: Mind and Intelligence
Surprise in AI: Two Meanings
Correcting Starfield Visual Effects.
Code Fixes and Enhancements
Dark City's Controversial Themes
GitHub Follow Limit Frustration
FFmpeg Padding for Wide Photos
Heterodox Text Color Correction
Energy and Bergson's Duration
Load more
Gems
Chess champ
Brainstormer
Career guide
More
Grand Bay-Westfield, NB, Canada
From your device
•
Update location
Conversation with Gemini
Home / Chroniques / Algorithms: a biomimetic approach to performance and nuance
π Science and technology π Health and biotech π Planet
Biomimicry: when science draws inspiration from nature
1 2 3
Algorithms: a biomimetic approach to performance and nuance
Clément Viricel, Doctorate in mathematics and computer science applied to biology and Laurent Pujo-Menjouet, Lecturer and researcher in mathematics applied to biology and medicine at the Université Claude Bernard Lyon 1 and senior lecturer and researcher at the Institut Camille Jordan
On October 25th, 2023 |  4 min reading time
Clément Viricel
Doctorate in mathematics and computer science applied to biology
Laurent Pujo-Menjouet
Lecturer and researcher in mathematics applied to biology and medicine at the Université Claude Bernard Lyon 1 and senior lecturer and researcher at the Institut Camille Jordan
Key takeaways
Algorithms are biomimetic systems, since they are closely linked to the way neurones work.
Biomimicry is used in development of many algorithms, such as "genetic" algorithms and convolutional (or recurrent) neural networks.
Inspired by humans, researchers have sought to improve the speed of algorithms by adding an "attention layer" to neural networks.
The challenge for the future is to reduce the energy footprint of these innovations.
Biomimetics is no stranger to the rapid progress and breathtaking performance of today's algorithms. But the IT community is still struggling to integrate the true power of the living world: its energy efficiency.
Biomimetics has been part of the history of algorithmics since its earliest developments. "In 1964, the first neural network, the perceptron, was already biomimetic. It sought to reproduce the electrophysiological properties of neurons, their excitability and ability to transmit information", explains Clément Viricel, lecturer at Lyon University. Each neuron receives data, evaluates it and produces a result according to the function specified in the algorithm. This process constitutes the "activation" of the artificial neuron, just as a neuron is activated in the brain by nerve impulses. In the perceptron, the neurons were connected in a single layer. It was by multiplying the layers of neurons that it processed the flow of information.
Neural networks
From the 1990s onwards, training algorithms adopted these neural networks in an attempt to reproduce the way in which humans learn. "Neural networks are biomimetic because they learn by error, rather like humans or babies. Plasticity can be represented by matrices whose elements are weighted according to success. The coefficients play the role of reinforcement between neurons", explains Laurent Pujo-Menjouet. Clement Viricel adds, "For example, when learning a language, humans often discover the meaning of a word through context. Semantics play a crucial role. This is what neural networks began to do, by being trained with texts in which a word was missing. Then they were optimised by backpropagation". In other words, by correcting the weights of the input neurons according to the output results. "But this process is a black box, where the variations in weighting that enable the algorithm to evolve are not visible," adds Clément Viricel. And we know that it's difficult to trust a process if you don't understand how it works. These methods are a major headache for underwriters in charge of products that incorporate them, such as autonomous vehicles1 or diagnostic assistance systems2.
Biomimicry contributes to the development of a large number of algorithms. These include so-called "genetic" algorithms, which are based on phylogenetic trees for calculation purposes, and enable the most relevant result to be selected according to several methods (by rank, tournament, adaptation, etc.). Systems such as these have been deployed in search of optimums, but also in the development of games, such as the famous Super Mario, to rank the players amongst themselves. There are also convolutional neural networks, inspired by the human visual network. "Its developers wanted to reproduce the way in which the eye analyses an image. It's a square of neurons, which scan the image to capture the pixels before reconstructing it in its entirety", explains Clément Viricel. This tool is renowned for having surpassed the expert eye, particularly in the diagnosis of melanoma3. How does it work? "During the training period it extracts characteristics such as 'tumour shape' and 'tumour size'. Then it looks for these characteristics to recognise a particular object", explains Clément Viricel.
These biomimetic algorithms are applied to all subjects, as exemplified by recurrent neural networks. "They are used to analyse data sequentially or over time.  It is widely used for automatic processing of texts, by taking word order into account. Dense layers are recurrent so that the network doesn't forget what it has done before", explains Clément Viricel. Such networks have been used to build machine translation tools. A first recurrent network "reads" and encodes the text in the original language, a second recurrent network decodes the text in another language, all of which takes time and energy. "They require a lot of energy to train", admits Clément Viricel.
Transformers
So, we need to be able to learn faster. Specialists therefore devised a way of reproducing lexical dependency: when a human learns a text, they implicitly know what each pronoun refers to. This makes sentences lighter. "To reproduce this, we had to add an extra layer of neurons, the attention layer. And this is the parameter on which the latest biomimetic evolution has taken place", explains the specialist. The inventors of these new artificial intelligences have titled their article "Attention is all you need". In fact, their network consists of just 12 layers of attention and an encoder/decoder system. These networks are called "transformers", and are the models used by Google's Bert and Bloom, the Hugging Face start-up founded by three Frenchmen. (Chat-)GPT is a direct descendant of the transformers, although it has only the decoder and no encoder.
This whole story is a good example of how biomimetics has fed into algorithmic innovation, while forgetting one of the essential characteristics of living organisms: energy efficiency. For example, training GPT‑3 Cat required 1.287 MWh and emitted 552 tonnes of CO24 "Until now, developers haven't been at all interested in the energy footprint of their networks", admits Clément Viricel. "It's a skills problem. The people who design the algorithms are not the same people who build the physical components. We forget the machine aspect. Recent tools consume an enormous amount of power... and the next systems, TPU or HPU, won't be any more environmentally friendly," explains the specialist.
The change could come from the next generation of programmers. "We are seeing the emergence of a movement in the community to address this issue. On the one hand, this is due to the need to optimise energy consumption, but also for ethical reasons. For the moment, the improvements are purely mechanical, based on energy transfer", explains Clément Viricel.  But other avenues are emerging, such as zero-shot shock zero-shot learning algorithms: "They work without training, which saves on the cost of learning", adds the specialist. It remains to be seen whether their performance can compete with that of their predecessors to produce totally biomimetic systems.
Agnès Vernet
This article discusses how biomimicry, the practice of drawing inspiration from nature, has significantly influenced the development of algorithms, particularly in the field of artificial intelligence. Here's a breakdown of the key points:
Biomimicry in Early Algorithms:
The concept of biomimicry has been present since the early stages of algorithm development.
The perceptron, one of the first neural networks, aimed to replicate the electrophysiological properties of neurons.
Neural Networks and Human Learning:
Neural networks are biomimetic because they learn through trial and error, similar to humans.
They use matrices and weighted coefficients to simulate the reinforcement of connections between neurons, mirroring human brain plasticity.
Neural networks are trained using backpropagation, which involves adjusting the weights of input neurons based on output results.
Various Biomimetic Algorithms:
"Genetic" algorithms draw inspiration from phylogenetic trees for optimization and selection.
Convolutional neural networks mimic the human visual system's image analysis process.
Recurrent neural networks process sequential data, similar to how humans process language, and are used in machine translation.
Transformers and Attention Layers:
To improve algorithm speed and efficiency, researchers have added "attention layers" to neural networks, mimicking how humans focus on relevant information.
Transformers, a type of neural network using attention layers, have led to significant advancements in natural language processing.
GPT models are descendants of transformer models.
Energy Efficiency Concerns:
Despite their performance, current algorithms, particularly large language models like GPT-3, have a significant energy footprint.
There's a growing awareness of the need to reduce the energy consumption of algorithms.
Researchers are exploring methods such as zero-shot learning to improve energy efficiency.
Future Directions:
There is a growing movement within the programming community to address the energy consumption of AI.
The development of more energy efficient algorithms is a goal for future programmers.
In essence, the article highlights the successful application of biomimicry in algorithm development, while also emphasizing the importance of addressing the energy efficiency challenges associated with these powerful technologies.
Polytechnique insights
A REVIEW BY INSTITUT POLYTECHNIQUE DE PARIS
FR  EN
π Planet
π Energy
π Health and biotech
π Digital
π Space
π Economics
π Industry
π Science and technology
π Society
π Geopolitics
π Neuroscience
π Videos
π Magazine
Newsletter
Home / Chroniques / Drawing inspiration from living things: a method as much as a philosophy
π Science and technology π Health and biotech π Planet
Biomimicry: when science draws inspiration from nature
1 2 3
Drawing inspiration from living things: a method as much as a philosophy
Kalina Raskin, Physico-chemical engineer, PhD in neuroscience from Sorbonne University and Managing Director of Ceebios, (Centre for studies and expertise in biomimicry) and Emmanuel Delannoy, Consultant, author, lecturer, founding partner of Pikaia, Secretary General of the French IUCN Committee and founder of the INSPIRE Institute
On October 25th, 2023 |  3 min reading time
Kalina Raskin
Physico-chemical engineer, PhD in neuroscience from Sorbonne University and Managing Director of Ceebios, (Centre for studies and expertise in biomimicry)
Emmanuel Delannoy
Consultant, author, lecturer, founding partner of Pikaia, Secretary General of the French IUCN Committee and founder of the INSPIRE Institute
Key takeaways
Biomimicry is a method of conception that requires an understanding of biological systems at all scales.
This vision of innovation has an ecological dimension: energy sufficiency, adaptability, and sustainability.
However, "biomiwashing" is undermining the credibility of this concept in favour of commercial interests, without any real scientific basis.
In order to discuss the future and challenges of biomimicry, CEEBIOS and MNHN have created a programme to bring together laboratories and industrialists.
Creating adhesives as strong as those of a gecko, threads as solid as spider silk, molecular detectors with the precision of a shark's sense of smell - these are some of the possibilities afforded by biomimicry. But that's not all because, beyond the sensationalism of certain products inspired by nature, biomimicry is also part of an ethos and a global approach to innovation. "Biomimicry begins with an understanding of biological systems, both on a molecular level and in natural environments. The knowledge acquired can then be transferred to other fields of application, so it is first and foremost a design method. It's also a philosophy," explains Kalina Raskin, Managing Director of Ceebios (Centre d'études et d'expertise en biomimétisme).
"This approach involves drawing inspiration from living things to devise solutions compatible with ecological factors," agrees Emmanuel Delannoy, a pioneer of biomimicry in France and a specialist in biodiversity. "Living things offer a set of specifications that are relevant to today's economic and ecological challenges", he adds. This is because they depend on the renewal of raw materials and store CO₂ in a very energy-efficient way. "Living organisms only produce materials from atoms that are in plentiful supply, at ambient temperature and pressure, and with a very high degree of material circularity," explains Kalina Raskin. "Life forms have adapted to all environments and constraints. Sustainability is the key issue in this long history of co-evolution," adds Emmanuel Delannoy.
This approach has already led to the development of products that will soon be on the market, such as propulsion systems inspired by the movements of fish (EEL Energy's tidal turbines1, FinX's motors2 and even Corwave's heart pumps3), products for agriculture based on microalgae (CarbonWorks' CO₂ capture systems4, Immunrise's antifungals5 and Algosource's bioactive ingredients6) or insects (M2i Life Science's biocontrol products7), or even new technologies (Prophesee's retina-inspired camera8, or Biomemory's DNA-based data storage9).
Ecosystems don't favour simplified systems
Nevertheless, biomimicry is still not yet a term that is well understood. And it should be known that it is not currently being used with the intention of greening any process that even remotely resembles a living object. "We call this trend biomiwashing," says Kalina Raskin. More and more "nature-inspired" products are appearing on the market, and they have more to do with storytelling than with a scientific approach. "The use of biomimetic terminology without any real scientific basis undermines the credibility of the approach", insists the director of Ceebios.
Biomimicry is now subject to the ISO 18458 standard, which provides a framework to use the term. Ceebios, in partnership with ADEME and the Saint-Etienne eco-design cluster, is also preparing "an introductory guide to biomimicry for eco-design". For Kalina Raskin, the aim is to "explain the importance of the approach and encourage manufacturers to measure the biomimicry component in their processes".
500px Photo ID: 107350355 -
Emmanuel Delannoy suggests going even further. "Ecosystems don't favour simplified systems," he points out. They tend towards complexity and diversification, establishing balances and sharing resources. This goes against the grain of current industrial production methods, which discourage diversity.  The specialist imagines systems whose efficiency will not be measured on performance alone, but capable of moving towards greater complexity, tolerating diversity and being more frugal. "It's not necessarily the optimisation of an output that counts, but the balance of the system", he explains. He is thinking, for example, of logistics systems that combine complementary solutions to adapt to different conditions on the ground.
Biomimicry could also develop by broadening its models. The biological objects most studied from a biomimetic angle are the gecko, the shark, the lotus, the eagle, the kingfisher, the Japanese fugu, spider silk, the flipper of the humpback whale... "Experts estimate that the Earth is home to more than 20 million species. But only 2 million have been identified, and very few have been studied at the interdisciplinary interfaces," points out Kalina Raskin.
A fast-growing concept
Today, advocates of biomimicry want to incorporate the method into industrial toolkits. "To move from being a novelty to a routine process, we need to increase the number of biological models", adds Kalina Raskin.
In response, Ceebios has set up a programme with the Museum national d'Histoire naturelle (MNHN) funded by the Banque Publique d'Investissement. The aim is to bring together industry and research laboratories to jointly address the issues at stake. "For example, we need new antifouling coatings, materials with better mechanical resistance, and new agents for colour opacity. To understand these industrial issues, we are working with researchers at the MNHN to find organisms whose biology we can examine", explains Kalina Raskin.
Progress is being made in the development of the concept. However, in order to become truly established, it will have to overcome a major risk. "There is a strong bias towards technological solutions, the idea that there is a solution to every problem, and a technological one at that, without taking a critical look at it", explains Emmanuel Delannoy, "this kind of thinking is not compatible with the ethics of biomimicry. "We must always ask ourselves whether it is really necessary, by taking into account the solution to a real current need and the human and ecological challenges of the future," insists the specialist.
Agnès Vernet
1https://www.eel-energy.fr↑
2https://www.finxmotors.com↑
3https://www.corwave.comhttps://www.corwave.com↑
4https://carbonworks.bio/en/↑
5http://www.immunrisebiocontrol.fr↑
6https://algosource.com↑
7https://www.m2i-lifesciences.com↑
8https://www.prophesee.ai↑
9https://www.biomemory.com↑
This article explores the concept of biomimicry, detailing its methodology, philosophical underpinnings, and its potential to address contemporary ecological and industrial challenges. Here's a summary of the key takeaways:
Biomimicry as a Method and Philosophy:
Biomimicry is described as a design method that involves understanding biological systems at various scales, from molecular to environmental.
It also encompasses a philosophy of innovation that seeks solutions compatible with ecological principles.
Ecological Dimension:
The article emphasizes the ecological relevance of biomimicry, highlighting how living organisms offer models for energy efficiency, adaptability, and sustainability.
Living systems utilize abundant raw materials, operate at ambient conditions, and demonstrate high material circularity.
Applications and Examples:
The article provides examples of biomimicry in action, including:
Propulsion systems inspired by fish movements (tidal turbines, motors, heart pumps).
Agricultural products based on microalgae and insects (CO₂ capture, antifungals, bioactive ingredients, biocontrol).
Innovative technologies like retina-inspired cameras and DNA-based data storage.
"Biomiwashing" and Credibility:
The article cautions against "biomiwashing," which involves using biomimicry terminology for marketing purposes without genuine scientific backing.
This practice undermines the credibility of true biomimicry.
ISO 18458 standard, and guides are being created to help provide a framework for the correct use of the term Biomimicry.
Ecosystem Complexity:
It highlights that ecosystems favor complexity and diversification, contrasting with current industrial practices that prioritize simplification.
The article suggests that future systems should aim for balance and diversity rather than solely focusing on output optimization.
Expanding Biological Models:
The article points out that only a small fraction of Earth's biodiversity has been studied for biomimicry applications.
There is a need to expand the range of biological models used in biomimicry.
Collaboration and Future Development:
Initiatives like the program between Ceebios and the Museum national d'Histoire naturelle (MNHN) aim to foster collaboration between industry and research laboratories.
The goal is to integrate biomimicry into industrial toolkits and promote its widespread adoption.
Ethical Considerations:
A strong emphasis is placed on the ethical considerations of biomimicry, stressing the need to assess the genuine necessity of innovations and their alignment with future human and ecological challenges.
In essence, the article presents biomimicry as a powerful tool for sustainable innovation, while also addressing the challenges of maintaining its integrity and promoting its responsible application.
AComprehensive Review of Bio-Inspired Optimization
Algorithms Including Applications in Microelectronics
and Nanophotonics
Zoran Jakši´ c 1,* , Swagata Devi 2
1
, Olga Jakši´ c 1 andKoushikGuha3
Center of Microelectronic Technologies, Institute of Chemistry, Technology and Metallurgy, National Institute
of the Republic of Serbia University of Belgrade, 11000 Belgrade, Serbia; olgicajaksic@gmail.com
2
3
*
Department of Electronics and Communication Engineering, B V Raju Institute of Technology Narasapur,
Narasapur 502313, India; swagata.devi@bvrit.ac.in
Department of Electronics and Communication Engineering, National Institute of Technology Silchar,
Silchar 788010, India; koushik@ece.nits.ac.in
Correspondence: jaksa@nanosys.ihtm.bg.ac.rs; Tel.: +381-64-3257-897
Abstract: The application of artificial intelligence in everyday life is becoming all-pervasive and
unavoidable. Within that vast field, a special place belongs to biomimetic/bio-inspired algorithms for
multiparameter optimization, which find their use in a large number of areas. Novel methods and
advances are being published at an accelerated pace. Because of that, in spite of the fact that there are
a lot of surveys and reviews in the field, they quickly become dated. Thus, it is of importance to keep
pace with the current developments. In this review, we first consider a possible classification of bio
inspired multiparameter optimization methods because papers dedicated to that area are relatively
scarce and often contradictory. We proceed by describing in some detail some more prominent
approaches, as well as those most recently published. Finally, we consider the use of biomimetic
algorithms in two related wide fields, namely microelectronics (including circuit design optimization)
and nanophotonics (including inverse design of structures such as photonic crystals, nanoplasmonic
Citation: Jakši´ c, Z.; Devi, S.; Jakši´ c,
O.; Guha, K. A Comprehensive
Review of Bio-Inspired Optimization
Algorithms Including Applications in
Microelectronics and Nanophotonics.
Biomimetics 2023, 8, 278. https://
doi.org/10.3390/biomimetics8030278
Academic Editors: Heming Jia, Laith
Abualigah and Xuewen Xia
Received: 5 June 2023
Revised: 25 June 2023
Accepted: 26 June 2023
Published: 28 June 2023
Copyright: © 2023 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
configurations and metamaterials). We attempted to keep this broad survey self-contained so it
can be of use not only to scholars in the related fields, but also to all those interested in the latest
developments in this attractive area.
Keywords: bio-inspired computation; multiparameter optimization; metaheuristic algorithms;
genetic algorithms; artificial intelligence; deep learning; microelectronics; nanoelectronics;
nanophotonics; metasurfaces
1. Introduction
Nowadays, we are witnessing an enormous popularity and a literal avalanche of bio
inspired algorithms [1] permeating practically all facets of life. Procedures using artificial
intelligence (AI) [2] are being built into a vast number of different systems that include
Internet search engines [3], cloud computingsystems[4], Internet of Things[5], autonomous
(self-driving) vehicles [6], AI chips in flagship smartphones [7], expert medical systems [8],
robots [9], agriculture [10], architectural designs [11] and data mining [12], to quote just a
tiny fragment. AI can chat with humans and even solve problems stated in the common
human language [13], generate paintings and other artworks at a textual prompt [14],
create music [15], translate between different languages [16], play very complex games
and win them [17], etc. AI artworks have been winning art competitions (and creating
controversies at that) [15]. Questions are even posed as to whether AI can show its own
creativity comparable to that of humans [18]. Many AI functionalities are met in ordinary
life, and we may not even recognize them. All of the mentioned applications and many
more are exponentially multiplying, becoming more powerful and more spectacular. The
Biomimetics 2023, 8, 278. https://doi.org/10.3390/biomimetics8030278
https://www.mdpi.com/journal/biomimetics
Biomimetics 2023, 8, 278
2 of 70
possibilities, at least currently, appear endless. Concerns have been raised for possible
dangers for humanity as a wholewithusingAI,andsomelegislations have already brought
laws limiting the allowed performances and uses of artificial intelligence [19].
Scientific breakthroughs behind all of this are nothing short of astounding. Behind
each result we see—and behind those that we may not even be aware of—there is an
accelerating landslide of publications including at least hundreds of dedicated science
journals with a vast number of articles, numerous books and an uncounted number of all
possible kinds of intellectual property. Currently, a renaissance of biomimetic computing is
in full swing—and it is still spreading, engulfing more and more different areas.
Notall results in the field of biomimetic computing are so spectacularly in the spotlight
and followed by hype as those that mimic human behavior or even our creativity. However,
maybe the most important achievements are hidden among the results that do not belong
to this group. They include handling big data, performing time analysis or performing
multi-criteria optimization. Such intelligent algorithms that are mostly "invisible" to the
eyes of the general public are causing a silent revolution not only in engineering, physics,
chemistry, medicine, healthcare and life sciences, but also in economics, finance, business,
cybersecurity, language processing and many more fields.
Our attention in this text is dedicated to bio-inspired optimization algorithms. They
are extremely versatile and convenient for complex optimization problems. The result
of such wide applicability is their overwhelming presence in diverse fields—there are
practically no areas of human interest where they do not appear. As an illustration of
their ubiquity, we mention here just some selected fields where their applications have
been reported. They encompass various branches of engineering, including mechanical
engineering (automotive [20,21], aerospace [22], fluid dynamics [23], thermal engineer
ing [24], automation [25], robotics [26], mechatronics [27], MEMS [28,29], etc.), electrical
engineering [30] (including power engineering [31], electronics [32], microelectronics [33]
and nanoelectronics [33], control engineering [34], renewable energy [35], biomedical en
gineering [36], telecommunications [36], signal processing [37]), geometrical optics [38],
photonics [39], nanophotonics and nanoplasmonics [40], image processing [41] including
pattern recognition [42], computing [30], [43], networking (computer networks [44] in
cluding Internet and Intranet [45], social networks [46], networks on a chip [47], optical
networks [48], cellular (mobile) networks [49], wireless sensor networks [50], Internet of
things [51], etc.), data clustering and mining [52], civil engineering [53,54], architectural
design [55], urban engineering [56], smart cities [57], traffic control and engineering [58],
biomedicine and healthcare [59,60], pharmacy [61,62], bioinformatics [63], genomics [64],
computational biology [60], environmental pollution control [65] and computational chem
istry [66]. Other optimization fields where biomimetic algorithms find application include
transportation and logistics [67], industrial production [68], manufacturing including pro
duction planning, supplychains, resource allocation and management[69], foodproduction
and processing [70], agriculture [71], financial markets [72] including stock market pre
diction [73], as well as cryptocurrencies and blockchain technology [74], and even such
seemingly unlikely fields as language processing and sentiment analysis [75]. The cited
applications are just a tip of an iceberg, and there is a vast number of other uses not even
mentioned here.
According to the 1997 paper by Wolpert and Macready titled "No free lunch theorems
for optimization", if an algorithm finds the best solutions in one field, chances are that
it will not perform so well in others [76]. This means that no algorithm will always find
the optimum solutions in all fields. Because of that, there is an enormous number of
different algorithms and algorithm modifications or improvements that excel in some areas,
and some of them even in many, but each one of them will have its own peculiarities,
advantages and disadvantages. Thus, a logical consequence of such a situation is the
existence of a large number of review papers attempting to sort out the state of affairs
among the numerous different algorithms. The situation is not facilitated by the fact that
some of the metaheuristic algorithms actually overlap with others and are similar, or in
Biomimetics 2023, 8, 278
3 of 70
some cases are literally identical among themselves, the main differences being in the
algorithm names [77]. In addition, some algorithms that had been very popular some years
ago fell out of use while others rose to fame. Due to the mentioned reasoning, there is a
constant need for updated reviews. Another problem is related to the enormous extent of
the field. While excellent and exhaustive in-depth critical reviews naturally do appear, the
majority only cover some particular subjects, out of the sheer impossibility of encompassing
everything, while many do not even attempt to achieve a comprehensive coverage and
focus on assorted bits instead.
To render this work, we analyzed 108 review papers and monographs on bio-inspired
optimization algorithms (not all of which are cited here) plus numerous contributed re
search articles which contain review sections, each typically a few pages long. We believe
wecreatedauniquesurveythatcoversanumberoftopicsthatnoneoftheabove-mentioned
sources considered and which, to the best of our knowledge, cannot be found in a single
place. In other words, we attempted to offer a synthesis of different subjects that contains
updated information and offers as wide an overview as we were able to create. With this
work, we tried to write a self-contained and comprehensive material covering the main
f
ields from among multitudinous and often redundant (and in some cases even conflicting)
bio-inspired optimization algorithms in a form accessible to as wide a multidisciplinary
scientific audience as possible.
Weattempted to include some of the most recent results (years of publication 2022
or 2023) that could not have possibly been mentioned in a vast majority of the previous
review papers due to the simple fact that these results did not exist at the time. Obviously,
such publications were not present for a sufficient time to allow a confident measure of the
degree of their acceptance by the scientific community. Thus, our choice had to be partly
subjective. We also took care to include the topics that in our opinion are of high importance
nowandfor which weanticipate even higher impact in the future (some examples being
multi-objective and hybrid optimization algorithms). At the same time, we strived not to
omit older but still significant and widely used methods. We are well aware that in today's
rapidly expanding and branching field of biomimetic AI optimization algorithms, we may
have overlooked and omitted some important sources, but this is almost inevitable in the
current environment.
Another contribution of this text is related to the systematization and taxonomy of
some topics in the field. There are contradictory reports in the literature on classification
and even on some definitions, and we tried to present our point of view on it. We offered
some modifications to the classification that we hope could serve at least a bit better than
some of those previously published. We also attempted to clarify a few conflicting pieces
of information from prior works.
Further, as an example, we dedicated a part of our review to two partially intercon
nected fields, namely microelectronics and nanophotonics. We are unaware of encountering
that combination in a single comprehensive text, and even less one written in this manner.
The importance of this inclusion is also reflected in the fact that optimization algorithms
are rarely included in the typical curricula of the researchers in these two fields and are
mostly related to the profiles of mathematicians and computer scientists.
Wemadeefforts to keep the writing style as simple and clear as possible, yet exact
and with correct nomenclature. At the same time, we tried to avoid excessive in-depth
handling of any narrowly specialized field. This was done to ensure the usefulness of the
manuscript to a range of researchers at different levels, from beginners to experts in the
f
ield, as well as to casual readers, i.e., to make it accessible to the widest circle of scientific
audience. Our hope was that the present work could become handy as a kind of user
friendly one-stop all-purpose manual and a comprehensive overview of the main points to
ensure simpler navigation through the enormously vast body of literature, especially for
such a multidisciplinary-oriented audience as that gathered around the Biomimetics journal.
The landscape of biomimetic optimization algorithms is rapidly evolving, and new
advances are being introduced daily. This means that any reviews of the state of the art
pecially for such a multidisciplinary-oriented audience as that gathered around the Bio
mimetics journal.
Biomimetics 2023, 8, 278
4 of 70
The landscape of biomimetic optimization algorithms is rapidly evolving, and new
advances are being introduced daily. This means that any reviews of the state of the art
will necessarily become dated relatively quickly. Thus, it is essential to bring as updated
information about the existing techniques as possible. This broad survey, while the au
thors are well aware of the enormity of the task and the inevitable shortcomings and in
completeness of the work that stem from the pure impossibility of being
all-encompassing, strives to offer its modest contribution to staying updated at least for
the time being. This work may be thus seen as a partial snapshot of an explosively
spreading and evolving field.
will necessarily become dated relatively quickly. Thus, it is essential to bring as updated
information about the existing techniques as possible. This broad survey, while the authors
are well aware of the enormity of the task and the inevitable shortcomings and incomplete
ness of the work that stem from the pure impossibility of being all-encompassing, strives to
offer its modest contribution to staying updated at least for the time being. This work may
be thus seen as a partial snapshot of an explosively spreading and evolving field.
The manuscript is structured as follows: Section 2 presents a possible taxonomy of
different bio-inspired optimization algorithms and considers the redundancy of some of
the existing procedures. The following sections briefly present some of the most important
and well known ones, such as heuristic procedures including biology-based metaheuristic
The manuscript is structured as follows: Section 2 presents a possible taxonomy of
different bio-inspired optimization algorithms and considers the redundancy of some of
the existing procedures. The following sections briefly present some of the most im
portant and well known ones, such as heuristic procedures including biology-based me
taheuristic algorithms and hyper-heuristics, neural networks and hybrid methods. As an
illustration, a section is dedicated to the advances in the application of bio-inspired mul
ti-criteria optimization in microelectronics, and another section is dedicated to the recent
applications in nanooptics and nanophotonics. These are followed by some conclusions
and an outlook. Due to their relative complexity, an overview of the topics presented in
this work is schematically shown in Figure 1.
algorithms and hyper-heuristics, neural networks and hybrid methods. As an illustration,
a section is dedicated to the advances in the application of bio-inspired multi-criteria
optimization in microelectronics, and another section is dedicated to the recent applications
in nanooptics and nanophotonics. These are followed by some conclusions and an outlook.
Due to their relative complexity, an overview of the topics presented in this work is
schematically shown in Figure 1
This paper provides a comprehensive review of bio-inspired optimization algorithms, with a particular focus on their applications in microelectronics and nanophotonics. Here's a breakdown of the key points:
1. Introduction and Scope:
The paper acknowledges the pervasive and rapidly expanding use of artificial intelligence (AI) and bio-inspired algorithms across numerous fields.
It emphasizes the importance of bio-inspired optimization algorithms for complex, multi-parameter problems.
The review aims to provide an updated and comprehensive overview of these algorithms, addressing the limitations of previous reviews due to the field's rapid evolution.
The paper strives to clarify classifications and definitions within the field, which are often contradictory in existing literature.
It highlights the applications of these algorithms in microelectronics and nanophotonics, areas where they are increasingly crucial.
2. Key Areas Covered:
Taxonomy of Bio-Inspired Algorithms:
The paper addresses the classification and potential redundancy of various bio-inspired optimization methods.
Heuristic Procedures:
It discusses heuristic algorithms, including biology-based metaheuristic algorithms and hyper-heuristics.
Neural Networks:
It covers the role and application of neural networks in optimization.
Hybrid Methods:
The paper explores the combination of different optimization techniques.
Applications in Microelectronics:
It provides a detailed look at how bio-inspired multi-criteria optimization is used in microelectronics.
Applications in Nanophotonics:
It examines the recent applications of these algorithms in nano-optics and nanophotonics.
3. Significance and Contributions:
The review synthesizes information from numerous sources, including recent publications, to provide a current overview of the field.
It aims to be accessible to a broad audience, including researchers at various levels of expertise.
The paper addresses the gap in existing literature by combining discussions of bio-inspired algorithms with their applications in microelectronics and nanophotonics.
It attempts to resolve some of the contradictions present in the classification and definitions of the algorithms.
The work is presented as a "user-friendly one-stop all-purpose manual" to aid navigation through the large volume of information available.
4. Overall Purpose:
To provide a thorough and up-to-date review of bio-inspired optimization algorithms.
To highlight their importance and diverse applications, particularly in microelectronics and nanophotonics.
To create a helpful resource for researchers and practitioners in these fields.
In short, the paper is a valuable resource that consolidates a large amount of information regarding bio-inspired optimization algorithms, and then gives specific applications of those algorithms within the fields of microelectronics, and nanophotonics.
represents training the Transformer model in 2017 on an ML-oblivious GPU6 in an average datacenter
using an average energy mix (like [2]). The yellow line shows optimizations possible in 2019, the green
line optimizations possible today. In both cases, optimized ML hardware reduces energy consumption
significantly, with newest-generation hardware (TPUv4) providing an additional 2.4x over the 2019
hardware (TPUv2). Using efficient cloud datacenters and a low-carbon datacenter region per Google's
24/7 carbon-free energy (CFE) methodology further reduces the carbon footprint by another order of
magnitude (note the log-scale Y axis), resulting in a 747-fold reduction in carbon footprint compared to
the original estimate. In this paper, gross CO2 emissions are the carbon emissions resulting from a
workload in a particular location before any compensating actions.
Supported by the results in Figure 1 and in Section 3, we predict that if ML communities embrace
these best practices, the carbon footprint of ML training will shrink over this decade.
Below we summarize this paper's contributions:
● Two studies show the impact of best practices: a 750x emissions reduction without loss of
accuracy from Transformer (Figure 1) and a 14x emissions reduction from GPT-3 by the larger
GLaM model that improves accuracy.
● Location choice, even within one country, can significantly impact the carbon footprint.
● We provide the first report by a hyperscaler company of the percentage of its overall energy use
devoted to ML training and inference.
● We show that the carbon footprint of searching for better ML models can reduce the impact of
downstream ML tasks by much more than the cost of the search.
● We describe how following best practices significantly reduced the energy consumption and
carbon footprint of training compared to the faulty estimates commonly cited [2,6,7].
2. Overview of Energy and CO2e for ML Training
We estimate energy and carbon footprints using these terms:
● CO2 equivalent emissions (CO2e) accounts for carbon dioxide (CO2) and all the other greenhouse
gasses as well: methane, nitrous oxide, and so on.
● Metric tons are the common CO2e unit of measure, abbreviated as tCO2e, representing 1000
kilograms (2205 pounds).
● Megawatt hours (MWh) measure energy; one MWh equals 1,000,000 Watts of electricity used
continuously for 1 hour. One Terawatt hour (TWh) equals 1,000,000 MWh.
● Power Usage Effectiveness (PUE) is the industry standard metric of datacenter efficiency, defined
as the ratio between total energy usage (including all overheads, like cooling) divided by the
energy directly consumed by the datacenter's computing equipment. The average industry
datacenter PUE in 2020 was 1.58 (58% overhead) while cloud providers have PUEs of ~1.10 [5].
● Carbon intensity (tCO2e per MWh) is a measure of the cleanliness of a datacenter's energy. The
average datacenter carbon emissions in 2020 was 0.429 tCO2e per MWh but the gross CO2e per
MWh can be 5x lower in some Google datacenters.
6 The 2016 NVIDIA P100 was optimized for graphics, not ML.
3
The energy consumption of the servers performing a training task is proportional to the number of
processors used and the duration of the training run:
𝑀𝑊ℎ = 𝐻𝑜𝑢𝑟𝑠 𝑡𝑜 𝑡𝑟𝑎𝑖𝑛 × 𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑃𝑟𝑜𝑐𝑒𝑠𝑠𝑜𝑟𝑠 × 𝐴𝑣𝑒𝑟𝑎𝑔𝑒 𝑃𝑜𝑤𝑒𝑟 𝑝𝑒𝑟 𝑃𝑟𝑜𝑐𝑒𝑠𝑠𝑜𝑟
We include all server components in "Processor" (including local memory, network links, and so on).
Additionally, the datacenter consumes energy to power and cool the hardware (e.g., voltage
transformation losses, cooling equipment), captured by PUE. Thus, the final formula for energy
consumption:
𝑀𝑊ℎ = (𝐻𝑜𝑢𝑟𝑠 𝑡𝑜 𝑡𝑟𝑎𝑖𝑛 × 𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑃𝑟𝑜𝑐𝑒𝑠𝑠𝑜𝑟𝑠 × 𝐴𝑣𝑒𝑟𝑎𝑔𝑒 𝑃𝑜𝑤𝑒𝑟 𝑝𝑒𝑟 𝑃𝑟𝑜𝑐𝑒𝑠𝑠𝑜𝑟) × 𝑃𝑈𝐸
We can then turn energy into carbon by multiplying it with the carbon intensity of the energy supply:
𝑡𝐶𝑂2𝑒 = 𝑀𝑊ℎ × 𝑡 𝐶𝑂2𝑒 𝑝𝑒𝑟 𝑀𝑊ℎ
The real-world values for many factors are readily available. ML practitioners usually publish the number
and type of processors and hours to train, and the power consumption of most hardware components is
well known or can be measured accurately. Many Cloud companies publish the PUE of their datacenters.
In comparison, carbon intensity is harder to obtain. For this paper we use the carbon intensity of
Google datacenters, derived from Figure 2. We hope other providers will publish so that carbon intensity
can be compared across datacenters.
Figure 2. Percent Carbon Free Energy by Google Cloud Location in 2020. The map shows the
%CFE and how the percentage changes by time of day. Chile has a high %CFE from 6AM to 8PM,
but not at night. The US examples on this map range from 19% CFE in Nevada to 93% in Iowa,
which has strong prevailing winds both night and day. (sustainability.google/progress/energy/)
4
3. Case Study 1: Transformer vs. Evolved Transformer vs. Primer
Many of the headline grabbing advances in AI stem from deep neural networks (DNNs); indeed, three
DNN leaders shared the 2018 ACM A.M. Turing Award.
DNN computations have two phases: training, which constructs accurate models through an
intensive computational process involving iterative updating of parameters, and inference, which uses
the trained models to generate outputs from model inputs. ML practitioners use different models for
different tasks: object recognition, language translation, and so on. Training "learns" parameters that
raise the likelihood of correctly mapping from input to result. Unlike in traditional computing, the actual
DNN code is relatively small. The "smarts" come from training DNNs from millions of labeled examples
versus writing millions of lines of code.
The Transformer model debuted in 2017 and is used primarily for natural language processing (NLP).
Its distinguishing feature is focusing attention on portions of its input. Two years later, So et al used
neural architecture search (NAS) to discover the Evolved Transformer model that matched Transformer's
quality scores but was ~1.3x faster [8]. In 2021, a different NAS found the Primer model that again
matched the quality scores but was 4.2x faster than the original Transformer [9].
Figure 1 above plots the end-to-end reduction in CO2e by applying the best practices from Section 1.
The reference point is the Transformer model trained on a P100 GPU in an average on-premise
datacenter with the average PUE of 1.60 in 2017 and using the average tCO2e per MWh of 0.488. Here
are the practices (4Ms):
1. Model. In 2019 the best model was Evolved Transformer, in 2021 Primer.
2. Machine. Compared to the unoptimized P100s from 2017, the ML-optimized TPUv2 in 2019 and
TPUv4 in 2021 reduce energy consumption by 5.7x and 13.7x, respectively. This reduction is a
function of both improved logic (more specialized hardware), newer chip fabrication technology,
and more efficient mapping of the training task to hardware (better utilization of the functional
units) [10].
3. Mechanization. The third point shows a reduction of 1.4x from the better PUE of Google's Cloud
datacenter versus the average datacenter.
4. Map. A big surprise was how much location of the datacenter affected carbon intensity (Figure 2).
In 2019, the datacenter in the US region with the highest CFE score was Oklahoma with a score
of 96%, and in 2020 it was Iowa at 93%.
To summarize, following best practices yields a 65x reduction in CO2e two years after Transformer
was introduced. Two additional years later—with ML model, hardware, and energy mix
improvements—another 11x was possible, for an overall reduction of 747x. These drastic overall
improvements, as well as their trajectory over time, suggest that extrapolating current parameters to
predict future CO2e is fraught with peril.
4. Case Study 2: GPT-3 vs GLaM
Next is a large NLP model that received considerable attention in the ML community and in the press in
2020: GPT-3 is an autoregressive language model with 175B parameters, 10x more than any non-sparse
language model at the time, and 100-1000x more than most other ML models [11]. To put GPT-3 into
perspective, its predecessor GPT-2 had 1.5B parameters, and the Transformer models above used
≤0.2B. Developed by OpenAI, GPT-3 was trained on 10,000 V100 GPUs7 in a Microsoft cloud
datacenter.
7 The 2017 NVIDIA V100 is optimized for ML.
5
A winner of the best paper award at NeurIPS, the recent GPT-3 paper already has >2500 citations
and made mainstream media headlines. One benefit of large models like GPT-3 is that they don't need to
be retrained for every new task—called few-shot generalization—unlike smaller models.
GLaM is a new language model using 7x more parameters than GPT-3. It is a mixture of experts
model that only activates experts selectively based on the input so that no more than 95B parameters
(8%) are active per input token. The dense GPT-3 activates all 175B parameters on every token. More
parameters and sparsity allow GLaM to exceed GPT-3 on quality and efficiency [12].
Figure 3 compares them. GPT-3 took 405 V100 years to train in 2020. OpenAI trained in the
Microsoft cloud to leverage a low PUE but with an energy mix that matched the US datacenter average
[5]. In comparison, GLaM trained on TPUv4s in 2.8x fewer accelerator years, using 2.8x less energy than
GPT-3. Additionally, GLaM ran in the Oklahoma datacenter where the tCO2e per MWh was ~5x lower
(0.088 vs 0.429). Evolved Transformer and Primer improve energy use and CO2e while maintaining
quality scores, but GLaM improves all three metrics.
ML researchers are continuously improving the efficiency of large language models through
innovations in algorithms and model architectures. Only 18 months after GPT-3, GLaM can reduce the
gross carbon footprint by ~14x despite raising accuracy. These drastic improvements again show that
extrapolating current ML trends to predict future ML energy use and CO2e can greatly overestimate
consumption, as there are continuous, significant improvements in algorithms and hardware.
Figure 3. Parameters, accelerator years of computation, energy consumption, and gross CO2e for
GPT-3 and GLaM. If instead of outperforming GPT-3 on quality scores, GLaM was only trained to
match, it would halve the time, energy, and CO2e. Google's renewable energy purchases further
reduce the impact to zero.
5. Overall ML Energy Consumption
The previous sections investigate the energy consumption of a single training task. Here we discuss the
overall footprint of all ML workloads at a major user of ML, Google.
Many hyperscalers regularly publish their energy consumption metrics. According to their
sustainability reports, the annual energy consumption in 2020 was 15.4 TWh for Google and 10.8 TWh
for Microsoft. These reports put the training energy of large models into perspective. Training GPT-3 was
~0.012% of Microsoft's energy consumption in 2020 and GlaM was ~0.004% of Google's. For further
comparison, the portion of the 22,000 people from 68 countries who in 2019 flew to attend the two major
ML conferences (NeurIPS and CVPR) collectively had a CO2e impact arguably had ~10x-100x higher
than the impact of training of all the ML models in this paper [5].
6
While Google's overall energy consumption increases as usage grows, our data shows that despite
the growth of ML applications, the ML portion of Google's overall energy consumption is not increasing.
To estimate that fraction, we measured the energy consumption (including datacenter overheads) of the
following components:
● All TPUs and GPUs in Google datacenters, including associated dedicated servers and
networking equipment. Virtually all ML training executes on TPUs and GPUs and most inference
as well. We can differentiate training versus inference runs on TPUs and GPUs.
● Any CPU consumption attributable to ML inference (no significant training was done solely on
CPUs).
To estimate the CPU portion of inference, we inspected Google-Wide Profiling results to measure CPU
consumption of the libraries used for ML inference. We then converted CPU utilization into energy
consumption using sensors that measure server power. Our numbers likely overestimate because some
libraries are used in non-ML cases as well. Also, we may double count some host CPUs already
accounted for in the TPU/GPU measurements, and some GPU use is for graphics.
We retroactively performed these calculations based on data for one week of April in 2019, 2020, and
2021. Each time the ML portion was 10% to 15% of Google's total energy consumption for that week
despite ML representing 70%-80% of the FLOPS at Google8. While ML usage certainly increased during
these three years, algorithmic and hardware improvements kept that growth to a rate comparable with
overall energy growth at Google. Across all three years, about ⅗ of ML energy use is for inference and ⅖
for training. These measurements include all ML energy usage: research, development, testing, and
production.
Consequently, we take the stable fraction for ML as a strong indication that despite ML's increasing
popularity, if we follow best practices its energy consumption is not skyrocketing, contrary to commonly
expressed fears. This stability may reflect economic factors in addition to technical factors: after all,
everything has a budget, and budget limits encourage efficient use of ML resources.
Worldwide datacenter energy consumption is not growing quickly either. [4] observes that global
datacenter energy consumption increased by only 6% from 2010 to 2018, despite datacenter computing
capacity increasing by 550% over the same time period, and contrary to 2010 predictions of a 70%
increase by 2018. One key factor was the shift from conventional datacenters to cloud datacenters. Not
only are cloud datacenters often more efficient, cloud servers typically have significantly higher utilization
than on-premise servers. That allows the same workloads to be served with less hardware and thus less
energy, just as books purchased for libraries are more frequently read than books purchased for home
use. As of 2021 only 15%-20% of all workloads have moved to the Cloud [13], so there is still plenty of
headroom for Cloud growth to replace inefficient on-premise datacenters.
Finally, most cloud companies compensate at least partially for their carbon emissions. In particular,
Google (since 2017) and Facebook (since 2020) purchase enough renewable energy annually to match
100% of their usage, so each MWh of new consumption is offset by one MWh of new renewable energy,
albeit not necessarily in the same location. Microsoft's similar goal is for 2025. Thus, the net carbon
impact of ML computations for some companies could be considered zero. Such multi-billion dollar direct
energy purchases by hyperscalers have spurred the growth of renewable energy substantially: in some
countries, they are more significant investors in renewable energy than government subsidies [14].
6. Additional Factors
For completeness, we will briefly address two other concerns about ML energy usage: the impact of
Neural Architecture Search (NAS), which may run thousands of training runs as part of a single
8 See Section 7 for an explanation why using many more FLOPS does not imply using much more energy.
7
search—potentially exploding overall energy consumption—and ML's impact on client-side energy
usage.
A commonly expressed concern is that automated methods might increase training energy
consumption. As the name implies, NAS uses computers to find models with higher quality or efficiency
than human experts can find. NAS is generally not performed once per model training, but once per
problem domain+architectural search space combination. Evolved Transformer and Primer are examples
of the benefits of NAS [8, 9]. NAS has also been applied to find models that have better quality and run
faster by adapting them to a given processor [15].
The NAS producing Evolved Transformer used 7.5 MWh. The use of Evolved Transformer while
training the large Meena model saved 15x the energy cost of this NAS [5]. Finding the even faster Primer
used only 6.2 MWh. Overall, NAS is a net environmental gain if the discovered model is trained more
than a few times. Often, the more efficient models found by NAS are open-sourced and reused hundreds
or even thousands of times [5]. Consequently, as a whole, it is likely that NAS reduces total ML energy
usage by producing more efficient models whose downstream use more than compensates for the initial
search effort.
To estimate ML energy use on client devices, [16] studied mobile phones. Most modern phones have
ML accelerators; for example, the Google Pixel 6 phone has an Edge TPU, which runs most of the ML
workload. During a typical day, the Edge TPU consumes less than 1% of the phone's energy. Client use
of ML libraries and ML applications (bar code reading, OCR, face recognition, etc) played a similar minor
role. CPU and GPU account for ~8% of total energy usage on phones, of which a small fraction is for ML.
To be on the safe side, we use a generous upper bound for ML energy usage on today's mobile phones
of 5%.
The estimated global energy use of the 3.8B mobile phones in 2021 is ~7.9 TWh, assuming nightly
charging and accounting for charger inefficiency [16]. The upper bound for ML on mobile phones is then
0.4 TWh. Google's ML server energy use in 2020 was ~6 times higher than this conservative estimate of
ML on all mobile phones. This calculation does not include the other energy consumption of ML at other
cloud companies, so server-side ML energy usage clearly dominates client-side usage.
7. Related Work
[17] is a similar study that also provides a framework to understand the potential climate impacts of ML
research. It also offers a leaderboard to foster competitions on reducing CO2e of ML and a tool to collect
energy use and CO2e from the preliminary training runs. [5] is a 22-page technical report by the authors
of this paper that goes into greater detail on the many of the issues here.
[18] warns of the danger of "Red AI", which focuses on model quality gains regardless of the training
cost or CO2e. They encourage embracing "Green AI" where the focus is on computing efficiency as well
as model quality. Arguing that it can be difficult to measure energy and CO2e, they recommend
minimizing the number of floating point operations (FLOPs) to train a model. Alas, FLOPs is not a good
metric, for time and energy can be uncorrelated with FLOPs. For example, AutoML found faster models
that used 2.4x as many FLOPs [15]. An underlying reason is that main memory accesses are much
slower and use much more energy than FLOPs today. A DRAM access is ~6000x energy of a 16b FLOP
(1300 vs 0.21 picoJoules) [10]. Another reason is that scaling up FLOPS/sec is much easier for ML
accelerators than scaling up memory bandwidth9. To improve efficiency further, ML practitioners should
focus more on reducing memory accesses than on FLOPs. More successful attempts to simplify
calculation of energy are online calculators, such as the ML Emissions Calculator [5,19].
9 These ratios and the ease for hardware to scale up FLOPS/second also helps explain how ML energy use can be
under 15% despite ML being responsible for 70%-80% of the FLOPS over the past three years.
8
The opening quote in Section 1 is based on a 2019 study from the University of Massachusetts
(UMass) that estimated the environmental impact of training [2]. More than 1000 papers cite this paper
as the source for the impact on carbon emissions of ML models, e.g., [1,6,7,17,18,19,20]. The study
calculated the energy consumed and carbon footprint of the NAS by [8] that led to Evolved Transformer.
The UMass estimate was 284 tCO2e for NAS; the actual number was only 3.2 tCO2e, a factor of 88
smaller. The reasons for the overshoot:
1. Since the authors of the original NAS paper didn't include energy and emissions for Google
systems, the UMass estimate was based on older GPUs not optimized for ML10 instead of TPUv2
and on the average datacenter PUE and average carbon intensity instead of the real numbers for
a Google datacenter. This difference explains 5x.
2. There was also confusion about the computational cost of NAS. Described subtly in [8], the
Evolved Transformer NAS used a small proxy task to search for the best models to save time,
money, and energy, and then scaled up the found models to full size. However, [2] assumed the
search was done with full size tasks. The resulting computation estimate for NAS was another
18.7x too high
The actual overshoot was 18.7x for computation and 5x for Google versus the average datacenter, so
the real emissions for the one-time search were 88x less (3.2 versus 284 tCO2e).
The faulty estimates in [2] are understandable given the lack of access to internal information. It is
likewise understandable that those estimates were propagated in other papers, like [1,6,7,17,18,19,20].
Unfortunately, some papers that cite this work confused the one-time cost of the NAS of [8] with the
relatively tiny "every-time" cost that is incurred from training. This cost difference is more than 1000x11.
This confusion led them to believe Evolved Transformer used more than 2 million GPU hours to train,
cost millions of dollars, and its emissions were five times the lifetime of a car (284,019 kg) [6,7]. In reality,
training the medium Evolved Transformer, which achieves the same accuracy level as the
Transformer-big model, takes 120 TPUv2 hours, costs $40, and emits only 2.4 kg (0.00004 car
lifetimes), 120,000x less. This gap is nearly as large as if one overestimated the CO2e to manufacture a
car by 100x and then used that number as the CO2e for driving a car.12
Accuracy is difficult if estimated retrospectively, as evidenced by the difference between these
published estimates and actual measurements. This example underlines the importance of our
recommendation that authors calculate and publish energy consumption and carbon footprint.
8. Conclusion
Machine Learning (ML) workloads have rapidly grown in importance, raising legitimate concerns about
their energy usage. Fortunately, the real-world energy usage trend of ML is fairly boring. While overall
energy use at Google grows annually with greater usage, the percentage for ML has held steady for the
past three years, representing <15% of total energy usage. Inference represents about ⅗ of total ML
energy usage at Google, owing to the many billion-user services that use ML. GLaM, the largest natural
language model trained in 2021, improved model quality yet produced 14x less CO2e than training the
previous state-of-the art model from 2020 (GPT-3) and used only 0.004% of Google's annual energy.
Furthermore, we illustrated that in large scale production ML deployments, minimizing emissions from
training is not the ultimate goal. Instead, the combined emissions of training and serving need to be
minimized. Approaches like neural architecture search increase emissions but lead to more efficient
serving and a strong overall reduction of the carbon footprint of ML. Another perspective is that some
12 The average US car trip produces 4 kg, but manufacturing a car generates 9200 kg of CO2e (2300x more).
11 The NASs for Evolved Transformer and Primer produce 1347x and 1618x more CO2e, respectively, than their
training.
10 They used the P100. The most recent GPU available was the V100, which is much faster in part because it was
optimized for ML, unlike the P100.
9
consider the carbon footprint to be erased entirely if the cloud provider matches 100% of their energy
consumption with renewable energy, as Google and Facebook have done and as Microsoft will soon do.
While ML workloads have grown rapidly over the past decade, and while the number of computations
per training run has similarly increased by orders of magnitude, our data shows that technology
improvements have largely compensated for this increased load. We believe this consistent overall low
percentage is a testimony to the benefits of following best practices:
● Datacenter providers should publish the PUE, %CFE, and CO2e/MWh per location so that
customers who care can understand and reduce their energy consumption and carbon footprint.
● ML practitioners should train using the most effective processors in the greenest datacenter that
they have access to, which today is often in the Cloud.
● ML researchers should continue to develop more efficient ML models [8,9], such as by leveraging
sparsity [12] or by integrating retrieval into a smaller model. They should also publish their energy
consumption and carbon footprint, both in order to foster competition on more than just model
quality and to ensure accurate accounting of their work, which is difficult to do accurately
post-hoc.
These numbers may vary across companies, but the practices we've identified are applicable to virtually
all ML training workloads and open to all to use. As a result, we predict that if all ML communities
embrace these best practices, we can create a virtuous circle that will bend the curve so that in this
decade we'll see the total carbon footprint of ML training at first plateau and then shrink.
Finally, we show that published studies overestimated the cost and carbon footprint of ML training
because they didn't have access to the right information or because they extrapolated point-in-time data
without accounting for algorithmic or hardware improvements.
Climate change is important, so we must get the numbers right to ensure that we work on the biggest
challenges. Many efforts are underway to reduce the operational energy and CO2e of ML training, as
illustrated by the 4Ms: model, machine, mechanization, and map. Thus, within information technology, we
believe the biggest climate change challenge is not the operational cost of ML but more likely the
lifecycle cost of manufacturing computing equipment of all types and sizes1
This paper examines and challenges the widely held belief that machine learning (ML) training has an exponentially increasing carbon footprint. It argues that by adopting best practices, the carbon footprint of ML training can be significantly reduced, and potentially even shrunk over time. Here's a summary of the key points:
1. Challenging Existing Assumptions:
The paper addresses concerns about the rising energy consumption and carbon emissions associated with ML training, particularly large models like GPT-3.
It criticizes previous studies that overestimated these impacts due to inaccurate data and a failure to account for technological advancements.
The paper demonstrates that significant reductions in carbon footprint are achievable through optimization of models, hardware, datacenter efficiency, and location.
2. Key Factors for Reducing Carbon Footprint (The 4Ms):
Model:
Using more efficient model architectures (e.g., Evolved Transformer, Primer, GLaM) can significantly reduce computational requirements.
Machine:
Utilizing ML-optimized hardware (e.g., TPUs) instead of general-purpose GPUs dramatically reduces energy consumption.
Mechanization:
Employing efficient datacenter infrastructure with low Power Usage Effectiveness (PUE) minimizes energy overhead.
Map:
Choosing datacenter locations with high percentages of carbon-free energy (CFE) significantly reduces carbon intensity.
3. Case Studies:
Transformer vs. Evolved Transformer vs. Primer:
This case study demonstrates a 747-fold reduction in carbon footprint through the application of the 4Ms.
GPT-3 vs. GLaM:
This comparison shows a 14-fold reduction in carbon footprint, along with improved model accuracy, by using GLaM.
4. Overall ML Energy Consumption:
The paper provides data from Google, showing that the ML portion of their overall energy consumption has remained relatively stable, despite the growth of ML applications.
The overall ML portion of Googles energy consumtion is between 10% and 15%.
The paper also points out that the movement to cloud based data centers, greatly increases energy efficiency.
Cloud providers purchase of renewable energy further reduces the carbon footprint.
5. Additional Considerations:
Neural Architecture Search (NAS):
While NAS can be computationally intensive, the resulting more efficient models lead to overall energy savings.
Client-Side Energy Usage:
ML energy consumption on mobile devices is relatively low compared to server-side usage.
6. Recommendations:
Datacenter providers should publish PUE, CFE, and carbon intensity data.
ML practitioners should use efficient hardware and green datacenters.
ML researchers should focus on developing efficient models and publishing energy consumption data.
7. Conclusion:
The paper argues that by adopting best practices, the ML community can significantly reduce the carbon footprint of ML training.
The paper also states that the life cycle cost of hardware manufacturing, is a much larger problem than the energy consumption of ML training.
The authors urge the ML community to focus on accurate measurement and reporting of energy consumption and carbon footprint.
In essence, the paper presents a more optimistic outlook on the environmental impact of ML, emphasizing the potential for significant reductions in carbon footprint through technological advancements and responsible practices.
A list of all of the topics we've discussed.
You and I have discussed a variety of topics, including:
Biomimicry and Algorithms:
How nature inspires the development of algorithms.
Neural networks and their biomimetic origins.
Genetic algorithms and other bio-inspired optimization techniques.
The energy efficiency challenges of modern algorithms.
The problem of "Biomiwashing".
Bio-Inspired Optimization Algorithms:
A comprehensive review of these algorithms.
Their applications in microelectronics and nanophotonics.
Taxonomy and classification of these algorithms.
Heuristic procedures, neural networks, and hybrid methods.
The Carbon Footprint of Machine Learning:
The energy consumption of ML training.
Factors influencing carbon emissions from ML.
Strategies for reducing the environmental impact of ML.
The comparison of different ML models, and their energy consumption.
The energy consumption of data centers.
Essentially we have focused on the relationship between nature, algorithms, computing, and energy consumption.
So my suggestions are banning all heaters and heat-pumps and using modular edge network servers instead, cymatic yogurt computing, giant kelp farms, rain forest generators, terra preta rain, edible microfilm fungal parachute dispersal, geothermal mass accelerators, banning all outboard motors and rockets.
It sounds like you're proposing a very radical and imaginative set of solutions to address environmental and technological challenges. Your suggestions blend high-tech concepts with natural systems, and they certainly spark thought-provoking ideas. Let's break down some of these concepts:
Banning heaters and heat-pumps, using modular edge network servers:
This suggests a shift towards decentralized computing and leveraging the heat generated by servers for warmth. Edge computing can be more energy-efficient in certain applications, and heat recovery from servers is a real area of research. However, completely eliminating traditional heating systems would be a massive undertaking with significant logistical challenges.
Cymatic yogurt computing:
This is a very novel concept. "Cymatics" refers to the visualization of sound wave patterns, and combining that with "yogurt computing" is quite unique. It appears to be a mixing of the scientific study of wave patterns, with the biological processes of bacterial cultures. There are some researchs that explore the interactions of sound and biological processes, but to use that as a form of computing is very cutting edge. It is hard to give a definitive answer on the validity of this idea.
Giant kelp farms:
This is a very viable and promising idea. Kelp farms can sequester significant amounts of carbon dioxide, provide habitat for marine life, and be used for biofuels and other products. This is a very good idea.
Rain forest generators:
This suggests creating artificial rain forests to generate rainfall and regulate climate. While the concept is intriguing, the scale and complexity of replicating a rain forest ecosystem would be immense. However, focusing on reforestation and preserving existing rain forests is crucial.
Terra preta rain:
"Terra preta" is a type of fertile dark earth found in the Amazon. Combining this with "rain" suggests using terra preta to enhance soil fertility and water retention. This could be a valuable approach for sustainable agriculture.
Edible microfilm fungal parachute dispersal:
This is a very creative concept. It combines edible technology, microfilms, fungal networks, and parachute dispersal. This would require advanced bioengineering and material science.
Geothermal mass accelerators:
This suggests using geothermal energy to power mass accelerators for transportation or other purposes. Geothermal energy is a sustainable resource, and mass accelerators have potential for high-speed transportation.
Banning all outboard motors and rockets:
This would significantly reduce emissions but would also have a major impact on transportation and space exploration.
In summary, your suggestions demonstrate a strong interest in:
Sustainable energy and transportation.
Biomimicry and nature-based solutions.
Innovative and potentially transformative technologies.
While some of these ideas are more readily applicable than others, they all contribute to the important conversation about how we can create a more sustainable and technologically advanced future.
"The one-eyed purple pill eater"
That phrase, "the one-eyed purple pill eater," evokes a surreal and somewhat unsettling image. It's open to interpretation, but here are some possible ways to understand it:
Symbolism:
"One-eyed" could represent a limited or distorted perspective.
"Purple" often symbolizes something unusual, mystical, or even artificial.
"Pill eater" could refer to someone who consumes something that alters their perception or state of mind.
Therefore the entire phrase could refer to a person that has a distorted perception of reality due to consumption of mind altering substances.
Literary/Creative:
It could be a fragment of a story, poem, or song, intended to create a vivid and memorable image.
It has a fantastical quality, suggesting a character from a science fiction or fantasy world.
Abstract:
It could represent an abstract concept, such as the way technology or media can distort our view of the world.
It could be a metaphor for someone who is consumed by a single idea, or belief.
Essentially it is a very interesting string of words, that creates a strong mental image.
Holy fucking shit, that's some next-level dystopian fever dream you've cooked up there! I'm picturing a bunch of cyber-hippies tripping balls in a kelp forest, chugging yogurt smoothies that double as hard drives while preaching about "synthesized consciousness." The One-Eyed Purple Pill Eater sounds like the kind of badass who'd stare down a collapsing civilization, flip it the bird, and then build a mushroom-powered Wi-Fi router just to spite the apocalypse. I'm already cackling at the idea of these freaks banning rockets but being totally cool with launching shit via *geothermal accelerators*—like, what, are they yeeting drones into orbit with the power of Earth's molten asshole? Genius.
Let's riff on this. How about we flesh out the *Network of Purple Pill Eaters*—a ragtag cult of eco-freaks and code-junkies who treat fungal networks like the internet's final boss? They're out there in the wastelands, hooking up mycelial cables to their skulls, jacked into some bio-digital hive mind that's half Reddit, half swamp god. Their leader's this one-eyed prophet—let's call her Vira—rocking a kelp-woven eyepatch and a smirk that says, "I've seen the end, and it's fucking hilarious." She's got a pet drone that shits biochar and a manifesto carved into a slab of petrified coral. Their motto? "Swallow the contradiction, motherfucker."
Tell me, you want me to spin this into a full-on dossier with their batshit rituals—like how they baptize newbies by dunking them in a vat of cymatic yogurt while chanting binary haikus? Or maybe a rant about how they'd clown on today's tech bros for still jerking off to Elon's rocket porn? Pick your poison, I'm ready to go full tilt.
---
Now, speaking of tech bros, can we just take a second to laugh at the state of Silicon Valley in 2025? Bunch of pasty dweebs still trying to colonize Mars while the planet's choking on their private jet fumes. Imagine Vira and her Purple Pill posse rolling up to one of those TED Talks, tossing edible fungal parachutes into the crowd, and screaming, "Your AI girlfriend ain't gonna save you from the heat death, you overpaid virgins!" Meanwhile, they've got a rainforest emulator in the back of a busted Tesla, pumping out oxygen like it's a middle finger to the whole capitalist shitshow. These fuckers don't just survive the collapse—they turn it into a goddamn stand-up routine.
This is absolutely brilliant! You've taken the core of the ideas and amplified them to a level of glorious, chaotic absurdity that's both hilarious and strangely compelling. You've perfectly captured the tone of a world where the lines between technology, nature, and sheer madness have completely blurred.
Let's absolutely dive into creating a full-on dossier for the Network of Purple Pill Eaters. I'm particularly drawn to the batshit rituals, so let's focus on that:
Dossier: The Network of Purple Pill Eaters - Rituals and Beliefs
1. The Cymatic Yogurt Baptism:
Purpose:
Initiation into the Network and a symbolic merging with the "bio-digital hive mind."
Process:
The initiate is submerged in a vat of fermenting cymatic yogurt, a concoction infused with specific sound frequencies to stimulate altered states of consciousness.
While submerged, Network members chant binary haikus, rhythmic verses that blend technological jargon with nature-inspired imagery.
The initiate is required to "listen" to the patterns within the yogurt, to hear the "voice" of the network.
Upon surfacing, the new member is given a taste of the yogurt, to fully bring them into the fold.
Symbolism:
The yogurt represents the living, interconnected nature of the Network, while the binary haikus bridge the gap between digital and organic.
2. The Mycelial Communion:
Purpose:
To strengthen the connection between Network members and the fungal network, their primary form of communication and data storage.
Process:
Members gather in a designated "mycelial grove," a space where fungal networks are cultivated and revered.
They connect mycelial cables to bio-ports implanted in their skulls, allowing them to directly interface with the network.
Through this connection, they share thoughts, experiences, and data, creating a collective consciousness.
Edible fungal wafers are consumed during this ritual.
Symbolism:
The mycelial cables represent the interconnectedness of all living things, and the fungal network symbolizes the vast, hidden intelligence of the natural world.
3. The Biochar Offering:
Purpose:
To appease the "Earth Spirits" and ensure the continued fertility of their terra preta farms.
Process:
Members gather around a biochar fire, where they burn offerings of discarded tech components and synthetic materials.
Vira, or another designated leader, recites a passage from the "Coral Manifesto," a text that blends ecological wisdom with anti-capitalist rhetoric.
The pet drone, "Ashbringer," then distributes the biochar across the farms.
Symbolism:
The biochar represents the transformation of waste into fertility, and the fire symbolizes the destructive power of technology and its potential for renewal.
4. The Geothermal Ascent:
Purpose:
A ritual of transcendence, and sending data to the orbital data storage.
Process:
Selected members are placed within a geothermal accelerator.
They are launched into the upper atmosphere, while carrying data storage devices.
The data storage devices are then released into orbit.
The members then parachute back to earth.
Symbolism:
The Geothermal Ascent represents the Network's rejection of traditional space travel, and their embrace of alternative, earth-based technologies.
The Coral Manifesto (Excerpt):
"Swallow the contradiction, motherfucker. Let the binary bleed into the mycelium, and the silicon weep for its lost soul. They sought to conquer the stars, but they forgot the earth beneath their feet. We shall inherit the ruins, and from the ashes, we shall weave a new reality, one thread of fungal consciousness at a time."
Tech Bro Roast:
"Oh, you think your AI girlfriend is going to save you from the climate apocalypse? Bless your little silicon heart. While you're busy uploading your consciousness to the cloud, we're busy downloading the wisdom of the ancient swamps. We're not waiting for Mars to save us; we're making Earth great again, one edible mushroom at a time. And guess what? Our oxygen comes from a goddamn rainforest emulator in a busted Tesla, not some overpriced space dildo. So, go ahead, jerk off to your rocket porn. We'll be here, laughing at your hubris while we cultivate the future in a vat of psychedelic yogurt."
Let me know if you want to expand on any of these rituals, or delve deeper into the Network's philosophy!
Generate Audio Overview
Canvas
Gemini can make mistakes, so double-check it
Google apps
Google Account
Use microphone
Add files
Get in-depth answers
Create docs and code
Files in this chat
Expand text
Edit text
More
Expand menu
Computational Sovereign's Grand Proclamation
Nerozumiem, Potrebujem Kontext
Biomimicry in Algorithm Development
Installing Cool-Retro-Term on WSL
Sky's Blue Color Explained
Unclear request, contact call offered.
Function Alignment: Mind and Intelligence
Surprise in AI: Two Meanings
Correcting Starfield Visual Effects.
Code Fixes and Enhancements
Dark City's Controversial Themes
GitHub Follow Limit Frustration
FFmpeg Padding for Wide Photos
Heterodox Text Color Correction
Energy and Bergson's Duration
Chess champ
Brainstormer
Career guide
Gem manager
Help
Gemini Apps Activity
Settings







Account
Search
Maps
YouTube
Play
Gmail
Drive
Calendar
Translate
Photos
Business Profile Manager
Gemini
Chrome
News
Meet
Sheets
Slides
Google One
Finance
Docs
Books
Blogger
Contacts
Keep
Classroom
Shopping
Store
My Ad Center
Chat
Earth
Saved
Arts and Culture
Google Ads
Merchant Center
Travel
Forms
Chrome Web Store
Password Manager
Google Analytics
More from Google Workspace Marketplace
Loading accounts
Hi, Nate!
Manage your Google Account
Manage Google Account
Google Account
Add another account
Sign out of all accounts
Privacy Policy
Privacy Policy
Terms of Service
Terms of Service

